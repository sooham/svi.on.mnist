{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sudoku import SudokuGenerator, Sudoku\n",
    "import math\n",
    "import subprocess\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Model: hidden_dim=64, num_layers=9, kernel_size=3\n",
      "  Embeddings: use_learned=True, embedding_dim=15\n",
      "  Training: dataset_size=20000, epochs=4000, batch_size=1024, lr=0.001\n",
      "  Diffusion: k_max=6\n",
      "  Device: cuda\n",
      "\n",
      "âš ï¸  Memory optimizations applied:\n",
      "  - Reduced BATCH_SIZE to 1024 (from 1024)\n",
      "  - Reduced K_MAX to 6 (from 10)\n",
      "  - Reduced DATASET_SIZE to 20000 (from 10000)\n",
      "  - Added memory cleanup in training loop\n",
      "\n",
      "âš¡ Performance optimizations enabled:\n",
      "  - torch.compile() for JIT compilation (2-3x speedup)\n",
      "  - Mixed precision training (AMP) (1.5-2x speedup)\n",
      "  - Fused AdamW optimizer\n",
      "  - Optimized compute_loss() (removed unnecessary clones/ops)\n",
      "  - Expected combined speedup: 3-6x faster training\n",
      "\n",
      "ðŸ’¡ If you still get OOM errors, restart the kernel first!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "# Modify these parameters to experiment with different model configurations\n",
    "\n",
    "# --- Model Architecture Parameters ---\n",
    "HIDDEN_DIM = 128              # Network width (default: 256, reduced for memory)\n",
    "NUM_LAYERS = 9                # Number of residual conv blocks (default: 6, reduced for memory)\n",
    "KERNEL_SIZE = 3               # Conv kernel size\n",
    "NUM_GROUPS = 8                # GroupNorm groups (must divide HIDDEN_DIM evenly)\n",
    "NUM_TIMESTEPS = 81            # Fixed at 81 for Sudoku (9x9 grid)\n",
    "\n",
    "# --- Embedding Parameters ---\n",
    "USE_LEARNED_EMBEDDINGS = True  # Use learned embeddings from LLM model\n",
    "EMBEDDING_MODEL_PATH = './sudoku2vec_trained_model.pt'  # Path to saved embedding model\n",
    "EMBEDDING_DIM = 15            # Dimension of learned embeddings (from LLM model)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "DATASET_SIZE = 20000          # Number of diffusion sequences to pre-generate\n",
    "NUM_EPOCHS = 4000             # Number of training epochs\n",
    "BATCH_SIZE = 1024              # Batch size (reduced from 1024 for memory)\n",
    "LEARNING_RATE = 1e-3          # Optimizer learning rate\n",
    "WEIGHT_DECAY = 1e-4           # AdamW weight decay for regularization\n",
    "GRAD_CLIP_MAX_NORM = 1.0      # Gradient clipping threshold\n",
    "\n",
    "# --- Logging & Evaluation ---\n",
    "LOG_INTERVAL = 10             # Log metrics every N epochs\n",
    "EVAL_INTERVAL = 50           # Evaluate and sample every N epochs\n",
    "\n",
    "# --- Diffusion Parameters ---\n",
    "K_MAX = 6                     # Maximum number of forward steps for multi-step prediction loss (reduced from 10)\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Wandb & Checkpointing Configuration ---\n",
    "USE_WANDB = True              # Enable Weights & Biases logging\n",
    "WANDB_PROJECT = \"sudoku-diffusion\"  # Wandb project name\n",
    "WANDB_ENTITY = None           # Wandb entity (None = default)\n",
    "CHECKPOINT_DIR = \"./checkpoints\"  # Directory to save model checkpoints\n",
    "CHECKPOINT_INTERVAL = 50      # Save checkpoint every N epochs\n",
    "RESUME_FROM_CHECKPOINT = None  # Path to checkpoint to resume from (None = start fresh)\n",
    "\n",
    "\n",
    "# --- Sudoku2Vec ---\n",
    "ATTENTION_DIM = 9\n",
    "N_HEADS = 9\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: hidden_dim={HIDDEN_DIM}, num_layers={NUM_LAYERS}, kernel_size={KERNEL_SIZE}\")\n",
    "print(f\"  Embeddings: use_learned={USE_LEARNED_EMBEDDINGS}, embedding_dim={EMBEDDING_DIM}\")\n",
    "print(f\"  Training: dataset_size={DATASET_SIZE}, epochs={NUM_EPOCHS}, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"  Diffusion: k_max={K_MAX}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Wandb: enabled={USE_WANDB}, project={WANDB_PROJECT}\")\n",
    "print(f\"  Checkpointing: dir={CHECKPOINT_DIR}, interval={CHECKPOINT_INTERVAL}\")\n",
    "print(\"\\nâš ï¸  Memory optimizations applied:\")\n",
    "print(f\"  - Reduced BATCH_SIZE to {BATCH_SIZE} (from 1024)\")\n",
    "print(f\"  - Reduced K_MAX to {K_MAX} (from 10)\")\n",
    "print(f\"  - Reduced DATASET_SIZE to {DATASET_SIZE} (from 10000)\")\n",
    "print(\"  - Added memory cleanup in training loop\")\n",
    "print(\"\\nâš¡ Performance optimizations enabled:\")\n",
    "print(\"  - torch.compile() for JIT compilation (2-3x speedup)\")\n",
    "print(\"  - Mixed precision training (AMP) (1.5-2x speedup)\")\n",
    "print(\"  - Fused AdamW optimizer\")\n",
    "print(\"  - Optimized compute_loss() (removed unnecessary clones/ops)\")\n",
    "print(\"  - Expected combined speedup: 3-6x faster training\")\n",
    "print(\"\\nðŸ’¡ If you still get OOM errors, restart the kernel first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bb31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD EMBEDDING MODEL (from llm_on_sudoku.ipynb)\n",
    "# ============================================================================\n",
    "# We need the Sudoku2Vec class definition to load the trained model\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding on unit circle for 9x9 Sudoku grid\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a grid of positions (0-8 for both x and y)\n",
    "        x_coords = torch.arange(0, 9).unsqueeze(0).repeat(9, 1)\n",
    "        y_coords = torch.arange(0, 9).unsqueeze(1).repeat(1, 9)\n",
    "        \n",
    "        # Convert grid positions to linear indices (0-80)\n",
    "        linear_indices = y_coords * 9 + x_coords  # shape: (9, 9)\n",
    "        \n",
    "        # Convert linear indices to angles on unit circle\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (9, 9)\n",
    "        \n",
    "        # Compute x, y coordinates on unit circle\n",
    "        x_circle = torch.cos(angles)\n",
    "        y_circle = torch.sin(angles)\n",
    "        \n",
    "        # Stack and add batch dimension\n",
    "        pos_encoding = torch.stack([x_circle, y_circle], dim=-1).unsqueeze(0)  # shape: (1, 9, 9, 2)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def get_embedding_for_position(self, pos):\n",
    "        # input (batch, 2) where pos[:, 0] is x and pos[:, 1] is y\n",
    "        linear_indices = pos[:, 1] * 9 + pos[:, 0]  # shape: (batch,)\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (batch,)\n",
    "        x_circle = torch.cos(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        y_circle = torch.sin(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        return torch.cat([x_circle, y_circle], dim=1)  # shape: (batch, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a (batch, 9, 9, embedding_dim) grid\n",
    "        # output (batch, 9, 9, embedding_dim + 2) grid by adding pos_encoding to x\n",
    "        batch_size = x.shape[0]\n",
    "        pos_expanded = self.pos_encoding.repeat(batch_size, 1, 1, 1)\n",
    "        return torch.cat([x, pos_expanded], dim=-1)\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # stack all weight matrices 1...h together for efficiency\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # seperate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3) # [batch, head, seqlen, dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0,2,1,3) # [batch, seqlen, head, dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values) # [batch, seq_length, 81]\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "class Sudoku2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_dim=ATTENTION_DIM, num_heads=N_HEADS, device='cpu'):\n",
    "        super(Sudoku2Vec, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim) # this will provide the key queries and values\n",
    "        self.total_dim = self.embedding_dim + 2\n",
    "\n",
    "        self.mha = MultiheadAttention(\n",
    "            input_dim=self.total_dim,\n",
    "            embed_dim=attention_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the learned token embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding weight matrix of shape [vocab_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embed.weight.detach()\n",
    "    \n",
    "    def get_embedding_for_token(self, token):\n",
    "        \"\"\"\n",
    "        Get the embedding vector for a specific token.\n",
    "        \n",
    "        Args:\n",
    "            token: Integer token ID or tensor of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector(s) for the given token(s)\n",
    "        \"\"\"\n",
    "        if isinstance(token, int):\n",
    "            token = torch.tensor([token], device=self.device)\n",
    "        elif not isinstance(token, torch.Tensor):\n",
    "            token = torch.tensor(token, device=self.device)\n",
    "        return self.embed(token).detach()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the model in a portable format that can be easily loaded.\n",
    "        This saves the model architecture and weights in a single file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path where to save the model (should end with .pt or .pth)\n",
    "        \"\"\"\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'model_config': {\n",
    "                'vocab_size': self.embed.num_embeddings,\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'attention_dim': self.mha.embed_dim,\n",
    "                'num_heads': self.num_heads,\n",
    "            },\n",
    "            'model_class': 'Sudoku2Vec',\n",
    "        }\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cpu'):\n",
    "        \"\"\"\n",
    "        Load a saved Sudoku2Vec model from file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the saved model file\n",
    "            device: Device to load the model on ('cpu', 'cuda', 'mps')\n",
    "            \n",
    "        Returns:\n",
    "            Sudoku2Vec: Loaded model instance\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        \n",
    "        # Extract configuration\n",
    "        config = checkpoint['model_config']\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            attention_dim=config['attention_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set to evaluation mode by default\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        print(f\"Configuration: vocab_size={config['vocab_size']}, \"\n",
    "              f\"embedding_dim={config['embedding_dim']}, \"\n",
    "              f\"attention_dim={config['attention_dim']}, \"\n",
    "              f\"num_heads={config['num_heads']}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, target, position, sudoku_grid, mask=True):\n",
    "        # target - the token in the target blank space we try to predict shape [batch] i.e [0, 3, 3, 5, 1, ...]\n",
    "        # position - the (x, y) position of the target shape [batch, 2] - [[1, 1], [0, 3], [7,7], ...]\n",
    "        # sudoku_grid - the sudoku grid for the problem with target we want to predict shape [batch, 9, 9]\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        target_token_embeddings = self.embed(target) # shape [batch, embedding_dim]\n",
    "        target_position_vectors = self.pe.get_embedding_for_position(position) # [batch, 2]\n",
    "        target_token_with_position = torch.cat([target_token_embeddings, target_position_vectors], dim=-1)  # shape [batch, total_dim]\n",
    "\n",
    "        # mask the target in the grid\n",
    "        sudoku_grid_masked = sudoku_grid\n",
    "        if mask:\n",
    "            batch_indices = torch.arange(sudoku_grid.shape[0], device=self.device)\n",
    "            sudoku_grid_masked = sudoku_grid.clone()\n",
    "            sudoku_grid_masked[batch_indices, position[:, 1], position[:, 0]] = 0 # 0 is a mask token aka blank\n",
    "        \n",
    "        masked_sudoku_grid_embeddings = self.embed(sudoku_grid_masked)\n",
    "        masked_sudoku_grid_with_position = self.pe(masked_sudoku_grid_embeddings) # shape [batch, 9, 9, total_dim]\n",
    "        # Reshape grid to sequence: [batch, 81, total_dim]\n",
    "        masked_grid_seq = masked_sudoku_grid_with_position.view(batch_size, 81, self.total_dim)\n",
    "\n",
    "        grid_seq_embeddings = self.embed(sudoku_grid)\n",
    "        grid_seq_embeddings = grid_seq_embeddings.view(batch_size, 81, self.embedding_dim) \n",
    "        \n",
    "        # Query from target token: [batch, 1, total_dim]\n",
    "        # query = target_token_with_position.unsqueeze(1)\n",
    "\n",
    "        output, attention = self.mha(masked_grid_seq, return_attention=True)\n",
    "        # output is shape [batch, 81, total_dim]\n",
    "        \n",
    "        return output, attention, target_token_with_position, grid_seq_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDiffusionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for pre-generated diffusion sequences.\n",
    "    \n",
    "    Each item is a diffusion sequence of shape (82, 9, 9) where:\n",
    "    - Index 0: completely masked grid (all zeros)\n",
    "    - Index 81: completely solved grid\n",
    "    - Indices 1-80: intermediate states with progressively more cells revealed\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Tensor of shape (dataset_size, 82, 9, 9) containing diffusion sequences\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "\n",
    "class SudokuDiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion model for Sudoku puzzles inspired by DDPM.\n",
    "    \n",
    "    Forward process: Progressively mask cells from a complete sudoku (T=81) to empty grid (T=0)\n",
    "    Reverse process: Learn to predict which cells to reveal to go from T to T+1\n",
    "    \n",
    "    The model learns to reverse the masking process, predicting which cell should be revealed\n",
    "    at each timestep given the current partially revealed grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, num_layers=6, kernel_size=3, num_groups=8, \n",
    "                 embedding_layer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_timesteps = 81  # 81 cells in a sudoku grid\n",
    "        self.embedding_layer = embedding_layer\n",
    "        \n",
    "        # Determine input channels based on whether we use embeddings\n",
    "        if embedding_layer is not None:\n",
    "            # Using learned embeddings: embedding_dim channels\n",
    "            input_channels = embedding_layer.embedding_dim\n",
    "            self.use_embeddings = True\n",
    "        else:\n",
    "            # Using simple normalization: 1 channel\n",
    "            input_channels = 1\n",
    "            self.use_embeddings = False\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers for processing the sudoku grid\n",
    "        self.conv_in = nn.Conv2d(input_channels, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output: dual heads for position and value prediction\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        # Position head: which cell to reveal (81 possibilities)\n",
    "        self.position_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 81)  # Logits for 81 cells\n",
    "        )\n",
    "        \n",
    "        # Value head: what value to place (10 classes: 0-9)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 10)  # Logits for 10 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Predict which cell should be revealed next and what value to place.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, 9, 9) sudoku grids at timestep t (0 = masked, 1-9 = revealed)\n",
    "            t: (batch,) timesteps (0 to 80)\n",
    "            \n",
    "        Returns:\n",
    "            position_logits: (batch, 81) logits for which cell should be revealed next\n",
    "            value_logits: (batch, 10) logits for what value (0-9) to place\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process input: either use embeddings or simple normalization\n",
    "        if self.use_embeddings:\n",
    "            # Use learned embeddings: (batch, 9, 9) -> (batch, 9, 9, embedding_dim)\n",
    "            x_embedded = self.embedding_layer(x.long())  # (batch, 9, 9, embedding_dim)\n",
    "            x_norm = x_embedded.permute(0, 3, 1, 2)  # (batch, embedding_dim, 9, 9)\n",
    "        else:\n",
    "            # Simple normalization to [-1, 1] range\n",
    "            x_norm = (x / 4.5) - 1.0\n",
    "            x_norm = x_norm.unsqueeze(1)  # (batch, 1, 9, 9)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_norm = t.float().unsqueeze(1) / self.num_timesteps  # (batch, 1)\n",
    "        t_emb = self.time_embed(t_norm)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Process through conv layers\n",
    "        h = self.conv_in(x_norm)  # (batch, hidden_dim, 9, 9)\n",
    "        \n",
    "        # Add time embedding to spatial features\n",
    "        t_emb_spatial = t_emb.view(batch_size, -1, 1, 1).expand(-1, -1, 9, 9)\n",
    "        h = h + t_emb_spatial\n",
    "        \n",
    "        # Apply conv blocks with residual connections\n",
    "        for block in self.conv_blocks:\n",
    "            h = h + block(h)\n",
    "        \n",
    "        # Output processing\n",
    "        h = self.conv_out(h)  # (batch, hidden_dim, 9, 9)\n",
    "        h_flat = h.reshape(batch_size, -1)  # (batch, hidden_dim * 81)\n",
    "        \n",
    "        # Dual predictions\n",
    "        position_logits = self.position_head(h_flat)  # (batch, 81)\n",
    "        value_logits = self.value_head(h_flat)  # (batch, 10)\n",
    "        \n",
    "        return position_logits, value_logits\n",
    "    \n",
    "    def compute_loss(self, sequences, k_max=10):\n",
    "        \"\"\"\n",
    "        Compute the diffusion loss for training using K-step iterative prediction.\n",
    "        \n",
    "        The forward diffusion process (from sudoku.py) goes from empty (T=0) to complete (T=81).\n",
    "        We learn to predict K steps ahead by iteratively applying the model.\n",
    "        \n",
    "        Args:\n",
    "            sequences: (batch, 82, 9, 9) diffusion sequences where:\n",
    "                      - sequences[:, 0] is completely masked (all zeros)\n",
    "                      - sequences[:, 81] is completely solved\n",
    "            k_max: Maximum number of forward steps for multi-step prediction\n",
    "                      \n",
    "        Returns:\n",
    "            loss: scalar combined loss (position + value)\n",
    "            accuracy: prediction accuracy for logging\n",
    "        \"\"\"\n",
    "        batch_size = sequences.shape[0]\n",
    "        \n",
    "        # Sample random starting timestep B from [0, 81-k_max]\n",
    "        max_start = max(1, self.num_timesteps - k_max)\n",
    "        B = torch.randint(0, max_start, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Sample random K from [1, k_max]\n",
    "        K = torch.randint(1, k_max + 1, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Get starting grids at timestep B (remove unnecessary clone)\n",
    "        x_current = sequences[torch.arange(batch_size), B].float()  # (batch, 9, 9)\n",
    "        \n",
    "        # Track losses and accuracies across all K steps\n",
    "        total_position_loss = 0.0\n",
    "        total_value_loss = 0.0\n",
    "        total_position_acc = 0.0\n",
    "        total_value_acc = 0.0\n",
    "        \n",
    "        # Pre-allocate batch_indices outside loop\n",
    "        batch_indices = torch.arange(batch_size, device=self.device)\n",
    "        \n",
    "        # Iteratively predict K steps\n",
    "        for step in range(k_max):\n",
    "            # Current timestep for each batch element\n",
    "            t_current = B + step\n",
    "            \n",
    "            # Only compute loss for elements where step < K[i] and t_current < 81\n",
    "            active_mask = (step < K) & (t_current < self.num_timesteps)\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            # Get target grid at next timestep\n",
    "            t_next = torch.clamp(t_current + 1, max=self.num_timesteps)\n",
    "            x_target = sequences[batch_indices, t_next].float()  # (batch, 9, 9)\n",
    "            \n",
    "            # Find which cell was revealed (difference between current and target)\n",
    "            diff = (x_target != x_current).view(batch_size, 81)  # (batch, 81)\n",
    "            \n",
    "            # Check if there's actually a difference (cell was revealed)\n",
    "            has_diff = diff.any(dim=1)  # (batch,)\n",
    "            active_mask = active_mask & has_diff  # Only process if there's a change\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            target_position = diff.float().argmax(dim=1)  # (batch,)\n",
    "            \n",
    "            # Get target values at revealed positions (vectorized for efficiency)\n",
    "            rows = target_position // 9\n",
    "            cols = target_position % 9\n",
    "            target_values = x_target[batch_indices, rows, cols].long()\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x_current, t_current)  # (batch, 81), (batch, 10)\n",
    "            \n",
    "            # Mask out already revealed cells in position prediction (in-place operation)\n",
    "            already_revealed = (x_current.view(batch_size, 81) != 0)  # (batch, 81)\n",
    "            position_logits.masked_fill_(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Compute losses only for active batch elements\n",
    "            if active_mask.any():\n",
    "                position_loss = F.cross_entropy(position_logits[active_mask], target_position[active_mask], reduction='sum')\n",
    "                value_loss = F.cross_entropy(value_logits[active_mask], target_values[active_mask], reduction='sum')\n",
    "                \n",
    "                # Accumulate losses (keep in computation graph for backprop)\n",
    "                total_position_loss = total_position_loss + position_loss\n",
    "                total_value_loss = total_value_loss + value_loss\n",
    "                \n",
    "                # Compute accuracy for logging\n",
    "                with torch.no_grad():\n",
    "                    pred_position = position_logits[active_mask].argmax(dim=1)\n",
    "                    pred_value = value_logits[active_mask].argmax(dim=1)\n",
    "                    total_position_acc += (pred_position == target_position[active_mask]).float().sum()\n",
    "                    total_value_acc += (pred_value == target_values[active_mask]).float().sum()\n",
    "            \n",
    "            # Update x_current with ground truth for next iteration (remove unnecessary clone)\n",
    "            x_current = x_target\n",
    "        \n",
    "        # Average losses over all active predictions\n",
    "        num_predictions = K.float().sum()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if num_predictions == 0:\n",
    "            return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        total_position_loss = total_position_loss / num_predictions\n",
    "        total_value_loss = total_value_loss / num_predictions\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = total_position_loss + total_value_loss\n",
    "        \n",
    "        # Average accuracy\n",
    "        accuracy = (total_position_acc + total_value_acc) / (2 * num_predictions)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=1, return_trajectory=False):\n",
    "        \"\"\"\n",
    "        Generate sudoku puzzles by running the reverse diffusion process.\n",
    "        Start from empty grid (T=0) and progressively reveal cells to T=81.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: number of puzzles to generate\n",
    "            return_trajectory: if True, return full trajectory of generation\n",
    "            \n",
    "        Returns:\n",
    "            samples: (batch_size, 9, 9) generated sudoku grids\n",
    "            trajectory: (batch_size, 82, 9, 9) if return_trajectory=True\n",
    "        \"\"\"\n",
    "        # Start from completely masked grid (T=0)\n",
    "        x = torch.zeros(batch_size, 9, 9, device=self.device)\n",
    "        \n",
    "        if return_trajectory:\n",
    "            trajectory = torch.zeros(batch_size, 82, 9, 9, device=self.device)\n",
    "            trajectory[:, 0] = x\n",
    "        \n",
    "        # Progressively reveal cells using model predictions\n",
    "        for t in tqdm(range(self.num_timesteps), desc='Sampling'):\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x, t_batch)\n",
    "            \n",
    "            # Mask out already revealed cells\n",
    "            already_revealed = (x.view(batch_size, 81) != 0)\n",
    "            position_logits = position_logits.masked_fill(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Sample or take argmax for position\n",
    "            position_probs = F.softmax(position_logits, dim=-1)\n",
    "            cell_idx = torch.multinomial(position_probs, 1).squeeze(-1)  # (batch,)\n",
    "            \n",
    "            # Take argmax for value (deterministic)\n",
    "            value_probs = F.softmax(value_logits, dim=-1)\n",
    "            values = torch.argmax(value_probs, dim=-1)  # (batch,)\n",
    "            \n",
    "            # Update grid with predicted values\n",
    "            for b in range(batch_size):\n",
    "                idx = cell_idx[b].item()\n",
    "                row = idx // 9\n",
    "                col = idx % 9\n",
    "                x[b, row, col] = values[b]\n",
    "            \n",
    "            if return_trajectory:\n",
    "                trajectory[:, t + 1] = x\n",
    "        \n",
    "        if return_trajectory:\n",
    "            return x.long(), trajectory.long()\n",
    "        return x.long()\n",
    "\n",
    "\n",
    "def train_sudoku_diffusion(model, dataset, num_epochs=1000, batch_size=32, lr=1e-4, \n",
    "                           weight_decay=1e-4, grad_clip_max_norm=1.0,\n",
    "                           log_interval=10, eval_interval=100, k_max=10, device='cuda',\n",
    "                           use_wandb=False, wandb_project=\"sudoku-diffusion\", wandb_entity=None,\n",
    "                           checkpoint_dir=\"./checkpoints\", checkpoint_interval=50, \n",
    "                           resume_from=None):\n",
    "    \"\"\"\n",
    "    Train the sudoku diffusion model with proper logging and performance optimizations.\n",
    "    \n",
    "    Args:\n",
    "        model: SudokuDiffusionModel instance\n",
    "        dataset: SudokuDiffusionDataset instance with pre-generated sequences\n",
    "        num_epochs: number of training epochs\n",
    "        batch_size: batch size for training\n",
    "        lr: learning rate\n",
    "        weight_decay: weight decay for AdamW optimizer\n",
    "        grad_clip_max_norm: max norm for gradient clipping\n",
    "        log_interval: log metrics every N epochs\n",
    "        eval_interval: evaluate and sample every N epochs\n",
    "        k_max: maximum number of forward steps for multi-step prediction\n",
    "        device: device to train on\n",
    "        use_wandb: whether to log to Weights & Biases\n",
    "        wandb_project: wandb project name\n",
    "        wandb_entity: wandb entity (None = default)\n",
    "        checkpoint_dir: directory to save checkpoints\n",
    "        checkpoint_interval: save checkpoint every N epochs\n",
    "        resume_from: path to checkpoint to resume from (None = start fresh)\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Create DataLoader for batching and shuffling with optimizations\n",
    "    # Note: pin_memory should be False when data is already on GPU\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Keep 0 for GPU tensors\n",
    "        pin_memory=False,  # Data is already on GPU, no need to pin\n",
    "        persistent_workers=False  # No workers, so this doesn't apply\n",
    "    )\n",
    "    \n",
    "    # Use fused optimizer for faster updates (requires CUDA)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay,\n",
    "        fused=True if device == 'cuda' else False\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    # Starting epoch\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from is not None and os.path.exists(resume_from):\n",
    "        print(f\"Loading checkpoint from {resume_from}...\")\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epoch_losses = checkpoint.get('epoch_losses', [])\n",
    "        epoch_accuracies = checkpoint.get('epoch_accuracies', [])\n",
    "        print(f\"âœ“ Resumed from epoch {start_epoch}\")\n",
    "        print(f\"  Previous best loss: {min(epoch_losses) if epoch_losses else 'N/A'}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if use_wandb:\n",
    "        wandb_config = {\n",
    "            \"hidden_dim\": model.conv_in.out_channels,\n",
    "            \"num_layers\": len(model.conv_blocks),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"k_max\": k_max,\n",
    "            \"dataset_size\": len(dataset),\n",
    "        }\n",
    "        if start_epoch == 0:\n",
    "            wandb.init(project=wandb_project, entity=wandb_entity, config=wandb_config)\n",
    "        else:\n",
    "            # Resume wandb run if checkpoint has run_id\n",
    "            run_id = checkpoint.get('wandb_run_id', None)\n",
    "            wandb.init(project=wandb_project, entity=wandb_entity, config=wandb_config, \n",
    "                      id=run_id, resume=\"allow\")\n",
    "        print(f\"âœ“ Wandb initialized: {wandb.run.name}\")\n",
    "    \n",
    "    model.train()\n",
    "    if start_epoch == 0:\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    else:\n",
    "        print(f\"Resuming training from epoch {start_epoch} to {num_epochs}...\")\n",
    "    print(f\"Dataset size: {len(dataset)}, Batch size: {batch_size}, Batches per epoch: {len(dataloader)}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Using mixed precision: {scaler is not None}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Iterate through batches in the dataset\n",
    "        for batch_sequences in dataloader:\n",
    "            # Mixed precision training\n",
    "            if scaler is not None:\n",
    "                # Forward pass with autocast\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training (CPU or non-CUDA)\n",
    "                loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics over all batches in the epoch\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        avg_epoch_acc = epoch_acc / num_batches\n",
    "        \n",
    "        # Record metrics\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_accuracies.append(avg_epoch_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train/loss\": avg_epoch_loss,\n",
    "                \"train/accuracy\": avg_epoch_acc,\n",
    "                \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "            })\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch + 1:4d}/{num_epochs} | \"\n",
    "                  f\"Loss: {avg_epoch_loss:.4f} | \"\n",
    "                  f\"Acc: {avg_epoch_acc:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Evaluation and sampling\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Evaluation at epoch {epoch + 1}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Generate a sample\n",
    "                sample = model.sample(batch_size=1)\n",
    "                print(\"\\nGenerated Sudoku:\")\n",
    "                print(sample[0].cpu().numpy())\n",
    "                \n",
    "                # Check if valid\n",
    "                sudoku_obj = Sudoku(sample[0].cpu().numpy(), backend='numpy')\n",
    "                is_valid = sudoku_obj.is_valid()\n",
    "                print(f\"\\nIs valid: {is_valid}\")\n",
    "                \n",
    "                # Log to wandb\n",
    "                if use_wandb:\n",
    "                    wandb.log({\n",
    "                        \"eval/is_valid\": int(is_valid),\n",
    "                        \"eval/sample\": wandb.Table(\n",
    "                            data=[[str(sample[0].cpu().numpy())]], \n",
    "                            columns=[\"sudoku_grid\"]\n",
    "                        )\n",
    "                    })\n",
    "            \n",
    "            model.train()\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == num_epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch_losses': epoch_losses,\n",
    "                'epoch_accuracies': epoch_accuracies,\n",
    "                'config': {\n",
    "                    'batch_size': batch_size,\n",
    "                    'learning_rate': lr,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'k_max': k_max,\n",
    "                }\n",
    "            }\n",
    "            if scaler is not None:\n",
    "                checkpoint_data['scaler_state_dict'] = scaler.state_dict()\n",
    "            if use_wandb:\n",
    "                checkpoint_data['wandb_run_id'] = wandb.run.id\n",
    "            \n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            print(f\"âœ“ Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            # Also save as \"latest\" for easy resumption\n",
    "            latest_path = os.path.join(checkpoint_dir, \"checkpoint_latest.pt\")\n",
    "            torch.save(checkpoint_data, latest_path)\n",
    "            print(f\"âœ“ Latest checkpoint updated: {latest_path}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "        print(\"âœ“ Wandb run finished\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(epoch_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epoch_accuracies)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, epoch_losses, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c719fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "  Device: NVIDIA RTX A4000\n",
      "  Total Memory: 15.63 GB\n",
      "\n",
      "Initial GPU Memory Usage:\n",
      "  Allocated: 0.54 GB\n",
      "  Cached: 3.44 GB\n",
      "\n",
      "âš ï¸  Clearing GPU memory...\n",
      "\n",
      "After clearing:\n",
      "  Allocated: 0.54 GB\n",
      "  Cached: 0.58 GB\n",
      "  Free: 15.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU information and clear memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Information:\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nInitial GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Aggressively clear GPU memory\n",
    "    print(\"\\nâš ï¸  Clearing GPU memory...\")\n",
    "    \n",
    "    # Delete all variables in the current namespace that might hold GPU tensors\n",
    "    if 'model' in dir():\n",
    "        del model\n",
    "    if 'generator' in dir():\n",
    "        del generator\n",
    "    if 'sequences' in dir():\n",
    "        del sequences\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    \n",
    "    print(f\"\\nAfter clearing:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # If still not enough memory, suggest kernel restart\n",
    "    free_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3\n",
    "    if free_memory < 1.0:\n",
    "        print(\"\\nâš ï¸  WARNING: Very little GPU memory available!\")\n",
    "        print(\"   Consider: Kernel -> Restart Kernel to fully clear GPU memory\")\n",
    "else:\n",
    "    print(\"CUDA not available, will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c4f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GPU Memory before setup:\n",
      "  Allocated: 0.54 GB\n",
      "  Reserved: 0.58 GB\n",
      "  Free: 15.05 GB\n",
      "\n",
      "============================================================\n",
      "Loading learned embeddings from LLM model...\n",
      "============================================================\n",
      "Model loaded from ./sudoku2vec_trained_model.pt\n",
      "Configuration: vocab_size=10, embedding_dim=15, attention_dim=9, num_heads=9\n",
      "âœ“ Successfully loaded embedding layer with 10 tokens\n",
      "  Embedding dimension: 15\n",
      "\n",
      "âœ“ Embedding layer ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: LOAD EMBEDDING MODEL\n",
    "# ============================================================================\n",
    "# Run this cell once per session to load the pre-trained Sudoku2Vec embeddings.\n",
    "# This is fast (~1 second) and only needs to run once.\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Clear GPU memory if using CUDA\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nGPU Memory before setup:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Load embedding model if configured\n",
    "embedding_layer = None\n",
    "if USE_LEARNED_EMBEDDINGS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Loading learned embeddings from LLM model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    try:\n",
    "        sudoku2vec_model = Sudoku2Vec.load_model(EMBEDDING_MODEL_PATH, device=DEVICE)\n",
    "        embedding_layer = sudoku2vec_model.embed\n",
    "        print(f\"âœ“ Successfully loaded embedding layer with {embedding_layer.num_embeddings} tokens\")\n",
    "        print(f\"  Embedding dimension: {embedding_layer.embedding_dim}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸  WARNING: Embedding model not found at {EMBEDDING_MODEL_PATH}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  WARNING: Failed to load embedding model: {e}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "else:\n",
    "    print(\"\\nUsing simple normalization (no learned embeddings)\")\n",
    "\n",
    "print(\"\\nâœ“ Embedding layer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55ecb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Pre-generating training dataset...\n",
      "============================================================\n",
      "Generating 20000 diffusion sequences...\n",
      "âœ“ Dataset generated in 262.51 seconds (13.13 ms per sequence)\n",
      "\n",
      "Dataset statistics:\n",
      "  Shape: torch.Size([20000, 82, 9, 9])\n",
      "  Memory: 506.74 MB\n",
      "  Device: cuda:0\n",
      "âœ“ Created SudokuDiffusionDataset with 20000 sequences\n",
      "\n",
      "ðŸ’¡ Dataset is ready! You can now rerun Cells 7 & 8 with different hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: GENERATE TRAINING DATASET\n",
    "# ============================================================================\n",
    "# Run this cell ONCE to generate the training dataset.\n",
    "# This is SLOW (~4 minutes for 20k sequences) but you only need to run it once!\n",
    "# \n",
    "# After running this cell, you can:\n",
    "# - Rerun Cell 7 to try different model architectures\n",
    "# - Rerun Cell 8 to try different training hyperparameters\n",
    "# - All without regenerating this expensive dataset!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Pre-generating training dataset...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = SudokuGenerator(backend='torch', device=DEVICE)\n",
    "\n",
    "# Generate diffusion sequences\n",
    "print(f\"Generating {DATASET_SIZE} diffusion sequences...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sequences = generator.generate_diffusion_sequence(size=DATASET_SIZE)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"âœ“ Dataset generated in {generation_time:.2f} seconds ({generation_time/DATASET_SIZE*1000:.2f} ms per sequence)\")\n",
    "\n",
    "# Report dataset statistics\n",
    "sequence_shape = sequences.shape\n",
    "memory_mb = sequences.element_size() * sequences.nelement() / (1024 ** 2)\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Shape: {sequence_shape}\")\n",
    "print(f\"  Memory: {memory_mb:.2f} MB\")\n",
    "print(f\"  Device: {sequences.device}\")\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "train_dataset = SudokuDiffusionDataset(sequences)\n",
    "print(f\"âœ“ Created SudokuDiffusionDataset with {len(train_dataset)} sequences\")\n",
    "print(f\"\\nðŸ’¡ Dataset is ready! You can now rerun Cells 7 & 8 with different hyperparameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8393681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Initializing Diffusion Model...\n",
      "============================================================\n",
      "âœ“ Model initialized successfully\n",
      "  Total parameters: 1,386,673\n",
      "  Trainable parameters: 1,386,673\n",
      "  Using embeddings: True\n",
      "\n",
      "âš¡ Compiling model with torch.compile for faster training...\n",
      "âœ“ Model compiled successfully\n",
      "\n",
      "GPU Memory after model loading:\n",
      "  Allocated: 1.04 GB\n",
      "  Reserved: 1.07 GB\n",
      "\n",
      "âœ“ Model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: INITIALIZE DIFFUSION MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to create and compile the diffusion model.\n",
    "# This is FAST (~5 seconds) and you can rerun it to experiment with:\n",
    "# - Different model sizes (HIDDEN_DIM, NUM_LAYERS)\n",
    "# - Different architectures (KERNEL_SIZE, NUM_GROUPS)\n",
    "# \n",
    "# The dataset from Cell 6 will be reused!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Initializing Diffusion Model...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model = SudokuDiffusionModel(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    num_groups=NUM_GROUPS,\n",
    "    embedding_layer=embedding_layer,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"âœ“ Model initialized successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Using embeddings: {model.use_embeddings}\")\n",
    "\n",
    "# Compile model for faster training (PyTorch 2.0+)\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\nâš¡ Compiling model with torch.compile for faster training...\")\n",
    "    try:\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "        print(f\"âœ“ Model compiled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not compile model: {e}\")\n",
    "        print(f\"   Continuing without compilation\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\nGPU Memory after model loading:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\nâœ“ Model is ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training...\n",
      "============================================================\n",
      "Starting training for 4000 epochs...\n",
      "Dataset size: 20000, Batch size: 1024, Batches per epoch: 20\n",
      "Learning rate: 0.001\n",
      "Using mixed precision: True\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_727013/3607855665.py:347: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
      "/tmp/ipykernel_727013/3607855665.py:372: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   10/4000 | Loss: 5.7406 | Acc: 0.0849 | LR: 0.001000\n",
      "Epoch   20/4000 | Loss: 5.7233 | Acc: 0.0941 | LR: 0.001000\n",
      "Epoch   30/4000 | Loss: 5.7025 | Acc: 0.1001 | LR: 0.001000\n",
      "Epoch   40/4000 | Loss: 5.7024 | Acc: 0.1018 | LR: 0.001000\n",
      "Epoch   50/4000 | Loss: 5.6892 | Acc: 0.1047 | LR: 0.001000\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 50\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 304.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 8 6 9 6 9 1 1 9]\n",
      " [2 3 1 7 3 6 7 1 7]\n",
      " [5 7 5 5 1 6 4 1 6]\n",
      " [7 5 4 8 6 8 9 6 5]\n",
      " [3 7 9 4 5 7 7 3 6]\n",
      " [7 7 1 3 3 7 8 9 6]\n",
      " [9 4 8 2 5 2 4 2 3]\n",
      " [6 4 3 6 3 4 6 5 8]\n",
      " [2 9 4 7 1 5 8 2 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch   60/4000 | Loss: 5.6712 | Acc: 0.1076 | LR: 0.000999\n",
      "Epoch   70/4000 | Loss: 5.6674 | Acc: 0.1077 | LR: 0.000999\n",
      "Epoch   80/4000 | Loss: 5.6630 | Acc: 0.1081 | LR: 0.000999\n",
      "Epoch   90/4000 | Loss: 5.6691 | Acc: 0.1079 | LR: 0.000999\n",
      "Epoch  100/4000 | Loss: 5.6532 | Acc: 0.1098 | LR: 0.000998\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 100\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 322.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 2 1 9 6 6 6 2 7]\n",
      " [6 3 6 4 1 1 4 5 1]\n",
      " [8 4 6 8 3 2 5 3 7]\n",
      " [6 3 1 7 2 5 7 5 5]\n",
      " [5 7 4 7 8 5 8 1 5]\n",
      " [6 9 9 7 2 4 6 3 6]\n",
      " [1 7 4 1 2 8 9 5 3]\n",
      " [7 8 9 8 9 4 9 9 2]\n",
      " [2 3 6 9 8 3 5 3 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  110/4000 | Loss: 5.6550 | Acc: 0.1084 | LR: 0.000998\n",
      "Epoch  120/4000 | Loss: 5.6514 | Acc: 0.1083 | LR: 0.000998\n",
      "Epoch  130/4000 | Loss: 5.6371 | Acc: 0.1113 | LR: 0.000997\n",
      "Epoch  140/4000 | Loss: 5.6469 | Acc: 0.1091 | LR: 0.000997\n",
      "Epoch  150/4000 | Loss: 5.6456 | Acc: 0.1097 | LR: 0.000997\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 150\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 310.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 9 3 9 7 3 8 1 1]\n",
      " [2 4 3 3 7 1 8 5 7]\n",
      " [5 2 3 7 1 4 9 1 6]\n",
      " [3 2 8 5 5 5 2 6 3]\n",
      " [8 1 6 5 7 2 7 9 4]\n",
      " [3 6 4 8 2 2 9 6 8]\n",
      " [5 8 2 6 9 1 4 7 1]\n",
      " [4 2 7 9 9 6 8 4 6]\n",
      " [5 6 4 9 4 3 5 8 4]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  160/4000 | Loss: 5.6405 | Acc: 0.1115 | LR: 0.000996\n",
      "Epoch  170/4000 | Loss: 5.6294 | Acc: 0.1121 | LR: 0.000996\n",
      "Epoch  180/4000 | Loss: 5.6372 | Acc: 0.1119 | LR: 0.000995\n",
      "Epoch  190/4000 | Loss: 5.6313 | Acc: 0.1126 | LR: 0.000994\n",
      "Epoch  200/4000 | Loss: 5.6402 | Acc: 0.1120 | LR: 0.000994\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 200\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 314.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 1 9 1 4 9 7 9 6]\n",
      " [6 6 5 6 1 9 2 9 6]\n",
      " [7 4 7 9 8 2 8 4 1]\n",
      " [4 5 7 8 6 9 3 2 7]\n",
      " [1 3 8 4 8 7 8 5 7]\n",
      " [4 5 5 8 9 2 4 4 1]\n",
      " [7 6 5 2 5 7 3 5 6]\n",
      " [3 1 3 2 2 7 3 6 8]\n",
      " [1 5 3 3 1 2 1 6 3]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  210/4000 | Loss: 5.6367 | Acc: 0.1126 | LR: 0.000993\n",
      "Epoch  220/4000 | Loss: 5.6234 | Acc: 0.1144 | LR: 0.000993\n",
      "Epoch  230/4000 | Loss: 5.6316 | Acc: 0.1134 | LR: 0.000992\n",
      "Epoch  240/4000 | Loss: 5.6276 | Acc: 0.1158 | LR: 0.000991\n",
      "Epoch  250/4000 | Loss: 5.6203 | Acc: 0.1162 | LR: 0.000990\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 250\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 311.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[9 2 7 5 1 3 9 1 9]\n",
      " [8 2 5 7 3 3 4 3 4]\n",
      " [3 6 8 8 9 1 2 6 4]\n",
      " [1 1 8 5 5 1 3 8 5]\n",
      " [5 5 4 6 7 1 4 2 9]\n",
      " [6 2 8 3 3 9 2 3 8]\n",
      " [6 4 7 2 4 6 6 7 8]\n",
      " [7 9 9 2 1 4 4 7 1]\n",
      " [5 9 7 2 7 8 6 9 5]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  260/4000 | Loss: 5.6331 | Acc: 0.1162 | LR: 0.000990\n",
      "Epoch  270/4000 | Loss: 5.6262 | Acc: 0.1164 | LR: 0.000989\n",
      "Epoch  280/4000 | Loss: 5.6305 | Acc: 0.1192 | LR: 0.000988\n",
      "Epoch  290/4000 | Loss: 5.6133 | Acc: 0.1195 | LR: 0.000987\n",
      "Epoch  300/4000 | Loss: 5.6106 | Acc: 0.1209 | LR: 0.000986\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 300\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 314.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[8 7 3 3 4 8 7 1 1]\n",
      " [6 4 2 1 3 1 4 9 1]\n",
      " [2 5 8 9 3 7 4 6 8]\n",
      " [5 8 8 2 1 2 3 7 8]\n",
      " [6 7 9 2 4 9 4 1 6]\n",
      " [6 3 5 6 4 5 6 2 7]\n",
      " [5 2 4 3 5 9 6 9 7]\n",
      " [7 9 5 8 9 5 8 5 7]\n",
      " [3 1 1 6 1 2 7 3 2]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  310/4000 | Loss: 5.6196 | Acc: 0.1205 | LR: 0.000985\n",
      "Epoch  320/4000 | Loss: 5.6047 | Acc: 0.1226 | LR: 0.000984\n",
      "Epoch  330/4000 | Loss: 5.5877 | Acc: 0.1244 | LR: 0.000983\n",
      "Epoch  340/4000 | Loss: 5.5990 | Acc: 0.1250 | LR: 0.000982\n",
      "Epoch  350/4000 | Loss: 5.5935 | Acc: 0.1259 | LR: 0.000981\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 350\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 316.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 3 4 7 8 4 9 9 9]\n",
      " [3 2 5 8 5 7 3 4 6]\n",
      " [3 7 9 6 1 8 7 3 6]\n",
      " [8 6 4 2 3 1 2 1 1]\n",
      " [7 9 7 8 2 8 4 3 9]\n",
      " [4 7 4 1 2 5 2 5 7]\n",
      " [5 7 4 6 8 5 8 5 1]\n",
      " [1 6 3 8 2 5 9 1 9]\n",
      " [1 9 2 5 2 6 6 3 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  360/4000 | Loss: 5.5967 | Acc: 0.1265 | LR: 0.000980\n",
      "Epoch  370/4000 | Loss: 5.5826 | Acc: 0.1265 | LR: 0.000979\n",
      "Epoch  380/4000 | Loss: 5.5867 | Acc: 0.1271 | LR: 0.000978\n",
      "Epoch  390/4000 | Loss: 5.5739 | Acc: 0.1296 | LR: 0.000977\n",
      "Epoch  400/4000 | Loss: 5.5669 | Acc: 0.1308 | LR: 0.000976\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 400\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 313.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 9 1 4 7 4 6 5 2]\n",
      " [1 7 3 9 1 3 6 2 4]\n",
      " [2 2 7 8 8 8 8 5 6]\n",
      " [1 9 9 3 2 4 6 7 2]\n",
      " [5 2 1 2 5 9 8 6 2]\n",
      " [3 6 5 7 7 3 1 5 3]\n",
      " [5 5 4 8 4 7 9 1 4]\n",
      " [3 5 3 3 1 6 6 4 8]\n",
      " [6 1 9 5 7 2 9 8 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  410/4000 | Loss: 5.5813 | Acc: 0.1335 | LR: 0.000974\n",
      "Epoch  420/4000 | Loss: 5.5682 | Acc: 0.1323 | LR: 0.000973\n",
      "Epoch  430/4000 | Loss: 5.5570 | Acc: 0.1340 | LR: 0.000972\n",
      "Epoch  440/4000 | Loss: 5.5390 | Acc: 0.1354 | LR: 0.000970\n",
      "Epoch  450/4000 | Loss: 5.5479 | Acc: 0.1373 | LR: 0.000969\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 450\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 318.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 1 2 4 1 9 8 6 2]\n",
      " [1 2 7 2 3 4 5 5 1]\n",
      " [7 8 1 2 9 3 9 1 7]\n",
      " [5 3 3 5 8 7 7 7 6]\n",
      " [8 2 6 8 3 6 5 2 1]\n",
      " [5 4 4 9 6 5 4 3 4]\n",
      " [8 3 9 5 7 5 6 5 1]\n",
      " [6 9 1 2 9 9 4 4 8]\n",
      " [2 1 3 7 6 3 9 6 2]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  460/4000 | Loss: 5.5398 | Acc: 0.1401 | LR: 0.000968\n",
      "Epoch  470/4000 | Loss: 5.5263 | Acc: 0.1392 | LR: 0.000966\n",
      "Epoch  480/4000 | Loss: 5.5302 | Acc: 0.1402 | LR: 0.000965\n",
      "Epoch  490/4000 | Loss: 5.5264 | Acc: 0.1421 | LR: 0.000963\n",
      "Epoch  500/4000 | Loss: 5.5039 | Acc: 0.1449 | LR: 0.000962\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 500\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 323.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[8 2 1 7 5 7 5 3 7]\n",
      " [5 1 5 2 7 3 2 5 3]\n",
      " [5 4 2 9 1 8 4 5 2]\n",
      " [2 8 9 8 3 9 7 6 9]\n",
      " [7 6 4 4 8 7 9 3 6]\n",
      " [2 1 9 9 7 5 9 5 2]\n",
      " [6 6 1 1 3 4 8 6 6]\n",
      " [9 1 4 4 3 8 3 2 4]\n",
      " [7 7 3 4 1 8 6 8 6]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  510/4000 | Loss: 5.5139 | Acc: 0.1454 | LR: 0.000960\n",
      "Epoch  520/4000 | Loss: 5.5264 | Acc: 0.1457 | LR: 0.000959\n",
      "Epoch  530/4000 | Loss: 5.5002 | Acc: 0.1472 | LR: 0.000957\n",
      "Epoch  540/4000 | Loss: 5.4960 | Acc: 0.1499 | LR: 0.000956\n",
      "Epoch  550/4000 | Loss: 5.4949 | Acc: 0.1516 | LR: 0.000954\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 550\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 320.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[8 3 5 3 9 2 2 7 6]\n",
      " [8 4 8 7 3 2 7 2 1]\n",
      " [8 9 6 1 4 2 3 8 2]\n",
      " [9 3 5 6 7 9 8 5 2]\n",
      " [1 6 9 9 4 7 4 5 6]\n",
      " [8 7 9 4 3 7 4 5 9]\n",
      " [5 1 6 8 3 9 1 2 7]\n",
      " [1 4 7 1 1 8 2 5 6]\n",
      " [8 5 6 6 1 3 2 3 5]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  560/4000 | Loss: 5.4898 | Acc: 0.1514 | LR: 0.000952\n",
      "Epoch  570/4000 | Loss: 5.4803 | Acc: 0.1515 | LR: 0.000951\n",
      "Epoch  580/4000 | Loss: 5.4719 | Acc: 0.1547 | LR: 0.000949\n",
      "Epoch  590/4000 | Loss: 5.4818 | Acc: 0.1531 | LR: 0.000947\n",
      "Epoch  600/4000 | Loss: 5.4679 | Acc: 0.1559 | LR: 0.000946\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 600\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 326.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 9 4 9 6 1 3 5 3]\n",
      " [7 2 2 7 4 7 7 4 5]\n",
      " [1 8 6 2 1 5 6 7 7]\n",
      " [5 4 5 4 9 8 9 3 1]\n",
      " [4 7 8 9 2 1 6 3 1]\n",
      " [3 3 1 5 8 4 5 8 2]\n",
      " [6 7 6 1 2 9 6 6 2]\n",
      " [9 2 3 8 9 3 8 2 8]\n",
      " [5 9 9 1 4 3 1 7 5]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  610/4000 | Loss: 5.4717 | Acc: 0.1544 | LR: 0.000944\n",
      "Epoch  620/4000 | Loss: 5.4517 | Acc: 0.1583 | LR: 0.000942\n",
      "Epoch  630/4000 | Loss: 5.4586 | Acc: 0.1585 | LR: 0.000940\n",
      "Epoch  640/4000 | Loss: 5.4290 | Acc: 0.1621 | LR: 0.000938\n",
      "Epoch  650/4000 | Loss: 5.4496 | Acc: 0.1609 | LR: 0.000936\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 650\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 318.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 5 2 4 1 9 2 6 8]\n",
      " [5 9 7 7 4 6 3 5 9]\n",
      " [2 5 1 8 8 3 3 3 9]\n",
      " [3 1 8 2 9 2 6 9 9]\n",
      " [8 5 8 1 6 4 5 8 6]\n",
      " [9 6 3 7 7 8 7 5 2]\n",
      " [4 2 2 1 7 4 6 9 1]\n",
      " [7 5 1 4 3 5 8 4 4]\n",
      " [6 2 3 7 1 2 7 9 4]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  660/4000 | Loss: 5.4484 | Acc: 0.1612 | LR: 0.000934\n",
      "Epoch  670/4000 | Loss: 5.4248 | Acc: 0.1646 | LR: 0.000932\n",
      "Epoch  680/4000 | Loss: 5.4293 | Acc: 0.1641 | LR: 0.000930\n",
      "Epoch  690/4000 | Loss: 5.4130 | Acc: 0.1671 | LR: 0.000928\n",
      "Epoch  700/4000 | Loss: 5.4116 | Acc: 0.1672 | LR: 0.000926\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 700\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 314.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 5 7 4 1 9 6 5 8]\n",
      " [6 7 3 1 4 8 2 9 7]\n",
      " [4 5 2 5 5 6 9 1 6]\n",
      " [6 1 3 6 6 5 1 9 8]\n",
      " [4 4 4 2 3 8 9 8 9]\n",
      " [3 5 2 7 8 4 3 2 7]\n",
      " [6 6 7 7 2 5 3 8 1]\n",
      " [8 9 4 7 3 3 2 9 2]\n",
      " [9 8 2 1 1 3 7 6 5]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  710/4000 | Loss: 5.4185 | Acc: 0.1676 | LR: 0.000924\n",
      "Epoch  720/4000 | Loss: 5.4100 | Acc: 0.1694 | LR: 0.000922\n",
      "Epoch  730/4000 | Loss: 5.3988 | Acc: 0.1716 | LR: 0.000920\n",
      "Epoch  740/4000 | Loss: 5.3887 | Acc: 0.1720 | LR: 0.000918\n",
      "Epoch  750/4000 | Loss: 5.3824 | Acc: 0.1743 | LR: 0.000916\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 750\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 335.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[8 5 4 1 4 4 3 9 4]\n",
      " [1 4 1 6 5 7 1 5 9]\n",
      " [3 4 1 5 6 2 9 6 8]\n",
      " [9 3 9 7 2 7 6 4 7]\n",
      " [1 5 2 3 6 8 2 9 7]\n",
      " [9 8 5 8 5 3 4 2 2]\n",
      " [3 3 5 7 7 8 4 7 1]\n",
      " [2 1 6 5 8 2 2 6 8]\n",
      " [7 6 1 6 9 3 8 9 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  760/4000 | Loss: 5.3986 | Acc: 0.1725 | LR: 0.000914\n",
      "Epoch  770/4000 | Loss: 5.3785 | Acc: 0.1745 | LR: 0.000911\n",
      "Epoch  780/4000 | Loss: 5.3683 | Acc: 0.1759 | LR: 0.000909\n",
      "Epoch  790/4000 | Loss: 5.3612 | Acc: 0.1780 | LR: 0.000907\n",
      "Epoch  800/4000 | Loss: 5.3448 | Acc: 0.1788 | LR: 0.000905\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 800\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 318.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 8 1 5 2 6 1 4 6]\n",
      " [6 2 4 5 6 3 8 7 2]\n",
      " [6 8 5 9 6 4 4 8 5]\n",
      " [2 6 7 8 2 9 5 2 2]\n",
      " [9 9 5 1 9 1 7 3 5]\n",
      " [4 6 3 4 3 7 9 5 7]\n",
      " [1 9 3 5 1 7 2 1 7]\n",
      " [9 7 3 4 5 4 2 8 6]\n",
      " [3 9 8 8 2 1 8 6 1]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  810/4000 | Loss: 5.3633 | Acc: 0.1778 | LR: 0.000902\n",
      "Epoch  820/4000 | Loss: 5.3515 | Acc: 0.1790 | LR: 0.000900\n",
      "Epoch  830/4000 | Loss: 5.3582 | Acc: 0.1796 | LR: 0.000897\n",
      "Epoch  840/4000 | Loss: 5.3494 | Acc: 0.1806 | LR: 0.000895\n",
      "Epoch  850/4000 | Loss: 5.3457 | Acc: 0.1806 | LR: 0.000893\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 850\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:00<00:00, 328.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 9 3 8 7 5 8 7 5]\n",
      " [2 4 7 2 3 2 7 5 2]\n",
      " [7 3 6 1 4 2 6 9 2]\n",
      " [9 6 7 1 6 9 5 4 4]\n",
      " [9 2 5 7 2 4 6 4 7]\n",
      " [1 1 6 1 4 9 6 5 8]\n",
      " [3 2 6 3 8 7 5 8 3]\n",
      " [3 4 8 9 8 8 9 1 5]\n",
      " [3 4 9 1 3 5 8 1 1]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to train the model with the current hyperparameters.\n",
    "# You can rerun this cell to experiment with different:\n",
    "# - Learning rates (LEARNING_RATE)\n",
    "# - Batch sizes (BATCH_SIZE)\n",
    "# - Training strategies (K_MAX, WEIGHT_DECAY, GRAD_CLIP_MAX_NORM)\n",
    "# - Logging intervals (LOG_INTERVAL, EVAL_INTERVAL)\n",
    "# \n",
    "# The model from Cell 7 and dataset from Cell 6 will be used!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Training...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model, losses, accuracies = train_sudoku_diffusion(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    grad_clip_max_norm=GRAD_CLIP_MAX_NORM,\n",
    "    log_interval=LOG_INTERVAL,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    k_max=K_MAX,\n",
    "    device=DEVICE,\n",
    "    use_wandb=USE_WANDB,\n",
    "    wandb_project=WANDB_PROJECT,\n",
    "    wandb_entity=WANDB_ENTITY,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "    resume_from=RESUME_FROM_CHECKPOINT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584306e",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# RESUMING TRAINING\n",
    "# ============================================================================\n",
    "# To resume training from a checkpoint:\n",
    "# \n",
    "# 1. Set RESUME_FROM_CHECKPOINT to the path of your checkpoint:\n",
    "#    RESUME_FROM_CHECKPOINT = \"./checkpoints/checkpoint_latest.pt\"\n",
    "#    or\n",
    "#    RESUME_FROM_CHECKPOINT = \"./checkpoints/checkpoint_epoch_500.pt\"\n",
    "# \n",
    "# 2. Rerun cells 7 & 8 (model initialization and training)\n",
    "# \n",
    "# The training will automatically:\n",
    "# - Load the model weights from the checkpoint\n",
    "# - Load the optimizer state\n",
    "# - Resume from the saved epoch\n",
    "# - Continue logging to the same wandb run (if using wandb)\n",
    "# \n",
    "# Your checkpoints are saved in: ./checkpoints/\n",
    "# - checkpoint_latest.pt: Always contains the most recent checkpoint\n",
    "# - checkpoint_epoch_N.pt: Checkpoints saved at specific epochs\n",
    "# \n",
    "# Example workflow after SSH disconnection:\n",
    "# 1. Reconnect to SSH\n",
    "# 2. Open this notebook\n",
    "# 3. Run cells 1-2 (config and imports)\n",
    "# 4. Set: RESUME_FROM_CHECKPOINT = \"./checkpoints/checkpoint_latest.pt\"\n",
    "# 5. Run cells 4-7 (load embeddings, dataset, initialize model)\n",
    "# 6. Run cell 8 (training will resume from checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c56309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
