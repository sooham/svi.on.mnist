{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f4a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sudoku import SudokuGenerator, Sudoku\n",
    "import math\n",
    "import subprocess\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Model: hidden_dim=20, num_layers=9, kernel_size=3\n",
      "  Embeddings: use_learned=True, embedding_dim=15\n",
      "  Training: dataset_size=20000, epochs=3000, batch_size=1024, lr=0.001\n",
      "  Diffusion: k_max=1\n",
      "  Device: cuda\n",
      "\n",
      "‚ö†Ô∏è  Memory optimizations applied:\n",
      "  - Reduced BATCH_SIZE to 1024 (from 1024)\n",
      "  - Reduced K_MAX to 1 (from 10)\n",
      "  - Reduced DATASET_SIZE to 20000 (from 10000)\n",
      "  - Added memory cleanup in training loop\n",
      "\n",
      "‚ö° Performance optimizations enabled:\n",
      "  - torch.compile() for JIT compilation (2-3x speedup)\n",
      "  - Mixed precision training (AMP) (1.5-2x speedup)\n",
      "  - Fused AdamW optimizer\n",
      "  - Optimized compute_loss() (removed unnecessary clones/ops)\n",
      "  - Expected combined speedup: 3-6x faster training\n",
      "\n",
      "üí° If you still get OOM errors, restart the kernel first!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "# Modify these parameters to experiment with different model configurations\n",
    "\n",
    "# --- Model Architecture Parameters ---\n",
    "HIDDEN_DIM = 30              # Network width (default: 256, reduced for memory)\n",
    "NUM_LAYERS = 9                # Number of residual conv blocks (default: 6, reduced for memory)\n",
    "KERNEL_SIZE = 3               # Conv kernel size\n",
    "NUM_GROUPS = 10                # GroupNorm groups (must divide HIDDEN_DIM evenly)\n",
    "NUM_TIMESTEPS = 81            # Fixed at 81 for Sudoku (9x9 grid)\n",
    "\n",
    "# --- Embedding Parameters ---\n",
    "USE_LEARNED_EMBEDDINGS = True  # Use learned embeddings from LLM model\n",
    "EMBEDDING_MODEL_PATH = './sudoku2vec_trained_model.pt'  # Path to saved embedding model\n",
    "EMBEDDING_DIM = 15            # Dimension of learned embeddings (from LLM model)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "DATASET_SIZE = 20000          # Number of diffusion sequences to pre-generate\n",
    "NUM_EPOCHS = 4000             # Number of training epochs\n",
    "BATCH_SIZE = 1024              # Batch size (reduced from 1024 for memory)\n",
    "LEARNING_RATE = 1e-3          # Optimizer learning rate\n",
    "WEIGHT_DECAY = 1e-4           # AdamW weight decay for regularization\n",
    "GRAD_CLIP_MAX_NORM = 1.0      # Gradient clipping threshold\n",
    "\n",
    "# --- Logging & Evaluation ---\n",
    "LOG_INTERVAL = 1             # Log metrics every N epochs\n",
    "EVAL_INTERVAL = 50           # Evaluate and sample every N epochs\n",
    "\n",
    "# --- Diffusion Parameters ---\n",
    "K_MAX = 6                     # Maximum number of forward steps for multi-step prediction loss (reduced from 10)\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# --- Sudoku2Vec ---\n",
    "ATTENTION_DIM = 9\n",
    "N_HEADS = 9\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: hidden_dim={HIDDEN_DIM}, num_layers={NUM_LAYERS}, kernel_size={KERNEL_SIZE}\")\n",
    "print(f\"  Embeddings: use_learned={USE_LEARNED_EMBEDDINGS}, embedding_dim={EMBEDDING_DIM}\")\n",
    "print(f\"  Training: dataset_size={DATASET_SIZE}, epochs={NUM_EPOCHS}, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"  Diffusion: k_max={K_MAX}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(\"\\n‚ö†Ô∏è  Memory optimizations applied:\")\n",
    "print(f\"  - Reduced BATCH_SIZE to {BATCH_SIZE} (from 1024)\")\n",
    "print(f\"  - Reduced K_MAX to {K_MAX} (from 10)\")\n",
    "print(f\"  - Reduced DATASET_SIZE to {DATASET_SIZE} (from 10000)\")\n",
    "print(\"  - Added memory cleanup in training loop\")\n",
    "print(\"\\n‚ö° Performance optimizations enabled:\")\n",
    "print(\"  - torch.compile() for JIT compilation (2-3x speedup)\")\n",
    "print(\"  - Mixed precision training (AMP) (1.5-2x speedup)\")\n",
    "print(\"  - Fused AdamW optimizer\")\n",
    "print(\"  - Optimized compute_loss() (removed unnecessary clones/ops)\")\n",
    "print(\"  - Expected combined speedup: 3-6x faster training\")\n",
    "print(\"\\nüí° If you still get OOM errors, restart the kernel first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1bb31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD EMBEDDING MODEL (from llm_on_sudoku.ipynb)\n",
    "# ============================================================================\n",
    "# We need the Sudoku2Vec class definition to load the trained model\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding on unit circle for 9x9 Sudoku grid\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a grid of positions (0-8 for both x and y)\n",
    "        x_coords = torch.arange(0, 9).unsqueeze(0).repeat(9, 1)\n",
    "        y_coords = torch.arange(0, 9).unsqueeze(1).repeat(1, 9)\n",
    "        \n",
    "        # Convert grid positions to linear indices (0-80)\n",
    "        linear_indices = y_coords * 9 + x_coords  # shape: (9, 9)\n",
    "        \n",
    "        # Convert linear indices to angles on unit circle\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (9, 9)\n",
    "        \n",
    "        # Compute x, y coordinates on unit circle\n",
    "        x_circle = torch.cos(angles)\n",
    "        y_circle = torch.sin(angles)\n",
    "        \n",
    "        # Stack and add batch dimension\n",
    "        pos_encoding = torch.stack([x_circle, y_circle], dim=-1).unsqueeze(0)  # shape: (1, 9, 9, 2)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def get_embedding_for_position(self, pos):\n",
    "        # input (batch, 2) where pos[:, 0] is x and pos[:, 1] is y\n",
    "        linear_indices = pos[:, 1] * 9 + pos[:, 0]  # shape: (batch,)\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (batch,)\n",
    "        x_circle = torch.cos(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        y_circle = torch.sin(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        return torch.cat([x_circle, y_circle], dim=1)  # shape: (batch, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a (batch, 9, 9, embedding_dim) grid\n",
    "        # output (batch, 9, 9, embedding_dim + 2) grid by adding pos_encoding to x\n",
    "        batch_size = x.shape[0]\n",
    "        pos_expanded = self.pos_encoding.repeat(batch_size, 1, 1, 1)\n",
    "        return torch.cat([x, pos_expanded], dim=-1)\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # stack all weight matrices 1...h together for efficiency\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # seperate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3) # [batch, head, seqlen, dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0,2,1,3) # [batch, seqlen, head, dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values) # [batch, seq_length, 81]\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "class Sudoku2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_dim=ATTENTION_DIM, num_heads=N_HEADS, device='cpu'):\n",
    "        super(Sudoku2Vec, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim) # this will provide the key queries and values\n",
    "        self.total_dim = self.embedding_dim + 2\n",
    "\n",
    "        self.mha = MultiheadAttention(\n",
    "            input_dim=self.total_dim,\n",
    "            embed_dim=attention_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the learned token embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding weight matrix of shape [vocab_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embed.weight.detach()\n",
    "    \n",
    "    def get_embedding_for_token(self, token):\n",
    "        \"\"\"\n",
    "        Get the embedding vector for a specific token.\n",
    "        \n",
    "        Args:\n",
    "            token: Integer token ID or tensor of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector(s) for the given token(s)\n",
    "        \"\"\"\n",
    "        if isinstance(token, int):\n",
    "            token = torch.tensor([token], device=self.device)\n",
    "        elif not isinstance(token, torch.Tensor):\n",
    "            token = torch.tensor(token, device=self.device)\n",
    "        return self.embed(token).detach()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the model in a portable format that can be easily loaded.\n",
    "        This saves the model architecture and weights in a single file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path where to save the model (should end with .pt or .pth)\n",
    "        \"\"\"\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'model_config': {\n",
    "                'vocab_size': self.embed.num_embeddings,\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'attention_dim': self.mha.embed_dim,\n",
    "                'num_heads': self.num_heads,\n",
    "            },\n",
    "            'model_class': 'Sudoku2Vec',\n",
    "        }\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cpu'):\n",
    "        \"\"\"\n",
    "        Load a saved Sudoku2Vec model from file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the saved model file\n",
    "            device: Device to load the model on ('cpu', 'cuda', 'mps')\n",
    "            \n",
    "        Returns:\n",
    "            Sudoku2Vec: Loaded model instance\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        \n",
    "        # Extract configuration\n",
    "        config = checkpoint['model_config']\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            attention_dim=config['attention_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set to evaluation mode by default\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        print(f\"Configuration: vocab_size={config['vocab_size']}, \"\n",
    "              f\"embedding_dim={config['embedding_dim']}, \"\n",
    "              f\"attention_dim={config['attention_dim']}, \"\n",
    "              f\"num_heads={config['num_heads']}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, target, position, sudoku_grid, mask=True):\n",
    "        # target - the token in the target blank space we try to predict shape [batch] i.e [0, 3, 3, 5, 1, ...]\n",
    "        # position - the (x, y) position of the target shape [batch, 2] - [[1, 1], [0, 3], [7,7], ...]\n",
    "        # sudoku_grid - the sudoku grid for the problem with target we want to predict shape [batch, 9, 9]\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        target_token_embeddings = self.embed(target) # shape [batch, embedding_dim]\n",
    "        target_position_vectors = self.pe.get_embedding_for_position(position) # [batch, 2]\n",
    "        target_token_with_position = torch.cat([target_token_embeddings, target_position_vectors], dim=-1)  # shape [batch, total_dim]\n",
    "\n",
    "        # mask the target in the grid\n",
    "        sudoku_grid_masked = sudoku_grid\n",
    "        if mask:\n",
    "            batch_indices = torch.arange(sudoku_grid.shape[0], device=self.device)\n",
    "            sudoku_grid_masked = sudoku_grid.clone()\n",
    "            sudoku_grid_masked[batch_indices, position[:, 1], position[:, 0]] = 0 # 0 is a mask token aka blank\n",
    "        \n",
    "        masked_sudoku_grid_embeddings = self.embed(sudoku_grid_masked)\n",
    "        masked_sudoku_grid_with_position = self.pe(masked_sudoku_grid_embeddings) # shape [batch, 9, 9, total_dim]\n",
    "        # Reshape grid to sequence: [batch, 81, total_dim]\n",
    "        masked_grid_seq = masked_sudoku_grid_with_position.view(batch_size, 81, self.total_dim)\n",
    "\n",
    "        grid_seq_embeddings = self.embed(sudoku_grid)\n",
    "        grid_seq_embeddings = grid_seq_embeddings.view(batch_size, 81, self.embedding_dim) \n",
    "        \n",
    "        # Query from target token: [batch, 1, total_dim]\n",
    "        # query = target_token_with_position.unsqueeze(1)\n",
    "\n",
    "        output, attention = self.mha(masked_grid_seq, return_attention=True)\n",
    "        # output is shape [batch, 81, total_dim]\n",
    "        \n",
    "        return output, attention, target_token_with_position, grid_seq_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "592c8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDiffusionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for pre-generated diffusion sequences.\n",
    "    \n",
    "    Each item is a diffusion sequence of shape (82, 9, 9) where:\n",
    "    - Index 0: completely masked grid (all zeros)\n",
    "    - Index 81: completely solved grid\n",
    "    - Indices 1-80: intermediate states with progressively more cells revealed\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Tensor of shape (dataset_size, 82, 9, 9) containing diffusion sequences\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "\n",
    "class SudokuDiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion model for Sudoku puzzles inspired by DDPM.\n",
    "    \n",
    "    Forward process: Progressively mask cells from a complete sudoku (T=81) to empty grid (T=0)\n",
    "    Reverse process: Learn to predict which cells to reveal to go from T to T+1\n",
    "    \n",
    "    The model learns to reverse the masking process, predicting which cell should be revealed\n",
    "    at each timestep given the current partially revealed grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, num_layers=6, kernel_size=3, num_groups=8, \n",
    "                 embedding_layer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_timesteps = 81  # 81 cells in a sudoku grid\n",
    "        self.embedding_layer = embedding_layer\n",
    "        \n",
    "        # Determine input channels based on whether we use embeddings\n",
    "        if embedding_layer is not None:\n",
    "            # Using learned embeddings: embedding_dim channels\n",
    "            input_channels = embedding_layer.embedding_dim\n",
    "            self.use_embeddings = True\n",
    "        else:\n",
    "            # Using simple normalization: 1 channel\n",
    "            input_channels = 1\n",
    "            self.use_embeddings = False\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers for processing the sudoku grid\n",
    "        self.conv_in = nn.Conv2d(input_channels, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output: dual heads for position and value prediction\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        # Position head: which cell to reveal (81 possibilities)\n",
    "        self.position_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 81)  # Logits for 81 cells\n",
    "        )\n",
    "        \n",
    "        # Value head: what value to place (10 classes: 0-9)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 10)  # Logits for 10 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Predict which cell should be revealed next and what value to place.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, 9, 9) sudoku grids at timestep t (0 = masked, 1-9 = revealed)\n",
    "            t: (batch,) timesteps (0 to 80)\n",
    "            \n",
    "        Returns:\n",
    "            position_logits: (batch, 81) logits for which cell should be revealed next\n",
    "            value_logits: (batch, 10) logits for what value (0-9) to place\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process input: either use embeddings or simple normalization\n",
    "        if self.use_embeddings:\n",
    "            # Use learned embeddings: (batch, 9, 9) -> (batch, 9, 9, embedding_dim)\n",
    "            x_embedded = self.embedding_layer(x.long())  # (batch, 9, 9, embedding_dim)\n",
    "            x_norm = x_embedded.permute(0, 3, 1, 2)  # (batch, embedding_dim, 9, 9)\n",
    "        else:\n",
    "            # Simple normalization to [-1, 1] range\n",
    "            x_norm = (x / 4.5) - 1.0\n",
    "            x_norm = x_norm.unsqueeze(1)  # (batch, 1, 9, 9)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_norm = t.float().unsqueeze(1) / self.num_timesteps  # (batch, 1)\n",
    "        t_emb = self.time_embed(t_norm)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Process through conv layers\n",
    "        h = self.conv_in(x_norm)  # (batch, hidden_dim, 9, 9)\n",
    "        \n",
    "        # Add time embedding to spatial features\n",
    "        t_emb_spatial = t_emb.view(batch_size, -1, 1, 1).expand(-1, -1, 9, 9)\n",
    "        h = h + t_emb_spatial\n",
    "        \n",
    "        # Apply conv blocks with residual connections\n",
    "        for block in self.conv_blocks:\n",
    "            h = h + block(h)\n",
    "        \n",
    "        # Output processing\n",
    "        h = self.conv_out(h)  # (batch, hidden_dim, 9, 9)\n",
    "        h_flat = h.reshape(batch_size, -1)  # (batch, hidden_dim * 81)\n",
    "        \n",
    "        # Dual predictions\n",
    "        position_logits = self.position_head(h_flat)  # (batch, 81)\n",
    "        value_logits = self.value_head(h_flat)  # (batch, 10)\n",
    "        \n",
    "        return position_logits, value_logits\n",
    "    \n",
    "    def compute_loss(self, sequences, k_max=10):\n",
    "        \"\"\"\n",
    "        Compute the diffusion loss for training using K-step iterative prediction.\n",
    "        \n",
    "        The forward diffusion process (from sudoku.py) goes from empty (T=0) to complete (T=81).\n",
    "        We learn to predict K steps ahead by iteratively applying the model.\n",
    "        \n",
    "        Args:\n",
    "            sequences: (batch, 82, 9, 9) diffusion sequences where:\n",
    "                      - sequences[:, 0] is completely masked (all zeros)\n",
    "                      - sequences[:, 81] is completely solved\n",
    "            k_max: Maximum number of forward steps for multi-step prediction\n",
    "                      \n",
    "        Returns:\n",
    "            loss: scalar combined loss (position + value)\n",
    "            accuracy: prediction accuracy for logging\n",
    "        \"\"\"\n",
    "        batch_size = sequences.shape[0]\n",
    "        \n",
    "        # Sample random starting timestep B from [0, 81-k_max]\n",
    "        max_start = max(1, self.num_timesteps - k_max)\n",
    "        B = torch.randint(0, max_start, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Sample random K from [1, k_max]\n",
    "        K = torch.randint(1, k_max + 1, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Get starting grids at timestep B (remove unnecessary clone)\n",
    "        x_current = sequences[torch.arange(batch_size), B].float()  # (batch, 9, 9)\n",
    "        \n",
    "        # Track losses and accuracies across all K steps\n",
    "        total_position_loss = 0.0\n",
    "        total_value_loss = 0.0\n",
    "        total_position_acc = 0.0\n",
    "        total_value_acc = 0.0\n",
    "        \n",
    "        # Pre-allocate batch_indices outside loop\n",
    "        batch_indices = torch.arange(batch_size, device=self.device)\n",
    "        \n",
    "        # Iteratively predict K steps\n",
    "        for step in range(k_max):\n",
    "            # Current timestep for each batch element\n",
    "            t_current = B + step\n",
    "            \n",
    "            # Only compute loss for elements where step < K[i] and t_current < 81\n",
    "            active_mask = (step < K) & (t_current < self.num_timesteps)\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            # Get target grid at next timestep\n",
    "            t_next = torch.clamp(t_current + 1, max=self.num_timesteps)\n",
    "            x_target = sequences[batch_indices, t_next].float()  # (batch, 9, 9)\n",
    "            \n",
    "            # Find which cell was revealed (difference between current and target)\n",
    "            diff = (x_target != x_current).view(batch_size, 81)  # (batch, 81)\n",
    "            \n",
    "            # Check if there's actually a difference (cell was revealed)\n",
    "            has_diff = diff.any(dim=1)  # (batch,)\n",
    "            active_mask = active_mask & has_diff  # Only process if there's a change\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            target_position = diff.float().argmax(dim=1)  # (batch,)\n",
    "            \n",
    "            # Get target values at revealed positions (vectorized for efficiency)\n",
    "            rows = target_position // 9\n",
    "            cols = target_position % 9\n",
    "            target_values = x_target[batch_indices, rows, cols].long()\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x_current, t_current)  # (batch, 81), (batch, 10)\n",
    "            \n",
    "            # Mask out already revealed cells in position prediction (in-place operation)\n",
    "            already_revealed = (x_current.view(batch_size, 81) != 0)  # (batch, 81)\n",
    "            position_logits.masked_fill_(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Compute losses only for active batch elements\n",
    "            if active_mask.any():\n",
    "                position_loss = F.cross_entropy(position_logits[active_mask], target_position[active_mask], reduction='sum')\n",
    "                value_loss = F.cross_entropy(value_logits[active_mask], target_values[active_mask], reduction='sum')\n",
    "                \n",
    "                # Accumulate losses (keep in computation graph for backprop)\n",
    "                total_position_loss = total_position_loss + position_loss\n",
    "                total_value_loss = total_value_loss + value_loss\n",
    "                \n",
    "                # Compute accuracy for logging\n",
    "                with torch.no_grad():\n",
    "                    pred_position = position_logits[active_mask].argmax(dim=1)\n",
    "                    pred_value = value_logits[active_mask].argmax(dim=1)\n",
    "                    total_position_acc += (pred_position == target_position[active_mask]).float().sum()\n",
    "                    total_value_acc += (pred_value == target_values[active_mask]).float().sum()\n",
    "            \n",
    "            # Update x_current with ground truth for next iteration (remove unnecessary clone)\n",
    "            x_current = x_target\n",
    "        \n",
    "        # Average losses over all active predictions\n",
    "        num_predictions = K.float().sum()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if num_predictions == 0:\n",
    "            return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        total_position_loss = total_position_loss / num_predictions\n",
    "        total_value_loss = total_value_loss / num_predictions\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = total_position_loss + total_value_loss\n",
    "        \n",
    "        # Average accuracy\n",
    "        accuracy = (total_position_acc + total_value_acc) / (2 * num_predictions)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=1, return_trajectory=False):\n",
    "        \"\"\"\n",
    "        Generate sudoku puzzles by running the reverse diffusion process.\n",
    "        Start from empty grid (T=0) and progressively reveal cells to T=81.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: number of puzzles to generate\n",
    "            return_trajectory: if True, return full trajectory of generation\n",
    "            \n",
    "        Returns:\n",
    "            samples: (batch_size, 9, 9) generated sudoku grids\n",
    "            trajectory: (batch_size, 82, 9, 9) if return_trajectory=True\n",
    "        \"\"\"\n",
    "        # Start from completely masked grid (T=0)\n",
    "        x = torch.zeros(batch_size, 9, 9, device=self.device)\n",
    "        \n",
    "        if return_trajectory:\n",
    "            trajectory = torch.zeros(batch_size, 82, 9, 9, device=self.device)\n",
    "            trajectory[:, 0] = x\n",
    "        \n",
    "        # Progressively reveal cells using model predictions\n",
    "        for t in tqdm(range(self.num_timesteps), desc='Sampling'):\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x, t_batch)\n",
    "            \n",
    "            # Mask out already revealed cells\n",
    "            already_revealed = (x.view(batch_size, 81) != 0)\n",
    "            position_logits = position_logits.masked_fill(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Sample or take argmax for position\n",
    "            position_probs = F.softmax(position_logits, dim=-1)\n",
    "            cell_idx = torch.multinomial(position_probs, 1).squeeze(-1)  # (batch,)\n",
    "            \n",
    "            # Take argmax for value (deterministic)\n",
    "            value_probs = F.softmax(value_logits, dim=-1)\n",
    "            values = torch.argmax(value_probs, dim=-1)  # (batch,)\n",
    "            \n",
    "            # Update grid with predicted values\n",
    "            for b in range(batch_size):\n",
    "                idx = cell_idx[b].item()\n",
    "                row = idx // 9\n",
    "                col = idx % 9\n",
    "                x[b, row, col] = values[b]\n",
    "            \n",
    "            if return_trajectory:\n",
    "                trajectory[:, t + 1] = x\n",
    "        \n",
    "        if return_trajectory:\n",
    "            return x.long(), trajectory.long()\n",
    "        return x.long()\n",
    "\n",
    "\n",
    "def train_sudoku_diffusion(model, dataset, num_epochs=1000, batch_size=32, lr=1e-4, \n",
    "                           weight_decay=1e-4, grad_clip_max_norm=1.0,\n",
    "                           log_interval=10, eval_interval=100, k_max=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the sudoku diffusion model with proper logging and performance optimizations.\n",
    "    \n",
    "    Args:\n",
    "        model: SudokuDiffusionModel instance\n",
    "        dataset: SudokuDiffusionDataset instance with pre-generated sequences\n",
    "        num_epochs: number of training epochs\n",
    "        batch_size: batch size for training\n",
    "        lr: learning rate\n",
    "        weight_decay: weight decay for AdamW optimizer\n",
    "        grad_clip_max_norm: max norm for gradient clipping\n",
    "        log_interval: log metrics every N epochs\n",
    "        eval_interval: evaluate and sample every N epochs\n",
    "        k_max: maximum number of forward steps for multi-step prediction\n",
    "        device: device to train on\n",
    "    \"\"\"\n",
    "    # Create DataLoader for batching and shuffling with optimizations\n",
    "    # Note: pin_memory should be False when data is already on GPU\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Keep 0 for GPU tensors\n",
    "        pin_memory=False,  # Data is already on GPU, no need to pin\n",
    "        persistent_workers=False  # No workers, so this doesn't apply\n",
    "    )\n",
    "    \n",
    "    # Use fused optimizer for faster updates (requires CUDA)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay,\n",
    "        fused=True if device == 'cuda' else False\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Dataset size: {len(dataset)}, Batch size: {batch_size}, Batches per epoch: {len(dataloader)}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Using mixed precision: {scaler is not None}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Iterate through batches in the dataset\n",
    "        for batch_sequences in dataloader:\n",
    "            # Mixed precision training\n",
    "            if scaler is not None:\n",
    "                # Forward pass with autocast\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training (CPU or non-CUDA)\n",
    "                loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics over all batches in the epoch\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        avg_epoch_acc = epoch_acc / num_batches\n",
    "        \n",
    "        # Record metrics\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_accuracies.append(avg_epoch_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch + 1:4d}/{num_epochs} | \"\n",
    "                  f\"Loss: {avg_epoch_loss:.4f} | \"\n",
    "                  f\"Acc: {avg_epoch_acc:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Evaluation and sampling\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Evaluation at epoch {epoch + 1}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Generate a sample\n",
    "                sample = model.sample(batch_size=1)\n",
    "                print(\"\\nGenerated Sudoku:\")\n",
    "                print(sample[0].cpu().numpy())\n",
    "                \n",
    "                # Check if valid\n",
    "                sudoku_obj = Sudoku(sample[0].cpu().numpy(), backend='numpy')\n",
    "                is_valid = sudoku_obj.is_valid()\n",
    "                print(f\"\\nIs valid: {is_valid}\")\n",
    "            \n",
    "            model.train()\n",
    "            print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(epoch_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epoch_accuracies)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, epoch_losses, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c719fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "  Device: NVIDIA RTX A4000\n",
      "  Total Memory: 15.63 GB\n",
      "\n",
      "Initial GPU Memory Usage:\n",
      "  Allocated: 1.28 GB\n",
      "  Cached: 2.92 GB\n",
      "\n",
      "‚ö†Ô∏è  Clearing GPU memory...\n",
      "\n",
      "After clearing:\n",
      "  Allocated: 0.54 GB\n",
      "  Cached: 0.58 GB\n",
      "  Free: 15.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU information and clear memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Information:\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nInitial GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Aggressively clear GPU memory\n",
    "    print(\"\\n‚ö†Ô∏è  Clearing GPU memory...\")\n",
    "    \n",
    "    # Delete all variables in the current namespace that might hold GPU tensors\n",
    "    if 'model' in dir():\n",
    "        del model\n",
    "    if 'generator' in dir():\n",
    "        del generator\n",
    "    if 'sequences' in dir():\n",
    "        del sequences\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    \n",
    "    print(f\"\\nAfter clearing:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # If still not enough memory, suggest kernel restart\n",
    "    free_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3\n",
    "    if free_memory < 1.0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Very little GPU memory available!\")\n",
    "        print(\"   Consider: Kernel -> Restart Kernel to fully clear GPU memory\")\n",
    "else:\n",
    "    print(\"CUDA not available, will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4c4f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GPU Memory before setup:\n",
      "  Allocated: 0.54 GB\n",
      "  Reserved: 0.58 GB\n",
      "  Free: 15.05 GB\n",
      "\n",
      "============================================================\n",
      "Loading learned embeddings from LLM model...\n",
      "============================================================\n",
      "Model loaded from ./sudoku2vec_trained_model.pt\n",
      "Configuration: vocab_size=10, embedding_dim=15, attention_dim=9, num_heads=9\n",
      "‚úì Successfully loaded embedding layer with 10 tokens\n",
      "  Embedding dimension: 15\n",
      "\n",
      "‚úì Embedding layer ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: LOAD EMBEDDING MODEL\n",
    "# ============================================================================\n",
    "# Run this cell once per session to load the pre-trained Sudoku2Vec embeddings.\n",
    "# This is fast (~1 second) and only needs to run once.\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Clear GPU memory if using CUDA\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nGPU Memory before setup:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Load embedding model if configured\n",
    "embedding_layer = None\n",
    "if USE_LEARNED_EMBEDDINGS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Loading learned embeddings from LLM model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    try:\n",
    "        sudoku2vec_model = Sudoku2Vec.load_model(EMBEDDING_MODEL_PATH, device=DEVICE)\n",
    "        embedding_layer = sudoku2vec_model.embed\n",
    "        print(f\"‚úì Successfully loaded embedding layer with {embedding_layer.num_embeddings} tokens\")\n",
    "        print(f\"  Embedding dimension: {embedding_layer.embedding_dim}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Embedding model not found at {EMBEDDING_MODEL_PATH}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Failed to load embedding model: {e}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "else:\n",
    "    print(\"\\nUsing simple normalization (no learned embeddings)\")\n",
    "\n",
    "print(\"\\n‚úì Embedding layer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55ecb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Pre-generating training dataset...\n",
      "============================================================\n",
      "Generating 20000 diffusion sequences...\n",
      "‚úì Dataset generated in 266.12 seconds (13.31 ms per sequence)\n",
      "\n",
      "Dataset statistics:\n",
      "  Shape: torch.Size([20000, 82, 9, 9])\n",
      "  Memory: 506.74 MB\n",
      "  Device: cuda:0\n",
      "‚úì Created SudokuDiffusionDataset with 20000 sequences\n",
      "\n",
      "üí° Dataset is ready! You can now rerun Cells 7 & 8 with different hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: GENERATE TRAINING DATASET\n",
    "# ============================================================================\n",
    "# Run this cell ONCE to generate the training dataset.\n",
    "# This is SLOW (~4 minutes for 20k sequences) but you only need to run it once!\n",
    "# \n",
    "# After running this cell, you can:\n",
    "# - Rerun Cell 7 to try different model architectures\n",
    "# - Rerun Cell 8 to try different training hyperparameters\n",
    "# - All without regenerating this expensive dataset!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Pre-generating training dataset...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = SudokuGenerator(backend='torch', device=DEVICE)\n",
    "\n",
    "# Generate diffusion sequences\n",
    "print(f\"Generating {DATASET_SIZE} diffusion sequences...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sequences = generator.generate_diffusion_sequence(size=DATASET_SIZE)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"‚úì Dataset generated in {generation_time:.2f} seconds ({generation_time/DATASET_SIZE*1000:.2f} ms per sequence)\")\n",
    "\n",
    "# Report dataset statistics\n",
    "sequence_shape = sequences.shape\n",
    "memory_mb = sequences.element_size() * sequences.nelement() / (1024 ** 2)\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Shape: {sequence_shape}\")\n",
    "print(f\"  Memory: {memory_mb:.2f} MB\")\n",
    "print(f\"  Device: {sequences.device}\")\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "train_dataset = SudokuDiffusionDataset(sequences)\n",
    "print(f\"‚úì Created SudokuDiffusionDataset with {len(train_dataset)} sequences\")\n",
    "print(f\"\\nüí° Dataset is ready! You can now rerun Cells 7 & 8 with different hyperparameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8393681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Initializing Diffusion Model...\n",
      "============================================================\n",
      "‚úì Model initialized successfully\n",
      "  Total parameters: 139,581\n",
      "  Trainable parameters: 139,581\n",
      "  Using embeddings: True\n",
      "\n",
      "‚ö° Compiling model with torch.compile for faster training...\n",
      "‚úì Model compiled successfully\n",
      "\n",
      "GPU Memory after model loading:\n",
      "  Allocated: 0.54 GB\n",
      "  Reserved: 0.58 GB\n",
      "\n",
      "‚úì Model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: INITIALIZE DIFFUSION MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to create and compile the diffusion model.\n",
    "# This is FAST (~5 seconds) and you can rerun it to experiment with:\n",
    "# - Different model sizes (HIDDEN_DIM, NUM_LAYERS)\n",
    "# - Different architectures (KERNEL_SIZE, NUM_GROUPS)\n",
    "# \n",
    "# The dataset from Cell 6 will be reused!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Initializing Diffusion Model...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model = SudokuDiffusionModel(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    num_groups=NUM_GROUPS,\n",
    "    embedding_layer=embedding_layer,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úì Model initialized successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Using embeddings: {model.use_embeddings}\")\n",
    "\n",
    "# Compile model for faster training (PyTorch 2.0+)\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\n‚ö° Compiling model with torch.compile for faster training...\")\n",
    "    try:\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "        print(f\"‚úì Model compiled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not compile model: {e}\")\n",
    "        print(f\"   Continuing without compilation\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\nGPU Memory after model loading:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úì Model is ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training...\n",
      "============================================================\n",
      "Starting training for 3000 epochs...\n",
      "Dataset size: 20000, Batch size: 1024, Batches per epoch: 20\n",
      "Learning rate: 0.001\n",
      "Using mixed precision: True\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_677680/3607855665.py:347: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
      "/tmp/ipykernel_677680/3607855665.py:372: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/3000 | Loss: 5.6753 | Acc: 0.0907 | LR: 0.001000\n",
      "Epoch    2/3000 | Loss: 5.6618 | Acc: 0.0981 | LR: 0.001000\n",
      "Epoch    3/3000 | Loss: 5.6508 | Acc: 0.0993 | LR: 0.001000\n",
      "Epoch    4/3000 | Loss: 5.6377 | Acc: 0.1013 | LR: 0.001000\n",
      "Epoch    5/3000 | Loss: 5.6433 | Acc: 0.1016 | LR: 0.001000\n",
      "Epoch    6/3000 | Loss: 5.6366 | Acc: 0.1025 | LR: 0.001000\n",
      "Epoch    7/3000 | Loss: 5.6353 | Acc: 0.1034 | LR: 0.001000\n",
      "Epoch    8/3000 | Loss: 5.6364 | Acc: 0.1035 | LR: 0.001000\n",
      "Epoch    9/3000 | Loss: 5.6277 | Acc: 0.1038 | LR: 0.001000\n",
      "Epoch   10/3000 | Loss: 5.6327 | Acc: 0.1017 | LR: 0.001000\n",
      "Epoch   11/3000 | Loss: 5.6294 | Acc: 0.1038 | LR: 0.001000\n",
      "Epoch   12/3000 | Loss: 5.6191 | Acc: 0.1042 | LR: 0.001000\n",
      "Epoch   13/3000 | Loss: 5.6209 | Acc: 0.1078 | LR: 0.001000\n",
      "Epoch   14/3000 | Loss: 5.6163 | Acc: 0.1049 | LR: 0.001000\n",
      "Epoch   15/3000 | Loss: 5.6296 | Acc: 0.1044 | LR: 0.001000\n",
      "Epoch   16/3000 | Loss: 5.6299 | Acc: 0.1056 | LR: 0.001000\n",
      "Epoch   17/3000 | Loss: 5.6221 | Acc: 0.1057 | LR: 0.001000\n",
      "Epoch   18/3000 | Loss: 5.6230 | Acc: 0.1049 | LR: 0.001000\n",
      "Epoch   19/3000 | Loss: 5.6190 | Acc: 0.1095 | LR: 0.001000\n",
      "Epoch   20/3000 | Loss: 5.6241 | Acc: 0.1036 | LR: 0.001000\n",
      "Epoch   21/3000 | Loss: 5.6114 | Acc: 0.1074 | LR: 0.001000\n",
      "Epoch   22/3000 | Loss: 5.6271 | Acc: 0.1061 | LR: 0.001000\n",
      "Epoch   23/3000 | Loss: 5.6258 | Acc: 0.1053 | LR: 0.001000\n",
      "Epoch   24/3000 | Loss: 5.6216 | Acc: 0.1109 | LR: 0.001000\n",
      "Epoch   25/3000 | Loss: 5.6179 | Acc: 0.1067 | LR: 0.001000\n",
      "Epoch   26/3000 | Loss: 5.6247 | Acc: 0.1069 | LR: 0.001000\n",
      "Epoch   27/3000 | Loss: 5.5997 | Acc: 0.1101 | LR: 0.001000\n",
      "Epoch   28/3000 | Loss: 5.6205 | Acc: 0.1056 | LR: 0.001000\n",
      "Epoch   29/3000 | Loss: 5.6124 | Acc: 0.1075 | LR: 0.001000\n",
      "Epoch   30/3000 | Loss: 5.6122 | Acc: 0.1056 | LR: 0.001000\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 282.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 4 2 4 6 2 8 3 2]\n",
      " [9 1 3 6 3 1 4 4 2]\n",
      " [4 3 1 7 9 8 3 9 7]\n",
      " [9 8 7 4 8 9 9 7 8]\n",
      " [9 7 3 6 4 9 7 6 4]\n",
      " [8 2 4 7 6 9 1 8 4]\n",
      " [8 7 4 1 4 7 8 4 6]\n",
      " [2 8 2 3 9 7 9 3 1]\n",
      " [8 9 7 8 2 6 7 4 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch   31/3000 | Loss: 5.6073 | Acc: 0.1070 | LR: 0.001000\n",
      "Epoch   32/3000 | Loss: 5.6040 | Acc: 0.1078 | LR: 0.001000\n",
      "Epoch   33/3000 | Loss: 5.6177 | Acc: 0.1072 | LR: 0.001000\n",
      "Epoch   34/3000 | Loss: 5.6056 | Acc: 0.1058 | LR: 0.001000\n",
      "Epoch   35/3000 | Loss: 5.6166 | Acc: 0.1061 | LR: 0.001000\n",
      "Epoch   36/3000 | Loss: 5.6069 | Acc: 0.1091 | LR: 0.001000\n",
      "Epoch   37/3000 | Loss: 5.6149 | Acc: 0.1103 | LR: 0.001000\n",
      "Epoch   38/3000 | Loss: 5.6133 | Acc: 0.1067 | LR: 0.001000\n",
      "Epoch   39/3000 | Loss: 5.6001 | Acc: 0.1089 | LR: 0.001000\n",
      "Epoch   40/3000 | Loss: 5.6038 | Acc: 0.1074 | LR: 0.001000\n",
      "Epoch   41/3000 | Loss: 5.6181 | Acc: 0.1107 | LR: 0.001000\n",
      "Epoch   42/3000 | Loss: 5.6050 | Acc: 0.1070 | LR: 0.001000\n",
      "Epoch   43/3000 | Loss: 5.6065 | Acc: 0.1097 | LR: 0.000999\n",
      "Epoch   44/3000 | Loss: 5.5963 | Acc: 0.1085 | LR: 0.000999\n",
      "Epoch   45/3000 | Loss: 5.6069 | Acc: 0.1089 | LR: 0.000999\n",
      "Epoch   46/3000 | Loss: 5.6055 | Acc: 0.1090 | LR: 0.000999\n",
      "Epoch   47/3000 | Loss: 5.6037 | Acc: 0.1078 | LR: 0.000999\n",
      "Epoch   48/3000 | Loss: 5.5982 | Acc: 0.1093 | LR: 0.000999\n",
      "Epoch   49/3000 | Loss: 5.6077 | Acc: 0.1102 | LR: 0.000999\n",
      "Epoch   50/3000 | Loss: 5.6190 | Acc: 0.1078 | LR: 0.000999\n",
      "Epoch   51/3000 | Loss: 5.6111 | Acc: 0.1102 | LR: 0.000999\n",
      "Epoch   52/3000 | Loss: 5.6123 | Acc: 0.1063 | LR: 0.000999\n",
      "Epoch   53/3000 | Loss: 5.6133 | Acc: 0.1066 | LR: 0.000999\n",
      "Epoch   54/3000 | Loss: 5.6087 | Acc: 0.1053 | LR: 0.000999\n",
      "Epoch   55/3000 | Loss: 5.6006 | Acc: 0.1136 | LR: 0.000999\n",
      "Epoch   56/3000 | Loss: 5.6078 | Acc: 0.1090 | LR: 0.000999\n",
      "Epoch   57/3000 | Loss: 5.6047 | Acc: 0.1104 | LR: 0.000999\n",
      "Epoch   58/3000 | Loss: 5.6113 | Acc: 0.1069 | LR: 0.000999\n",
      "Epoch   59/3000 | Loss: 5.5991 | Acc: 0.1091 | LR: 0.000999\n",
      "Epoch   60/3000 | Loss: 5.6102 | Acc: 0.1098 | LR: 0.000999\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 60\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 286.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[9 2 9 7 9 2 4 8 5]\n",
      " [8 7 4 5 2 7 6 2 9]\n",
      " [9 8 4 9 9 9 8 6 7]\n",
      " [7 9 6 3 2 7 3 9 5]\n",
      " [3 8 8 9 8 8 8 4 5]\n",
      " [8 4 8 2 8 4 6 6 5]\n",
      " [6 3 8 8 4 5 5 7 6]\n",
      " [5 6 2 8 9 4 5 4 7]\n",
      " [9 5 3 8 6 2 9 2 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch   61/3000 | Loss: 5.6036 | Acc: 0.1081 | LR: 0.000999\n",
      "Epoch   62/3000 | Loss: 5.5973 | Acc: 0.1101 | LR: 0.000999\n",
      "Epoch   63/3000 | Loss: 5.5916 | Acc: 0.1122 | LR: 0.000999\n",
      "Epoch   64/3000 | Loss: 5.6075 | Acc: 0.1086 | LR: 0.000999\n",
      "Epoch   65/3000 | Loss: 5.6055 | Acc: 0.1087 | LR: 0.000999\n",
      "Epoch   66/3000 | Loss: 5.6044 | Acc: 0.1082 | LR: 0.000999\n",
      "Epoch   67/3000 | Loss: 5.6103 | Acc: 0.1114 | LR: 0.000999\n",
      "Epoch   68/3000 | Loss: 5.5946 | Acc: 0.1096 | LR: 0.000999\n",
      "Epoch   69/3000 | Loss: 5.6082 | Acc: 0.1093 | LR: 0.000999\n",
      "Epoch   70/3000 | Loss: 5.6065 | Acc: 0.1129 | LR: 0.000999\n",
      "Epoch   71/3000 | Loss: 5.6036 | Acc: 0.1098 | LR: 0.000999\n",
      "Epoch   72/3000 | Loss: 5.6158 | Acc: 0.1076 | LR: 0.000999\n",
      "Epoch   73/3000 | Loss: 5.5932 | Acc: 0.1099 | LR: 0.000999\n",
      "Epoch   74/3000 | Loss: 5.6063 | Acc: 0.1084 | LR: 0.000998\n",
      "Epoch   75/3000 | Loss: 5.6028 | Acc: 0.1058 | LR: 0.000998\n",
      "Epoch   76/3000 | Loss: 5.6068 | Acc: 0.1063 | LR: 0.000998\n",
      "Epoch   77/3000 | Loss: 5.5859 | Acc: 0.1104 | LR: 0.000998\n",
      "Epoch   78/3000 | Loss: 5.5886 | Acc: 0.1099 | LR: 0.000998\n",
      "Epoch   79/3000 | Loss: 5.6049 | Acc: 0.1114 | LR: 0.000998\n",
      "Epoch   80/3000 | Loss: 5.6026 | Acc: 0.1119 | LR: 0.000998\n",
      "Epoch   81/3000 | Loss: 5.5878 | Acc: 0.1110 | LR: 0.000998\n",
      "Epoch   82/3000 | Loss: 5.5962 | Acc: 0.1110 | LR: 0.000998\n",
      "Epoch   83/3000 | Loss: 5.5852 | Acc: 0.1123 | LR: 0.000998\n",
      "Epoch   84/3000 | Loss: 5.5985 | Acc: 0.1110 | LR: 0.000998\n",
      "Epoch   85/3000 | Loss: 5.6037 | Acc: 0.1102 | LR: 0.000998\n",
      "Epoch   86/3000 | Loss: 5.5890 | Acc: 0.1142 | LR: 0.000998\n",
      "Epoch   87/3000 | Loss: 5.5958 | Acc: 0.1139 | LR: 0.000998\n",
      "Epoch   88/3000 | Loss: 5.5983 | Acc: 0.1107 | LR: 0.000998\n",
      "Epoch   89/3000 | Loss: 5.5959 | Acc: 0.1105 | LR: 0.000998\n",
      "Epoch   90/3000 | Loss: 5.5944 | Acc: 0.1109 | LR: 0.000998\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 90\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 292.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 1 2 1 7 1 1 5 1]\n",
      " [9 4 9 5 2 4 9 7 7]\n",
      " [3 4 9 8 9 2 2 9 2]\n",
      " [4 6 3 3 7 1 6 3 1]\n",
      " [5 2 1 2 6 4 5 1 2]\n",
      " [1 1 9 2 7 5 9 1 4]\n",
      " [4 7 8 2 5 1 8 9 6]\n",
      " [3 8 7 3 3 3 1 8 4]\n",
      " [6 2 1 6 3 1 8 6 3]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch   91/3000 | Loss: 5.5841 | Acc: 0.1106 | LR: 0.000998\n",
      "Epoch   92/3000 | Loss: 5.6005 | Acc: 0.1105 | LR: 0.000998\n",
      "Epoch   93/3000 | Loss: 5.5945 | Acc: 0.1113 | LR: 0.000998\n",
      "Epoch   94/3000 | Loss: 5.5905 | Acc: 0.1147 | LR: 0.000998\n",
      "Epoch   95/3000 | Loss: 5.6010 | Acc: 0.1087 | LR: 0.000998\n",
      "Epoch   96/3000 | Loss: 5.5843 | Acc: 0.1123 | LR: 0.000997\n",
      "Epoch   97/3000 | Loss: 5.6071 | Acc: 0.1091 | LR: 0.000997\n",
      "Epoch   98/3000 | Loss: 5.6036 | Acc: 0.1084 | LR: 0.000997\n",
      "Epoch   99/3000 | Loss: 5.5978 | Acc: 0.1088 | LR: 0.000997\n",
      "Epoch  100/3000 | Loss: 5.5937 | Acc: 0.1109 | LR: 0.000997\n",
      "Epoch  101/3000 | Loss: 5.5931 | Acc: 0.1119 | LR: 0.000997\n",
      "Epoch  102/3000 | Loss: 5.6007 | Acc: 0.1115 | LR: 0.000997\n",
      "Epoch  103/3000 | Loss: 5.5976 | Acc: 0.1097 | LR: 0.000997\n",
      "Epoch  104/3000 | Loss: 5.5991 | Acc: 0.1125 | LR: 0.000997\n",
      "Epoch  105/3000 | Loss: 5.6099 | Acc: 0.1100 | LR: 0.000997\n",
      "Epoch  106/3000 | Loss: 5.5957 | Acc: 0.1108 | LR: 0.000997\n",
      "Epoch  107/3000 | Loss: 5.6181 | Acc: 0.1088 | LR: 0.000997\n",
      "Epoch  108/3000 | Loss: 5.6011 | Acc: 0.1116 | LR: 0.000997\n",
      "Epoch  109/3000 | Loss: 5.5928 | Acc: 0.1118 | LR: 0.000997\n",
      "Epoch  110/3000 | Loss: 5.5986 | Acc: 0.1091 | LR: 0.000997\n",
      "Epoch  111/3000 | Loss: 5.5971 | Acc: 0.1106 | LR: 0.000997\n",
      "Epoch  112/3000 | Loss: 5.6102 | Acc: 0.1086 | LR: 0.000997\n",
      "Epoch  113/3000 | Loss: 5.6012 | Acc: 0.1094 | LR: 0.000997\n",
      "Epoch  114/3000 | Loss: 5.6053 | Acc: 0.1090 | LR: 0.000996\n",
      "Epoch  115/3000 | Loss: 5.5929 | Acc: 0.1122 | LR: 0.000996\n",
      "Epoch  116/3000 | Loss: 5.5848 | Acc: 0.1110 | LR: 0.000996\n",
      "Epoch  117/3000 | Loss: 5.5930 | Acc: 0.1128 | LR: 0.000996\n",
      "Epoch  118/3000 | Loss: 5.5788 | Acc: 0.1122 | LR: 0.000996\n",
      "Epoch  119/3000 | Loss: 5.6000 | Acc: 0.1102 | LR: 0.000996\n",
      "Epoch  120/3000 | Loss: 5.5770 | Acc: 0.1136 | LR: 0.000996\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 120\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 280.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 8 6 5 2 6 1 8 8]\n",
      " [4 6 5 3 6 5 5 5 5]\n",
      " [9 8 3 8 6 6 2 4 5]\n",
      " [6 1 6 5 5 5 5 5 8]\n",
      " [5 6 6 4 6 5 5 1 5]\n",
      " [5 9 8 2 6 5 2 4 7]\n",
      " [3 7 7 6 8 6 2 6 5]\n",
      " [5 3 6 5 5 5 5 5 2]\n",
      " [3 5 8 2 6 3 1 8 8]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  121/3000 | Loss: 5.6100 | Acc: 0.1083 | LR: 0.000996\n",
      "Epoch  122/3000 | Loss: 5.5937 | Acc: 0.1086 | LR: 0.000996\n",
      "Epoch  123/3000 | Loss: 5.5855 | Acc: 0.1120 | LR: 0.000996\n",
      "Epoch  124/3000 | Loss: 5.6094 | Acc: 0.1082 | LR: 0.000996\n",
      "Epoch  125/3000 | Loss: 5.5839 | Acc: 0.1112 | LR: 0.000996\n",
      "Epoch  126/3000 | Loss: 5.6003 | Acc: 0.1102 | LR: 0.000996\n",
      "Epoch  127/3000 | Loss: 5.5843 | Acc: 0.1115 | LR: 0.000996\n",
      "Epoch  128/3000 | Loss: 5.5953 | Acc: 0.1134 | LR: 0.000996\n",
      "Epoch  129/3000 | Loss: 5.5885 | Acc: 0.1112 | LR: 0.000995\n",
      "Epoch  130/3000 | Loss: 5.5979 | Acc: 0.1105 | LR: 0.000995\n",
      "Epoch  131/3000 | Loss: 5.5831 | Acc: 0.1117 | LR: 0.000995\n",
      "Epoch  132/3000 | Loss: 5.5942 | Acc: 0.1110 | LR: 0.000995\n",
      "Epoch  133/3000 | Loss: 5.6137 | Acc: 0.1089 | LR: 0.000995\n",
      "Epoch  134/3000 | Loss: 5.5927 | Acc: 0.1112 | LR: 0.000995\n",
      "Epoch  135/3000 | Loss: 5.6063 | Acc: 0.1126 | LR: 0.000995\n",
      "Epoch  136/3000 | Loss: 5.5940 | Acc: 0.1102 | LR: 0.000995\n",
      "Epoch  137/3000 | Loss: 5.5935 | Acc: 0.1106 | LR: 0.000995\n",
      "Epoch  138/3000 | Loss: 5.5765 | Acc: 0.1132 | LR: 0.000995\n",
      "Epoch  139/3000 | Loss: 5.5759 | Acc: 0.1117 | LR: 0.000995\n",
      "Epoch  140/3000 | Loss: 5.5877 | Acc: 0.1117 | LR: 0.000995\n",
      "Epoch  141/3000 | Loss: 5.5958 | Acc: 0.1100 | LR: 0.000995\n",
      "Epoch  142/3000 | Loss: 5.5952 | Acc: 0.1103 | LR: 0.000994\n",
      "Epoch  143/3000 | Loss: 5.5817 | Acc: 0.1130 | LR: 0.000994\n",
      "Epoch  144/3000 | Loss: 5.5925 | Acc: 0.1087 | LR: 0.000994\n",
      "Epoch  145/3000 | Loss: 5.5716 | Acc: 0.1148 | LR: 0.000994\n",
      "Epoch  146/3000 | Loss: 5.6025 | Acc: 0.1114 | LR: 0.000994\n",
      "Epoch  147/3000 | Loss: 5.5872 | Acc: 0.1103 | LR: 0.000994\n",
      "Epoch  148/3000 | Loss: 5.5985 | Acc: 0.1094 | LR: 0.000994\n",
      "Epoch  149/3000 | Loss: 5.5946 | Acc: 0.1123 | LR: 0.000994\n",
      "Epoch  150/3000 | Loss: 5.5892 | Acc: 0.1112 | LR: 0.000994\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 150\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 273.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 1 2 4 4 6 3 8 4]\n",
      " [3 9 3 3 5 7 5 6 4]\n",
      " [6 9 8 5 9 9 8 5 8]\n",
      " [4 8 9 4 6 8 6 7 5]\n",
      " [2 9 4 2 6 5 8 8 5]\n",
      " [2 6 7 8 9 4 8 7 8]\n",
      " [2 4 5 2 2 7 4 3 6]\n",
      " [6 3 7 9 9 1 8 5 3]\n",
      " [5 6 6 6 7 5 6 5 8]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  151/3000 | Loss: 5.5939 | Acc: 0.1119 | LR: 0.000994\n",
      "Epoch  152/3000 | Loss: 5.6038 | Acc: 0.1111 | LR: 0.000994\n",
      "Epoch  153/3000 | Loss: 5.5876 | Acc: 0.1126 | LR: 0.000994\n",
      "Epoch  154/3000 | Loss: 5.5951 | Acc: 0.1102 | LR: 0.000994\n",
      "Epoch  155/3000 | Loss: 5.6016 | Acc: 0.1108 | LR: 0.000993\n",
      "Epoch  156/3000 | Loss: 5.5876 | Acc: 0.1114 | LR: 0.000993\n",
      "Epoch  157/3000 | Loss: 5.5901 | Acc: 0.1103 | LR: 0.000993\n",
      "Epoch  158/3000 | Loss: 5.5939 | Acc: 0.1098 | LR: 0.000993\n",
      "Epoch  159/3000 | Loss: 5.5952 | Acc: 0.1093 | LR: 0.000993\n",
      "Epoch  160/3000 | Loss: 5.5852 | Acc: 0.1108 | LR: 0.000993\n",
      "Epoch  161/3000 | Loss: 5.5862 | Acc: 0.1103 | LR: 0.000993\n",
      "Epoch  162/3000 | Loss: 5.5948 | Acc: 0.1094 | LR: 0.000993\n",
      "Epoch  163/3000 | Loss: 5.5984 | Acc: 0.1107 | LR: 0.000993\n",
      "Epoch  164/3000 | Loss: 5.5799 | Acc: 0.1138 | LR: 0.000993\n",
      "Epoch  165/3000 | Loss: 5.5786 | Acc: 0.1139 | LR: 0.000993\n",
      "Epoch  166/3000 | Loss: 5.5914 | Acc: 0.1099 | LR: 0.000992\n",
      "Epoch  167/3000 | Loss: 5.5881 | Acc: 0.1123 | LR: 0.000992\n",
      "Epoch  168/3000 | Loss: 5.5871 | Acc: 0.1120 | LR: 0.000992\n",
      "Epoch  169/3000 | Loss: 5.5861 | Acc: 0.1133 | LR: 0.000992\n",
      "Epoch  170/3000 | Loss: 5.5956 | Acc: 0.1127 | LR: 0.000992\n",
      "Epoch  171/3000 | Loss: 5.5781 | Acc: 0.1140 | LR: 0.000992\n",
      "Epoch  172/3000 | Loss: 5.5868 | Acc: 0.1137 | LR: 0.000992\n",
      "Epoch  173/3000 | Loss: 5.5741 | Acc: 0.1142 | LR: 0.000992\n",
      "Epoch  174/3000 | Loss: 5.5951 | Acc: 0.1127 | LR: 0.000992\n",
      "Epoch  175/3000 | Loss: 5.6005 | Acc: 0.1095 | LR: 0.000992\n",
      "Epoch  176/3000 | Loss: 5.5944 | Acc: 0.1119 | LR: 0.000992\n",
      "Epoch  177/3000 | Loss: 5.5756 | Acc: 0.1142 | LR: 0.000991\n",
      "Epoch  178/3000 | Loss: 5.5923 | Acc: 0.1146 | LR: 0.000991\n",
      "Epoch  179/3000 | Loss: 5.5792 | Acc: 0.1119 | LR: 0.000991\n",
      "Epoch  180/3000 | Loss: 5.5949 | Acc: 0.1098 | LR: 0.000991\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 180\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 280.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[9 7 8 2 9 8 7 6 4]\n",
      " [7 7 4 8 6 4 3 4 1]\n",
      " [4 4 2 1 9 4 6 7 7]\n",
      " [4 3 7 7 4 7 7 4 9]\n",
      " [1 9 3 8 5 4 6 4 2]\n",
      " [9 9 6 7 8 2 3 7 9]\n",
      " [1 4 3 3 1 5 7 6 4]\n",
      " [2 6 7 7 1 3 3 6 3]\n",
      " [7 3 4 8 7 8 1 7 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  181/3000 | Loss: 5.6021 | Acc: 0.1129 | LR: 0.000991\n",
      "Epoch  182/3000 | Loss: 5.5820 | Acc: 0.1131 | LR: 0.000991\n",
      "Epoch  183/3000 | Loss: 5.5964 | Acc: 0.1137 | LR: 0.000991\n",
      "Epoch  184/3000 | Loss: 5.5728 | Acc: 0.1161 | LR: 0.000991\n",
      "Epoch  185/3000 | Loss: 5.5879 | Acc: 0.1105 | LR: 0.000991\n",
      "Epoch  186/3000 | Loss: 5.5817 | Acc: 0.1124 | LR: 0.000991\n",
      "Epoch  187/3000 | Loss: 5.5916 | Acc: 0.1111 | LR: 0.000990\n",
      "Epoch  188/3000 | Loss: 5.5993 | Acc: 0.1126 | LR: 0.000990\n",
      "Epoch  189/3000 | Loss: 5.5782 | Acc: 0.1134 | LR: 0.000990\n",
      "Epoch  190/3000 | Loss: 5.5764 | Acc: 0.1146 | LR: 0.000990\n",
      "Epoch  191/3000 | Loss: 5.5733 | Acc: 0.1135 | LR: 0.000990\n",
      "Epoch  192/3000 | Loss: 5.5829 | Acc: 0.1111 | LR: 0.000990\n",
      "Epoch  193/3000 | Loss: 5.5876 | Acc: 0.1160 | LR: 0.000990\n",
      "Epoch  194/3000 | Loss: 5.5917 | Acc: 0.1128 | LR: 0.000990\n",
      "Epoch  195/3000 | Loss: 5.5933 | Acc: 0.1110 | LR: 0.000990\n",
      "Epoch  196/3000 | Loss: 5.5952 | Acc: 0.1127 | LR: 0.000990\n",
      "Epoch  197/3000 | Loss: 5.5926 | Acc: 0.1118 | LR: 0.000989\n",
      "Epoch  198/3000 | Loss: 5.5892 | Acc: 0.1132 | LR: 0.000989\n",
      "Epoch  199/3000 | Loss: 5.5842 | Acc: 0.1140 | LR: 0.000989\n",
      "Epoch  200/3000 | Loss: 5.5876 | Acc: 0.1144 | LR: 0.000989\n",
      "Epoch  201/3000 | Loss: 5.5856 | Acc: 0.1138 | LR: 0.000989\n",
      "Epoch  202/3000 | Loss: 5.5826 | Acc: 0.1109 | LR: 0.000989\n",
      "Epoch  203/3000 | Loss: 5.5774 | Acc: 0.1125 | LR: 0.000989\n",
      "Epoch  204/3000 | Loss: 5.5918 | Acc: 0.1094 | LR: 0.000989\n",
      "Epoch  205/3000 | Loss: 5.5858 | Acc: 0.1125 | LR: 0.000989\n",
      "Epoch  206/3000 | Loss: 5.5860 | Acc: 0.1134 | LR: 0.000988\n",
      "Epoch  207/3000 | Loss: 5.5960 | Acc: 0.1114 | LR: 0.000988\n",
      "Epoch  208/3000 | Loss: 5.5990 | Acc: 0.1119 | LR: 0.000988\n",
      "Epoch  209/3000 | Loss: 5.5884 | Acc: 0.1159 | LR: 0.000988\n",
      "Epoch  210/3000 | Loss: 5.5997 | Acc: 0.1119 | LR: 0.000988\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 210\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 281.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 4 5 9 9 5 3 9 3]\n",
      " [8 9 7 2 2 2 9 7 9]\n",
      " [2 3 9 7 8 9 8 8 8]\n",
      " [6 5 9 8 9 4 6 2 5]\n",
      " [9 8 5 8 2 4 6 9 8]\n",
      " [9 8 8 6 3 9 4 6 5]\n",
      " [4 8 9 9 7 2 4 8 8]\n",
      " [8 8 4 3 7 4 6 6 4]\n",
      " [4 4 1 6 7 5 7 6 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  211/3000 | Loss: 5.5775 | Acc: 0.1116 | LR: 0.000988\n",
      "Epoch  212/3000 | Loss: 5.5853 | Acc: 0.1115 | LR: 0.000988\n",
      "Epoch  213/3000 | Loss: 5.5848 | Acc: 0.1135 | LR: 0.000988\n",
      "Epoch  214/3000 | Loss: 5.5796 | Acc: 0.1144 | LR: 0.000987\n",
      "Epoch  215/3000 | Loss: 5.5787 | Acc: 0.1125 | LR: 0.000987\n",
      "Epoch  216/3000 | Loss: 5.5847 | Acc: 0.1150 | LR: 0.000987\n",
      "Epoch  217/3000 | Loss: 5.5796 | Acc: 0.1123 | LR: 0.000987\n",
      "Epoch  218/3000 | Loss: 5.5751 | Acc: 0.1128 | LR: 0.000987\n",
      "Epoch  219/3000 | Loss: 5.5896 | Acc: 0.1124 | LR: 0.000987\n",
      "Epoch  220/3000 | Loss: 5.5808 | Acc: 0.1134 | LR: 0.000987\n",
      "Epoch  221/3000 | Loss: 5.5830 | Acc: 0.1151 | LR: 0.000987\n",
      "Epoch  222/3000 | Loss: 5.5879 | Acc: 0.1158 | LR: 0.000987\n",
      "Epoch  223/3000 | Loss: 5.5887 | Acc: 0.1127 | LR: 0.000986\n",
      "Epoch  224/3000 | Loss: 5.5890 | Acc: 0.1137 | LR: 0.000986\n",
      "Epoch  225/3000 | Loss: 5.5912 | Acc: 0.1133 | LR: 0.000986\n",
      "Epoch  226/3000 | Loss: 5.5903 | Acc: 0.1119 | LR: 0.000986\n",
      "Epoch  227/3000 | Loss: 5.5830 | Acc: 0.1145 | LR: 0.000986\n",
      "Epoch  228/3000 | Loss: 5.5958 | Acc: 0.1137 | LR: 0.000986\n",
      "Epoch  229/3000 | Loss: 5.5842 | Acc: 0.1140 | LR: 0.000986\n",
      "Epoch  230/3000 | Loss: 5.5906 | Acc: 0.1131 | LR: 0.000986\n",
      "Epoch  231/3000 | Loss: 5.5895 | Acc: 0.1104 | LR: 0.000985\n",
      "Epoch  232/3000 | Loss: 5.5928 | Acc: 0.1129 | LR: 0.000985\n",
      "Epoch  233/3000 | Loss: 5.5809 | Acc: 0.1104 | LR: 0.000985\n",
      "Epoch  234/3000 | Loss: 5.5753 | Acc: 0.1118 | LR: 0.000985\n",
      "Epoch  235/3000 | Loss: 5.5698 | Acc: 0.1140 | LR: 0.000985\n",
      "Epoch  236/3000 | Loss: 5.5936 | Acc: 0.1149 | LR: 0.000985\n",
      "Epoch  237/3000 | Loss: 5.5837 | Acc: 0.1141 | LR: 0.000985\n",
      "Epoch  238/3000 | Loss: 5.5862 | Acc: 0.1118 | LR: 0.000985\n",
      "Epoch  239/3000 | Loss: 5.5781 | Acc: 0.1140 | LR: 0.000984\n",
      "Epoch  240/3000 | Loss: 5.5828 | Acc: 0.1121 | LR: 0.000984\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 240\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 289.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 3 1 2 9 8 6 7 4]\n",
      " [3 1 5 6 9 4 7 4 9]\n",
      " [3 3 9 7 2 5 8 8 2]\n",
      " [4 8 1 9 5 4 8 9 2]\n",
      " [2 5 4 8 6 2 9 8 1]\n",
      " [6 6 9 2 1 6 9 8 7]\n",
      " [9 9 8 6 1 3 5 4 1]\n",
      " [3 4 3 9 8 5 9 7 2]\n",
      " [1 1 8 7 2 5 2 3 2]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  241/3000 | Loss: 5.5806 | Acc: 0.1128 | LR: 0.000984\n",
      "Epoch  242/3000 | Loss: 5.5786 | Acc: 0.1139 | LR: 0.000984\n",
      "Epoch  243/3000 | Loss: 5.5761 | Acc: 0.1148 | LR: 0.000984\n",
      "Epoch  244/3000 | Loss: 5.5870 | Acc: 0.1152 | LR: 0.000984\n",
      "Epoch  245/3000 | Loss: 5.5894 | Acc: 0.1121 | LR: 0.000984\n",
      "Epoch  246/3000 | Loss: 5.5670 | Acc: 0.1170 | LR: 0.000984\n",
      "Epoch  247/3000 | Loss: 5.5924 | Acc: 0.1124 | LR: 0.000983\n",
      "Epoch  248/3000 | Loss: 5.5969 | Acc: 0.1143 | LR: 0.000983\n",
      "Epoch  249/3000 | Loss: 5.5854 | Acc: 0.1146 | LR: 0.000983\n",
      "Epoch  250/3000 | Loss: 5.5698 | Acc: 0.1156 | LR: 0.000983\n",
      "Epoch  251/3000 | Loss: 5.5790 | Acc: 0.1131 | LR: 0.000983\n",
      "Epoch  252/3000 | Loss: 5.5669 | Acc: 0.1194 | LR: 0.000983\n",
      "Epoch  253/3000 | Loss: 5.5831 | Acc: 0.1155 | LR: 0.000983\n",
      "Epoch  254/3000 | Loss: 5.5837 | Acc: 0.1126 | LR: 0.000982\n",
      "Epoch  255/3000 | Loss: 5.5788 | Acc: 0.1155 | LR: 0.000982\n",
      "Epoch  256/3000 | Loss: 5.5895 | Acc: 0.1140 | LR: 0.000982\n",
      "Epoch  257/3000 | Loss: 5.5680 | Acc: 0.1161 | LR: 0.000982\n",
      "Epoch  258/3000 | Loss: 5.5683 | Acc: 0.1155 | LR: 0.000982\n",
      "Epoch  259/3000 | Loss: 5.5693 | Acc: 0.1158 | LR: 0.000982\n",
      "Epoch  260/3000 | Loss: 5.5685 | Acc: 0.1167 | LR: 0.000982\n",
      "Epoch  261/3000 | Loss: 5.5705 | Acc: 0.1160 | LR: 0.000981\n",
      "Epoch  262/3000 | Loss: 5.5763 | Acc: 0.1131 | LR: 0.000981\n",
      "Epoch  263/3000 | Loss: 5.5768 | Acc: 0.1155 | LR: 0.000981\n",
      "Epoch  264/3000 | Loss: 5.5830 | Acc: 0.1140 | LR: 0.000981\n",
      "Epoch  265/3000 | Loss: 5.5866 | Acc: 0.1139 | LR: 0.000981\n",
      "Epoch  266/3000 | Loss: 5.5792 | Acc: 0.1131 | LR: 0.000981\n",
      "Epoch  267/3000 | Loss: 5.5720 | Acc: 0.1131 | LR: 0.000981\n",
      "Epoch  268/3000 | Loss: 5.5739 | Acc: 0.1143 | LR: 0.000980\n",
      "Epoch  269/3000 | Loss: 5.5809 | Acc: 0.1153 | LR: 0.000980\n",
      "Epoch  270/3000 | Loss: 5.5977 | Acc: 0.1139 | LR: 0.000980\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 270\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 298.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 5 6 6 5 1 3 5 2]\n",
      " [6 1 1 2 1 8 7 4 7]\n",
      " [7 4 2 7 2 1 5 3 1]\n",
      " [6 5 3 6 6 2 5 4 3]\n",
      " [2 3 1 4 3 6 2 5 1]\n",
      " [4 2 6 6 5 3 7 8 5]\n",
      " [8 1 5 6 5 2 2 6 3]\n",
      " [1 5 5 7 5 3 5 1 9]\n",
      " [3 1 7 1 7 1 3 3 1]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  271/3000 | Loss: 5.5907 | Acc: 0.1153 | LR: 0.000980\n",
      "Epoch  272/3000 | Loss: 5.5785 | Acc: 0.1140 | LR: 0.000980\n",
      "Epoch  273/3000 | Loss: 5.5779 | Acc: 0.1152 | LR: 0.000980\n",
      "Epoch  274/3000 | Loss: 5.5702 | Acc: 0.1129 | LR: 0.000980\n",
      "Epoch  275/3000 | Loss: 5.5743 | Acc: 0.1161 | LR: 0.000979\n",
      "Epoch  276/3000 | Loss: 5.5818 | Acc: 0.1147 | LR: 0.000979\n",
      "Epoch  277/3000 | Loss: 5.5721 | Acc: 0.1160 | LR: 0.000979\n",
      "Epoch  278/3000 | Loss: 5.5727 | Acc: 0.1144 | LR: 0.000979\n",
      "Epoch  279/3000 | Loss: 5.5775 | Acc: 0.1144 | LR: 0.000979\n",
      "Epoch  280/3000 | Loss: 5.5920 | Acc: 0.1163 | LR: 0.000979\n",
      "Epoch  281/3000 | Loss: 5.5760 | Acc: 0.1180 | LR: 0.000979\n",
      "Epoch  282/3000 | Loss: 5.5700 | Acc: 0.1171 | LR: 0.000978\n",
      "Epoch  283/3000 | Loss: 5.5912 | Acc: 0.1130 | LR: 0.000978\n",
      "Epoch  284/3000 | Loss: 5.5833 | Acc: 0.1174 | LR: 0.000978\n",
      "Epoch  285/3000 | Loss: 5.5766 | Acc: 0.1153 | LR: 0.000978\n",
      "Epoch  286/3000 | Loss: 5.5802 | Acc: 0.1134 | LR: 0.000978\n",
      "Epoch  287/3000 | Loss: 5.5763 | Acc: 0.1137 | LR: 0.000978\n",
      "Epoch  288/3000 | Loss: 5.5826 | Acc: 0.1142 | LR: 0.000977\n",
      "Epoch  289/3000 | Loss: 5.5891 | Acc: 0.1121 | LR: 0.000977\n",
      "Epoch  290/3000 | Loss: 5.5786 | Acc: 0.1149 | LR: 0.000977\n",
      "Epoch  291/3000 | Loss: 5.5741 | Acc: 0.1140 | LR: 0.000977\n",
      "Epoch  292/3000 | Loss: 5.5697 | Acc: 0.1180 | LR: 0.000977\n",
      "Epoch  293/3000 | Loss: 5.5719 | Acc: 0.1143 | LR: 0.000977\n",
      "Epoch  294/3000 | Loss: 5.5788 | Acc: 0.1140 | LR: 0.000976\n",
      "Epoch  295/3000 | Loss: 5.5702 | Acc: 0.1149 | LR: 0.000976\n",
      "Epoch  296/3000 | Loss: 5.5785 | Acc: 0.1165 | LR: 0.000976\n",
      "Epoch  297/3000 | Loss: 5.5713 | Acc: 0.1150 | LR: 0.000976\n",
      "Epoch  298/3000 | Loss: 5.5804 | Acc: 0.1154 | LR: 0.000976\n",
      "Epoch  299/3000 | Loss: 5.5703 | Acc: 0.1183 | LR: 0.000976\n",
      "Epoch  300/3000 | Loss: 5.5874 | Acc: 0.1130 | LR: 0.000976\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 300\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 275.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[1 7 8 6 3 1 2 5 5]\n",
      " [6 9 3 7 3 9 8 5 5]\n",
      " [1 5 3 7 7 2 3 5 8]\n",
      " [1 2 2 1 6 7 7 4 6]\n",
      " [1 1 3 3 3 4 4 4 1]\n",
      " [3 8 6 7 4 7 6 5 6]\n",
      " [1 9 6 6 2 4 8 1 2]\n",
      " [6 5 1 5 5 1 5 2 1]\n",
      " [4 5 2 2 7 6 3 3 1]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  301/3000 | Loss: 5.5820 | Acc: 0.1167 | LR: 0.000975\n",
      "Epoch  302/3000 | Loss: 5.5773 | Acc: 0.1133 | LR: 0.000975\n",
      "Epoch  303/3000 | Loss: 5.5616 | Acc: 0.1175 | LR: 0.000975\n",
      "Epoch  304/3000 | Loss: 5.5690 | Acc: 0.1122 | LR: 0.000975\n",
      "Epoch  305/3000 | Loss: 5.5757 | Acc: 0.1158 | LR: 0.000975\n",
      "Epoch  306/3000 | Loss: 5.5732 | Acc: 0.1164 | LR: 0.000975\n",
      "Epoch  307/3000 | Loss: 5.5757 | Acc: 0.1130 | LR: 0.000974\n",
      "Epoch  308/3000 | Loss: 5.5796 | Acc: 0.1136 | LR: 0.000974\n",
      "Epoch  309/3000 | Loss: 5.5750 | Acc: 0.1151 | LR: 0.000974\n",
      "Epoch  310/3000 | Loss: 5.5685 | Acc: 0.1146 | LR: 0.000974\n",
      "Epoch  311/3000 | Loss: 5.5655 | Acc: 0.1160 | LR: 0.000974\n",
      "Epoch  312/3000 | Loss: 5.5743 | Acc: 0.1135 | LR: 0.000974\n",
      "Epoch  313/3000 | Loss: 5.5779 | Acc: 0.1145 | LR: 0.000973\n",
      "Epoch  314/3000 | Loss: 5.5743 | Acc: 0.1146 | LR: 0.000973\n",
      "Epoch  315/3000 | Loss: 5.5774 | Acc: 0.1156 | LR: 0.000973\n",
      "Epoch  316/3000 | Loss: 5.5654 | Acc: 0.1176 | LR: 0.000973\n",
      "Epoch  317/3000 | Loss: 5.5726 | Acc: 0.1160 | LR: 0.000973\n",
      "Epoch  318/3000 | Loss: 5.5640 | Acc: 0.1182 | LR: 0.000973\n",
      "Epoch  319/3000 | Loss: 5.5668 | Acc: 0.1161 | LR: 0.000972\n",
      "Epoch  320/3000 | Loss: 5.5724 | Acc: 0.1164 | LR: 0.000972\n",
      "Epoch  321/3000 | Loss: 5.5713 | Acc: 0.1193 | LR: 0.000972\n",
      "Epoch  322/3000 | Loss: 5.5686 | Acc: 0.1154 | LR: 0.000972\n",
      "Epoch  323/3000 | Loss: 5.5633 | Acc: 0.1165 | LR: 0.000972\n",
      "Epoch  324/3000 | Loss: 5.5773 | Acc: 0.1153 | LR: 0.000971\n",
      "Epoch  325/3000 | Loss: 5.5766 | Acc: 0.1163 | LR: 0.000971\n",
      "Epoch  326/3000 | Loss: 5.5678 | Acc: 0.1173 | LR: 0.000971\n",
      "Epoch  327/3000 | Loss: 5.5692 | Acc: 0.1168 | LR: 0.000971\n",
      "Epoch  328/3000 | Loss: 5.5596 | Acc: 0.1178 | LR: 0.000971\n",
      "Epoch  329/3000 | Loss: 5.5554 | Acc: 0.1197 | LR: 0.000971\n",
      "Epoch  330/3000 | Loss: 5.5667 | Acc: 0.1162 | LR: 0.000970\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 330\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 278.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 9 3 7 4 7 1 1 5]\n",
      " [1 1 6 9 4 8 4 6 7]\n",
      " [5 4 6 2 4 3 9 4 7]\n",
      " [2 8 4 8 6 9 4 1 3]\n",
      " [9 3 7 3 3 2 7 5 1]\n",
      " [6 3 2 2 7 3 3 4 6]\n",
      " [8 9 9 7 7 2 6 2 8]\n",
      " [2 4 1 5 8 6 8 1 9]\n",
      " [2 7 5 8 9 7 1 8 5]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  331/3000 | Loss: 5.5723 | Acc: 0.1168 | LR: 0.000970\n",
      "Epoch  332/3000 | Loss: 5.5710 | Acc: 0.1168 | LR: 0.000970\n",
      "Epoch  333/3000 | Loss: 5.5717 | Acc: 0.1151 | LR: 0.000970\n",
      "Epoch  334/3000 | Loss: 5.5590 | Acc: 0.1169 | LR: 0.000970\n",
      "Epoch  335/3000 | Loss: 5.5669 | Acc: 0.1174 | LR: 0.000970\n",
      "Epoch  336/3000 | Loss: 5.5599 | Acc: 0.1162 | LR: 0.000969\n",
      "Epoch  337/3000 | Loss: 5.5716 | Acc: 0.1167 | LR: 0.000969\n",
      "Epoch  338/3000 | Loss: 5.5796 | Acc: 0.1134 | LR: 0.000969\n",
      "Epoch  339/3000 | Loss: 5.5652 | Acc: 0.1174 | LR: 0.000969\n",
      "Epoch  340/3000 | Loss: 5.5705 | Acc: 0.1171 | LR: 0.000969\n",
      "Epoch  341/3000 | Loss: 5.5701 | Acc: 0.1168 | LR: 0.000968\n",
      "Epoch  342/3000 | Loss: 5.5646 | Acc: 0.1184 | LR: 0.000968\n",
      "Epoch  343/3000 | Loss: 5.5681 | Acc: 0.1162 | LR: 0.000968\n",
      "Epoch  344/3000 | Loss: 5.5761 | Acc: 0.1158 | LR: 0.000968\n",
      "Epoch  345/3000 | Loss: 5.5731 | Acc: 0.1159 | LR: 0.000968\n",
      "Epoch  346/3000 | Loss: 5.5847 | Acc: 0.1136 | LR: 0.000968\n",
      "Epoch  347/3000 | Loss: 5.5730 | Acc: 0.1174 | LR: 0.000967\n",
      "Epoch  348/3000 | Loss: 5.5682 | Acc: 0.1177 | LR: 0.000967\n",
      "Epoch  349/3000 | Loss: 5.5592 | Acc: 0.1163 | LR: 0.000967\n",
      "Epoch  350/3000 | Loss: 5.5655 | Acc: 0.1168 | LR: 0.000967\n",
      "Epoch  351/3000 | Loss: 5.5699 | Acc: 0.1155 | LR: 0.000967\n",
      "Epoch  352/3000 | Loss: 5.5660 | Acc: 0.1168 | LR: 0.000966\n",
      "Epoch  353/3000 | Loss: 5.5723 | Acc: 0.1167 | LR: 0.000966\n",
      "Epoch  354/3000 | Loss: 5.5926 | Acc: 0.1134 | LR: 0.000966\n",
      "Epoch  355/3000 | Loss: 5.5522 | Acc: 0.1177 | LR: 0.000966\n",
      "Epoch  356/3000 | Loss: 5.5566 | Acc: 0.1181 | LR: 0.000966\n",
      "Epoch  357/3000 | Loss: 5.5679 | Acc: 0.1170 | LR: 0.000965\n",
      "Epoch  358/3000 | Loss: 5.5859 | Acc: 0.1144 | LR: 0.000965\n",
      "Epoch  359/3000 | Loss: 5.5645 | Acc: 0.1185 | LR: 0.000965\n",
      "Epoch  360/3000 | Loss: 5.5843 | Acc: 0.1175 | LR: 0.000965\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 360\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 281.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[3 3 4 7 4 7 1 1 6]\n",
      " [6 7 4 4 2 2 8 6 3]\n",
      " [9 1 7 4 8 7 6 3 3]\n",
      " [5 3 5 8 4 1 7 9 7]\n",
      " [4 1 4 4 9 8 6 7 2]\n",
      " [3 1 7 4 2 3 1 7 5]\n",
      " [4 9 7 6 8 4 9 3 3]\n",
      " [2 1 8 2 7 9 2 1 6]\n",
      " [2 5 3 9 9 3 7 7 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  361/3000 | Loss: 5.5694 | Acc: 0.1185 | LR: 0.000965\n",
      "Epoch  362/3000 | Loss: 5.5716 | Acc: 0.1171 | LR: 0.000965\n",
      "Epoch  363/3000 | Loss: 5.5713 | Acc: 0.1162 | LR: 0.000964\n",
      "Epoch  364/3000 | Loss: 5.5693 | Acc: 0.1169 | LR: 0.000964\n",
      "Epoch  365/3000 | Loss: 5.5615 | Acc: 0.1158 | LR: 0.000964\n",
      "Epoch  366/3000 | Loss: 5.5630 | Acc: 0.1146 | LR: 0.000964\n",
      "Epoch  367/3000 | Loss: 5.5587 | Acc: 0.1163 | LR: 0.000964\n",
      "Epoch  368/3000 | Loss: 5.5689 | Acc: 0.1169 | LR: 0.000963\n",
      "Epoch  369/3000 | Loss: 5.5698 | Acc: 0.1166 | LR: 0.000963\n",
      "Epoch  370/3000 | Loss: 5.5622 | Acc: 0.1154 | LR: 0.000963\n",
      "Epoch  371/3000 | Loss: 5.5754 | Acc: 0.1137 | LR: 0.000963\n",
      "Epoch  372/3000 | Loss: 5.5651 | Acc: 0.1169 | LR: 0.000963\n",
      "Epoch  373/3000 | Loss: 5.5591 | Acc: 0.1157 | LR: 0.000962\n",
      "Epoch  374/3000 | Loss: 5.5530 | Acc: 0.1176 | LR: 0.000962\n",
      "Epoch  375/3000 | Loss: 5.5619 | Acc: 0.1161 | LR: 0.000962\n",
      "Epoch  376/3000 | Loss: 5.5678 | Acc: 0.1143 | LR: 0.000962\n",
      "Epoch  377/3000 | Loss: 5.5721 | Acc: 0.1154 | LR: 0.000962\n",
      "Epoch  378/3000 | Loss: 5.5649 | Acc: 0.1170 | LR: 0.000961\n",
      "Epoch  379/3000 | Loss: 5.5652 | Acc: 0.1169 | LR: 0.000961\n",
      "Epoch  380/3000 | Loss: 5.5653 | Acc: 0.1162 | LR: 0.000961\n",
      "Epoch  381/3000 | Loss: 5.5739 | Acc: 0.1181 | LR: 0.000961\n",
      "Epoch  382/3000 | Loss: 5.5666 | Acc: 0.1170 | LR: 0.000961\n",
      "Epoch  383/3000 | Loss: 5.5740 | Acc: 0.1147 | LR: 0.000960\n",
      "Epoch  384/3000 | Loss: 5.5604 | Acc: 0.1165 | LR: 0.000960\n",
      "Epoch  385/3000 | Loss: 5.5446 | Acc: 0.1194 | LR: 0.000960\n",
      "Epoch  386/3000 | Loss: 5.5730 | Acc: 0.1144 | LR: 0.000960\n",
      "Epoch  387/3000 | Loss: 5.5526 | Acc: 0.1179 | LR: 0.000959\n",
      "Epoch  388/3000 | Loss: 5.5747 | Acc: 0.1168 | LR: 0.000959\n",
      "Epoch  389/3000 | Loss: 5.5770 | Acc: 0.1168 | LR: 0.000959\n",
      "Epoch  390/3000 | Loss: 5.5630 | Acc: 0.1144 | LR: 0.000959\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 390\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 280.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[6 6 2 7 8 8 7 2 6]\n",
      " [9 3 4 7 4 6 2 3 5]\n",
      " [5 3 8 7 1 6 5 2 8]\n",
      " [5 9 3 5 4 2 7 1 3]\n",
      " [3 1 5 5 9 2 4 8 3]\n",
      " [8 9 7 1 3 5 6 2 6]\n",
      " [4 9 9 9 1 4 1 1 8]\n",
      " [3 6 6 7 2 7 4 7 1]\n",
      " [3 2 4 4 7 7 1 4 8]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  391/3000 | Loss: 5.5672 | Acc: 0.1173 | LR: 0.000959\n",
      "Epoch  392/3000 | Loss: 5.5730 | Acc: 0.1169 | LR: 0.000958\n",
      "Epoch  393/3000 | Loss: 5.5692 | Acc: 0.1148 | LR: 0.000958\n",
      "Epoch  394/3000 | Loss: 5.5660 | Acc: 0.1146 | LR: 0.000958\n",
      "Epoch  395/3000 | Loss: 5.5432 | Acc: 0.1213 | LR: 0.000958\n",
      "Epoch  396/3000 | Loss: 5.5766 | Acc: 0.1171 | LR: 0.000958\n",
      "Epoch  397/3000 | Loss: 5.5568 | Acc: 0.1186 | LR: 0.000957\n",
      "Epoch  398/3000 | Loss: 5.5572 | Acc: 0.1196 | LR: 0.000957\n",
      "Epoch  399/3000 | Loss: 5.5592 | Acc: 0.1176 | LR: 0.000957\n",
      "Epoch  400/3000 | Loss: 5.5649 | Acc: 0.1173 | LR: 0.000957\n",
      "Epoch  401/3000 | Loss: 5.5558 | Acc: 0.1178 | LR: 0.000957\n",
      "Epoch  402/3000 | Loss: 5.5662 | Acc: 0.1170 | LR: 0.000956\n",
      "Epoch  403/3000 | Loss: 5.5667 | Acc: 0.1169 | LR: 0.000956\n",
      "Epoch  404/3000 | Loss: 5.5731 | Acc: 0.1144 | LR: 0.000956\n",
      "Epoch  405/3000 | Loss: 5.5767 | Acc: 0.1147 | LR: 0.000956\n",
      "Epoch  406/3000 | Loss: 5.5774 | Acc: 0.1168 | LR: 0.000955\n",
      "Epoch  407/3000 | Loss: 5.5564 | Acc: 0.1144 | LR: 0.000955\n",
      "Epoch  408/3000 | Loss: 5.5648 | Acc: 0.1159 | LR: 0.000955\n",
      "Epoch  409/3000 | Loss: 5.5636 | Acc: 0.1170 | LR: 0.000955\n",
      "Epoch  410/3000 | Loss: 5.5716 | Acc: 0.1171 | LR: 0.000955\n",
      "Epoch  411/3000 | Loss: 5.5747 | Acc: 0.1170 | LR: 0.000954\n",
      "Epoch  412/3000 | Loss: 5.5611 | Acc: 0.1176 | LR: 0.000954\n",
      "Epoch  413/3000 | Loss: 5.5802 | Acc: 0.1165 | LR: 0.000954\n",
      "Epoch  414/3000 | Loss: 5.5711 | Acc: 0.1169 | LR: 0.000954\n",
      "Epoch  415/3000 | Loss: 5.5677 | Acc: 0.1177 | LR: 0.000954\n",
      "Epoch  416/3000 | Loss: 5.5581 | Acc: 0.1194 | LR: 0.000953\n",
      "Epoch  417/3000 | Loss: 5.5675 | Acc: 0.1141 | LR: 0.000953\n",
      "Epoch  418/3000 | Loss: 5.5568 | Acc: 0.1152 | LR: 0.000953\n",
      "Epoch  419/3000 | Loss: 5.5522 | Acc: 0.1190 | LR: 0.000953\n",
      "Epoch  420/3000 | Loss: 5.5627 | Acc: 0.1157 | LR: 0.000952\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 420\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 283.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[1 9 9 8 2 2 6 8 4]\n",
      " [2 6 1 6 8 6 4 8 3]\n",
      " [5 1 3 1 9 9 1 9 5]\n",
      " [4 2 6 6 5 5 2 1 2]\n",
      " [1 5 8 3 6 2 2 3 2]\n",
      " [8 8 5 9 5 8 5 3 2]\n",
      " [6 8 8 7 5 2 4 7 4]\n",
      " [1 9 3 6 5 5 8 9 7]\n",
      " [6 7 5 4 1 4 5 9 9]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  421/3000 | Loss: 5.5669 | Acc: 0.1163 | LR: 0.000952\n",
      "Epoch  422/3000 | Loss: 5.5664 | Acc: 0.1147 | LR: 0.000952\n",
      "Epoch  423/3000 | Loss: 5.5664 | Acc: 0.1186 | LR: 0.000952\n",
      "Epoch  424/3000 | Loss: 5.5660 | Acc: 0.1170 | LR: 0.000952\n",
      "Epoch  425/3000 | Loss: 5.5685 | Acc: 0.1189 | LR: 0.000951\n",
      "Epoch  426/3000 | Loss: 5.5655 | Acc: 0.1161 | LR: 0.000951\n",
      "Epoch  427/3000 | Loss: 5.5538 | Acc: 0.1175 | LR: 0.000951\n",
      "Epoch  428/3000 | Loss: 5.5417 | Acc: 0.1206 | LR: 0.000951\n",
      "Epoch  429/3000 | Loss: 5.5545 | Acc: 0.1159 | LR: 0.000950\n",
      "Epoch  430/3000 | Loss: 5.5627 | Acc: 0.1162 | LR: 0.000950\n",
      "Epoch  431/3000 | Loss: 5.5605 | Acc: 0.1149 | LR: 0.000950\n",
      "Epoch  432/3000 | Loss: 5.5555 | Acc: 0.1181 | LR: 0.000950\n",
      "Epoch  433/3000 | Loss: 5.5633 | Acc: 0.1212 | LR: 0.000949\n",
      "Epoch  434/3000 | Loss: 5.5559 | Acc: 0.1164 | LR: 0.000949\n",
      "Epoch  435/3000 | Loss: 5.5609 | Acc: 0.1194 | LR: 0.000949\n",
      "Epoch  436/3000 | Loss: 5.5593 | Acc: 0.1168 | LR: 0.000949\n",
      "Epoch  437/3000 | Loss: 5.5697 | Acc: 0.1195 | LR: 0.000949\n",
      "Epoch  438/3000 | Loss: 5.5685 | Acc: 0.1164 | LR: 0.000948\n",
      "Epoch  439/3000 | Loss: 5.5569 | Acc: 0.1206 | LR: 0.000948\n",
      "Epoch  440/3000 | Loss: 5.5500 | Acc: 0.1179 | LR: 0.000948\n",
      "Epoch  441/3000 | Loss: 5.5654 | Acc: 0.1171 | LR: 0.000948\n",
      "Epoch  442/3000 | Loss: 5.5647 | Acc: 0.1146 | LR: 0.000947\n",
      "Epoch  443/3000 | Loss: 5.5597 | Acc: 0.1207 | LR: 0.000947\n",
      "Epoch  444/3000 | Loss: 5.5507 | Acc: 0.1217 | LR: 0.000947\n",
      "Epoch  445/3000 | Loss: 5.5521 | Acc: 0.1185 | LR: 0.000947\n",
      "Epoch  446/3000 | Loss: 5.5754 | Acc: 0.1170 | LR: 0.000946\n",
      "Epoch  447/3000 | Loss: 5.5503 | Acc: 0.1179 | LR: 0.000946\n",
      "Epoch  448/3000 | Loss: 5.5587 | Acc: 0.1189 | LR: 0.000946\n",
      "Epoch  449/3000 | Loss: 5.5671 | Acc: 0.1188 | LR: 0.000946\n",
      "Epoch  450/3000 | Loss: 5.5725 | Acc: 0.1177 | LR: 0.000946\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 450\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 280.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[9 9 9 7 1 1 4 3 8]\n",
      " [5 4 2 8 3 6 6 3 4]\n",
      " [7 9 5 7 9 8 1 7 3]\n",
      " [5 1 8 8 4 2 4 3 3]\n",
      " [4 6 2 1 1 7 8 3 3]\n",
      " [2 8 7 7 6 8 2 6 9]\n",
      " [8 3 4 2 1 4 3 9 4]\n",
      " [7 9 6 6 9 7 5 5 7]\n",
      " [7 9 4 5 2 7 1 4 7]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  451/3000 | Loss: 5.5491 | Acc: 0.1186 | LR: 0.000945\n",
      "Epoch  452/3000 | Loss: 5.5525 | Acc: 0.1171 | LR: 0.000945\n",
      "Epoch  453/3000 | Loss: 5.5566 | Acc: 0.1178 | LR: 0.000945\n",
      "Epoch  454/3000 | Loss: 5.5560 | Acc: 0.1189 | LR: 0.000945\n",
      "Epoch  455/3000 | Loss: 5.5308 | Acc: 0.1192 | LR: 0.000944\n",
      "Epoch  456/3000 | Loss: 5.5466 | Acc: 0.1215 | LR: 0.000944\n",
      "Epoch  457/3000 | Loss: 5.5674 | Acc: 0.1191 | LR: 0.000944\n",
      "Epoch  458/3000 | Loss: 5.5416 | Acc: 0.1182 | LR: 0.000944\n",
      "Epoch  459/3000 | Loss: 5.5594 | Acc: 0.1186 | LR: 0.000943\n",
      "Epoch  460/3000 | Loss: 5.5531 | Acc: 0.1175 | LR: 0.000943\n",
      "Epoch  461/3000 | Loss: 5.5526 | Acc: 0.1172 | LR: 0.000943\n",
      "Epoch  462/3000 | Loss: 5.5641 | Acc: 0.1170 | LR: 0.000943\n",
      "Epoch  463/3000 | Loss: 5.5518 | Acc: 0.1227 | LR: 0.000942\n",
      "Epoch  464/3000 | Loss: 5.5551 | Acc: 0.1183 | LR: 0.000942\n",
      "Epoch  465/3000 | Loss: 5.5614 | Acc: 0.1128 | LR: 0.000942\n",
      "Epoch  466/3000 | Loss: 5.5568 | Acc: 0.1194 | LR: 0.000942\n",
      "Epoch  467/3000 | Loss: 5.5616 | Acc: 0.1182 | LR: 0.000941\n",
      "Epoch  468/3000 | Loss: 5.5727 | Acc: 0.1182 | LR: 0.000941\n",
      "Epoch  469/3000 | Loss: 5.5509 | Acc: 0.1174 | LR: 0.000941\n",
      "Epoch  470/3000 | Loss: 5.5467 | Acc: 0.1199 | LR: 0.000941\n",
      "Epoch  471/3000 | Loss: 5.5550 | Acc: 0.1150 | LR: 0.000940\n",
      "Epoch  472/3000 | Loss: 5.5613 | Acc: 0.1182 | LR: 0.000940\n",
      "Epoch  473/3000 | Loss: 5.5456 | Acc: 0.1222 | LR: 0.000940\n",
      "Epoch  474/3000 | Loss: 5.5537 | Acc: 0.1180 | LR: 0.000940\n",
      "Epoch  475/3000 | Loss: 5.5506 | Acc: 0.1189 | LR: 0.000939\n",
      "Epoch  476/3000 | Loss: 5.5494 | Acc: 0.1193 | LR: 0.000939\n",
      "Epoch  477/3000 | Loss: 5.5419 | Acc: 0.1206 | LR: 0.000939\n",
      "Epoch  478/3000 | Loss: 5.5493 | Acc: 0.1180 | LR: 0.000939\n",
      "Epoch  479/3000 | Loss: 5.5486 | Acc: 0.1178 | LR: 0.000938\n",
      "Epoch  480/3000 | Loss: 5.5586 | Acc: 0.1191 | LR: 0.000938\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 480\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 278.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 6 2 5 1 8 4 5 5]\n",
      " [3 4 9 2 1 9 7 4 5]\n",
      " [2 2 3 3 1 4 5 5 5]\n",
      " [4 2 3 3 6 7 9 8 9]\n",
      " [8 8 6 9 8 6 1 7 5]\n",
      " [4 2 6 1 2 8 6 8 9]\n",
      " [6 6 8 6 9 3 1 7 3]\n",
      " [5 5 2 1 8 9 6 5 2]\n",
      " [5 2 8 5 4 3 2 7 1]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  481/3000 | Loss: 5.5603 | Acc: 0.1191 | LR: 0.000938\n",
      "Epoch  482/3000 | Loss: 5.5652 | Acc: 0.1183 | LR: 0.000938\n",
      "Epoch  483/3000 | Loss: 5.5457 | Acc: 0.1194 | LR: 0.000937\n",
      "Epoch  484/3000 | Loss: 5.5599 | Acc: 0.1202 | LR: 0.000937\n",
      "Epoch  485/3000 | Loss: 5.5547 | Acc: 0.1163 | LR: 0.000937\n",
      "Epoch  486/3000 | Loss: 5.5575 | Acc: 0.1203 | LR: 0.000937\n",
      "Epoch  487/3000 | Loss: 5.5617 | Acc: 0.1196 | LR: 0.000936\n",
      "Epoch  488/3000 | Loss: 5.5408 | Acc: 0.1196 | LR: 0.000936\n",
      "Epoch  489/3000 | Loss: 5.5530 | Acc: 0.1173 | LR: 0.000936\n",
      "Epoch  490/3000 | Loss: 5.5570 | Acc: 0.1213 | LR: 0.000936\n",
      "Epoch  491/3000 | Loss: 5.5453 | Acc: 0.1201 | LR: 0.000935\n",
      "Epoch  492/3000 | Loss: 5.5429 | Acc: 0.1223 | LR: 0.000935\n",
      "Epoch  493/3000 | Loss: 5.5568 | Acc: 0.1215 | LR: 0.000935\n",
      "Epoch  494/3000 | Loss: 5.5514 | Acc: 0.1209 | LR: 0.000935\n",
      "Epoch  495/3000 | Loss: 5.5565 | Acc: 0.1181 | LR: 0.000934\n",
      "Epoch  496/3000 | Loss: 5.5636 | Acc: 0.1187 | LR: 0.000934\n",
      "Epoch  497/3000 | Loss: 5.5530 | Acc: 0.1162 | LR: 0.000934\n",
      "Epoch  498/3000 | Loss: 5.5458 | Acc: 0.1197 | LR: 0.000934\n",
      "Epoch  499/3000 | Loss: 5.5329 | Acc: 0.1191 | LR: 0.000933\n",
      "Epoch  500/3000 | Loss: 5.5642 | Acc: 0.1180 | LR: 0.000933\n",
      "Epoch  501/3000 | Loss: 5.5460 | Acc: 0.1195 | LR: 0.000933\n",
      "Epoch  502/3000 | Loss: 5.5587 | Acc: 0.1184 | LR: 0.000932\n",
      "Epoch  503/3000 | Loss: 5.5604 | Acc: 0.1183 | LR: 0.000932\n",
      "Epoch  504/3000 | Loss: 5.5637 | Acc: 0.1203 | LR: 0.000932\n",
      "Epoch  505/3000 | Loss: 5.5572 | Acc: 0.1165 | LR: 0.000932\n",
      "Epoch  506/3000 | Loss: 5.5595 | Acc: 0.1160 | LR: 0.000931\n",
      "Epoch  507/3000 | Loss: 5.5502 | Acc: 0.1168 | LR: 0.000931\n",
      "Epoch  508/3000 | Loss: 5.5530 | Acc: 0.1223 | LR: 0.000931\n",
      "Epoch  509/3000 | Loss: 5.5608 | Acc: 0.1163 | LR: 0.000931\n",
      "Epoch  510/3000 | Loss: 5.5627 | Acc: 0.1192 | LR: 0.000930\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 510\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 286.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 8 6 6 8 4 5 7 3]\n",
      " [7 2 8 6 7 4 8 7 1]\n",
      " [7 5 6 4 5 7 1 6 7]\n",
      " [1 6 9 2 8 5 4 6 7]\n",
      " [9 4 5 9 2 4 1 3 6]\n",
      " [1 9 5 3 3 9 9 4 3]\n",
      " [2 3 8 9 2 9 1 7 7]\n",
      " [4 3 5 7 8 1 3 1 8]\n",
      " [3 6 4 3 1 5 2 4 2]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  511/3000 | Loss: 5.5561 | Acc: 0.1169 | LR: 0.000930\n",
      "Epoch  512/3000 | Loss: 5.5477 | Acc: 0.1185 | LR: 0.000930\n",
      "Epoch  513/3000 | Loss: 5.5607 | Acc: 0.1189 | LR: 0.000930\n",
      "Epoch  514/3000 | Loss: 5.5491 | Acc: 0.1184 | LR: 0.000929\n",
      "Epoch  515/3000 | Loss: 5.5541 | Acc: 0.1202 | LR: 0.000929\n",
      "Epoch  516/3000 | Loss: 5.5520 | Acc: 0.1174 | LR: 0.000929\n",
      "Epoch  517/3000 | Loss: 5.5543 | Acc: 0.1179 | LR: 0.000928\n",
      "Epoch  518/3000 | Loss: 5.5569 | Acc: 0.1190 | LR: 0.000928\n",
      "Epoch  519/3000 | Loss: 5.5519 | Acc: 0.1178 | LR: 0.000928\n",
      "Epoch  520/3000 | Loss: 5.5412 | Acc: 0.1193 | LR: 0.000928\n",
      "Epoch  521/3000 | Loss: 5.5524 | Acc: 0.1189 | LR: 0.000927\n",
      "Epoch  522/3000 | Loss: 5.5482 | Acc: 0.1208 | LR: 0.000927\n",
      "Epoch  523/3000 | Loss: 5.5428 | Acc: 0.1190 | LR: 0.000927\n",
      "Epoch  524/3000 | Loss: 5.5712 | Acc: 0.1172 | LR: 0.000927\n",
      "Epoch  525/3000 | Loss: 5.5637 | Acc: 0.1170 | LR: 0.000926\n",
      "Epoch  526/3000 | Loss: 5.5472 | Acc: 0.1199 | LR: 0.000926\n",
      "Epoch  527/3000 | Loss: 5.5469 | Acc: 0.1211 | LR: 0.000926\n",
      "Epoch  528/3000 | Loss: 5.5470 | Acc: 0.1193 | LR: 0.000925\n",
      "Epoch  529/3000 | Loss: 5.5504 | Acc: 0.1208 | LR: 0.000925\n",
      "Epoch  530/3000 | Loss: 5.5483 | Acc: 0.1217 | LR: 0.000925\n",
      "Epoch  531/3000 | Loss: 5.5518 | Acc: 0.1207 | LR: 0.000925\n",
      "Epoch  532/3000 | Loss: 5.5498 | Acc: 0.1210 | LR: 0.000924\n",
      "Epoch  533/3000 | Loss: 5.5444 | Acc: 0.1190 | LR: 0.000924\n",
      "Epoch  534/3000 | Loss: 5.5486 | Acc: 0.1208 | LR: 0.000924\n",
      "Epoch  535/3000 | Loss: 5.5572 | Acc: 0.1194 | LR: 0.000924\n",
      "Epoch  536/3000 | Loss: 5.5388 | Acc: 0.1221 | LR: 0.000923\n",
      "Epoch  537/3000 | Loss: 5.5520 | Acc: 0.1189 | LR: 0.000923\n",
      "Epoch  538/3000 | Loss: 5.5459 | Acc: 0.1202 | LR: 0.000923\n",
      "Epoch  539/3000 | Loss: 5.5406 | Acc: 0.1190 | LR: 0.000922\n",
      "Epoch  540/3000 | Loss: 5.5497 | Acc: 0.1209 | LR: 0.000922\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 540\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 288.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[7 6 7 9 5 7 2 9 7]\n",
      " [2 9 7 8 8 2 4 9 2]\n",
      " [8 6 3 4 2 6 8 5 8]\n",
      " [1 5 5 5 9 6 6 9 4]\n",
      " [8 5 1 2 3 7 2 1 3]\n",
      " [5 9 6 1 5 6 6 3 1]\n",
      " [5 2 1 5 8 2 1 8 2]\n",
      " [4 3 6 2 8 4 5 3 5]\n",
      " [4 1 3 9 1 4 5 9 3]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  541/3000 | Loss: 5.5576 | Acc: 0.1161 | LR: 0.000922\n",
      "Epoch  542/3000 | Loss: 5.5545 | Acc: 0.1181 | LR: 0.000922\n",
      "Epoch  543/3000 | Loss: 5.5657 | Acc: 0.1176 | LR: 0.000921\n",
      "Epoch  544/3000 | Loss: 5.5469 | Acc: 0.1196 | LR: 0.000921\n",
      "Epoch  545/3000 | Loss: 5.5423 | Acc: 0.1200 | LR: 0.000921\n",
      "Epoch  546/3000 | Loss: 5.5475 | Acc: 0.1196 | LR: 0.000920\n",
      "Epoch  547/3000 | Loss: 5.5380 | Acc: 0.1212 | LR: 0.000920\n",
      "Epoch  548/3000 | Loss: 5.5447 | Acc: 0.1191 | LR: 0.000920\n",
      "Epoch  549/3000 | Loss: 5.5562 | Acc: 0.1189 | LR: 0.000920\n",
      "Epoch  550/3000 | Loss: 5.5494 | Acc: 0.1200 | LR: 0.000919\n",
      "Epoch  551/3000 | Loss: 5.5410 | Acc: 0.1196 | LR: 0.000919\n",
      "Epoch  552/3000 | Loss: 5.5467 | Acc: 0.1188 | LR: 0.000919\n",
      "Epoch  553/3000 | Loss: 5.5488 | Acc: 0.1213 | LR: 0.000918\n",
      "Epoch  554/3000 | Loss: 5.5509 | Acc: 0.1194 | LR: 0.000918\n",
      "Epoch  555/3000 | Loss: 5.5468 | Acc: 0.1192 | LR: 0.000918\n",
      "Epoch  556/3000 | Loss: 5.5358 | Acc: 0.1219 | LR: 0.000918\n",
      "Epoch  557/3000 | Loss: 5.5466 | Acc: 0.1220 | LR: 0.000917\n",
      "Epoch  558/3000 | Loss: 5.5417 | Acc: 0.1205 | LR: 0.000917\n",
      "Epoch  559/3000 | Loss: 5.5654 | Acc: 0.1190 | LR: 0.000917\n",
      "Epoch  560/3000 | Loss: 5.5403 | Acc: 0.1216 | LR: 0.000916\n",
      "Epoch  561/3000 | Loss: 5.5459 | Acc: 0.1201 | LR: 0.000916\n",
      "Epoch  562/3000 | Loss: 5.5483 | Acc: 0.1181 | LR: 0.000916\n",
      "Epoch  563/3000 | Loss: 5.5581 | Acc: 0.1181 | LR: 0.000916\n",
      "Epoch  564/3000 | Loss: 5.5466 | Acc: 0.1216 | LR: 0.000915\n",
      "Epoch  565/3000 | Loss: 5.5386 | Acc: 0.1243 | LR: 0.000915\n",
      "Epoch  566/3000 | Loss: 5.5324 | Acc: 0.1236 | LR: 0.000915\n",
      "Epoch  567/3000 | Loss: 5.5483 | Acc: 0.1199 | LR: 0.000914\n",
      "Epoch  568/3000 | Loss: 5.5469 | Acc: 0.1207 | LR: 0.000914\n",
      "Epoch  569/3000 | Loss: 5.5474 | Acc: 0.1193 | LR: 0.000914\n",
      "Epoch  570/3000 | Loss: 5.5415 | Acc: 0.1188 | LR: 0.000914\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 570\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 284.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[4 7 3 1 3 9 5 5 8]\n",
      " [1 4 7 5 9 3 6 7 4]\n",
      " [5 2 8 2 1 5 8 3 9]\n",
      " [6 5 6 3 1 4 6 3 2]\n",
      " [3 8 9 8 9 6 6 3 3]\n",
      " [1 4 9 4 2 5 1 5 1]\n",
      " [8 1 2 7 4 1 4 2 7]\n",
      " [8 2 8 2 7 6 5 9 8]\n",
      " [4 5 9 7 7 6 2 6 2]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  571/3000 | Loss: 5.5435 | Acc: 0.1181 | LR: 0.000913\n",
      "Epoch  572/3000 | Loss: 5.5361 | Acc: 0.1225 | LR: 0.000913\n",
      "Epoch  573/3000 | Loss: 5.5572 | Acc: 0.1198 | LR: 0.000913\n",
      "Epoch  574/3000 | Loss: 5.5541 | Acc: 0.1206 | LR: 0.000912\n",
      "Epoch  575/3000 | Loss: 5.5493 | Acc: 0.1187 | LR: 0.000912\n",
      "Epoch  576/3000 | Loss: 5.5553 | Acc: 0.1170 | LR: 0.000912\n",
      "Epoch  577/3000 | Loss: 5.5274 | Acc: 0.1253 | LR: 0.000911\n",
      "Epoch  578/3000 | Loss: 5.5459 | Acc: 0.1203 | LR: 0.000911\n",
      "Epoch  579/3000 | Loss: 5.5459 | Acc: 0.1180 | LR: 0.000911\n",
      "Epoch  580/3000 | Loss: 5.5416 | Acc: 0.1188 | LR: 0.000911\n",
      "Epoch  581/3000 | Loss: 5.5384 | Acc: 0.1190 | LR: 0.000910\n",
      "Epoch  582/3000 | Loss: 5.5520 | Acc: 0.1190 | LR: 0.000910\n",
      "Epoch  583/3000 | Loss: 5.5556 | Acc: 0.1222 | LR: 0.000910\n",
      "Epoch  584/3000 | Loss: 5.5571 | Acc: 0.1215 | LR: 0.000909\n",
      "Epoch  585/3000 | Loss: 5.5440 | Acc: 0.1198 | LR: 0.000909\n",
      "Epoch  586/3000 | Loss: 5.5481 | Acc: 0.1179 | LR: 0.000909\n",
      "Epoch  587/3000 | Loss: 5.5556 | Acc: 0.1194 | LR: 0.000908\n",
      "Epoch  588/3000 | Loss: 5.5528 | Acc: 0.1194 | LR: 0.000908\n",
      "Epoch  589/3000 | Loss: 5.5472 | Acc: 0.1215 | LR: 0.000908\n",
      "Epoch  590/3000 | Loss: 5.5359 | Acc: 0.1191 | LR: 0.000908\n",
      "Epoch  591/3000 | Loss: 5.5346 | Acc: 0.1220 | LR: 0.000907\n",
      "Epoch  592/3000 | Loss: 5.5514 | Acc: 0.1204 | LR: 0.000907\n",
      "Epoch  593/3000 | Loss: 5.5400 | Acc: 0.1202 | LR: 0.000907\n",
      "Epoch  594/3000 | Loss: 5.5388 | Acc: 0.1220 | LR: 0.000906\n",
      "Epoch  595/3000 | Loss: 5.5485 | Acc: 0.1219 | LR: 0.000906\n",
      "Epoch  596/3000 | Loss: 5.5392 | Acc: 0.1191 | LR: 0.000906\n",
      "Epoch  597/3000 | Loss: 5.5560 | Acc: 0.1175 | LR: 0.000905\n",
      "Epoch  598/3000 | Loss: 5.5481 | Acc: 0.1205 | LR: 0.000905\n",
      "Epoch  599/3000 | Loss: 5.5455 | Acc: 0.1212 | LR: 0.000905\n",
      "Epoch  600/3000 | Loss: 5.5391 | Acc: 0.1222 | LR: 0.000905\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 600\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 287.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[2 8 2 5 4 9 6 1 5]\n",
      " [8 3 6 8 1 2 1 7 5]\n",
      " [2 9 4 3 5 1 3 5 3]\n",
      " [2 1 5 5 9 6 2 7 6]\n",
      " [6 6 8 9 8 1 4 2 3]\n",
      " [1 2 4 9 6 3 9 9 7]\n",
      " [6 4 5 8 2 1 5 7 7]\n",
      " [3 7 4 5 2 5 8 4 9]\n",
      " [8 3 6 1 9 8 8 6 3]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  601/3000 | Loss: 5.5406 | Acc: 0.1203 | LR: 0.000904\n",
      "Epoch  602/3000 | Loss: 5.5474 | Acc: 0.1207 | LR: 0.000904\n",
      "Epoch  603/3000 | Loss: 5.5651 | Acc: 0.1197 | LR: 0.000904\n",
      "Epoch  604/3000 | Loss: 5.5580 | Acc: 0.1198 | LR: 0.000903\n",
      "Epoch  605/3000 | Loss: 5.5331 | Acc: 0.1229 | LR: 0.000903\n",
      "Epoch  606/3000 | Loss: 5.5368 | Acc: 0.1241 | LR: 0.000903\n",
      "Epoch  607/3000 | Loss: 5.5479 | Acc: 0.1212 | LR: 0.000902\n",
      "Epoch  608/3000 | Loss: 5.5535 | Acc: 0.1221 | LR: 0.000902\n",
      "Epoch  609/3000 | Loss: 5.5392 | Acc: 0.1203 | LR: 0.000902\n",
      "Epoch  610/3000 | Loss: 5.5446 | Acc: 0.1193 | LR: 0.000901\n",
      "Epoch  611/3000 | Loss: 5.5534 | Acc: 0.1185 | LR: 0.000901\n",
      "Epoch  612/3000 | Loss: 5.5539 | Acc: 0.1203 | LR: 0.000901\n",
      "Epoch  613/3000 | Loss: 5.5408 | Acc: 0.1214 | LR: 0.000900\n",
      "Epoch  614/3000 | Loss: 5.5514 | Acc: 0.1196 | LR: 0.000900\n",
      "Epoch  615/3000 | Loss: 5.5414 | Acc: 0.1190 | LR: 0.000900\n",
      "Epoch  616/3000 | Loss: 5.5366 | Acc: 0.1199 | LR: 0.000900\n",
      "Epoch  617/3000 | Loss: 5.5499 | Acc: 0.1211 | LR: 0.000899\n",
      "Epoch  618/3000 | Loss: 5.5297 | Acc: 0.1243 | LR: 0.000899\n",
      "Epoch  619/3000 | Loss: 5.5452 | Acc: 0.1191 | LR: 0.000899\n",
      "Epoch  620/3000 | Loss: 5.5481 | Acc: 0.1232 | LR: 0.000898\n",
      "Epoch  621/3000 | Loss: 5.5601 | Acc: 0.1207 | LR: 0.000898\n",
      "Epoch  622/3000 | Loss: 5.5430 | Acc: 0.1213 | LR: 0.000898\n",
      "Epoch  623/3000 | Loss: 5.5303 | Acc: 0.1217 | LR: 0.000897\n",
      "Epoch  624/3000 | Loss: 5.5308 | Acc: 0.1211 | LR: 0.000897\n",
      "Epoch  625/3000 | Loss: 5.5465 | Acc: 0.1212 | LR: 0.000897\n",
      "Epoch  626/3000 | Loss: 5.5315 | Acc: 0.1242 | LR: 0.000896\n",
      "Epoch  627/3000 | Loss: 5.5329 | Acc: 0.1235 | LR: 0.000896\n",
      "Epoch  628/3000 | Loss: 5.5365 | Acc: 0.1212 | LR: 0.000896\n",
      "Epoch  629/3000 | Loss: 5.5325 | Acc: 0.1217 | LR: 0.000895\n",
      "Epoch  630/3000 | Loss: 5.5291 | Acc: 0.1207 | LR: 0.000895\n",
      "\n",
      "============================================================\n",
      "Evaluation at epoch 630\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:00<00:00, 282.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sudoku:\n",
      "[[1 7 9 2 7 1 4 7 1]\n",
      " [4 8 5 4 2 1 6 2 8]\n",
      " [2 3 6 6 1 4 9 2 9]\n",
      " [8 5 5 9 7 3 5 1 1]\n",
      " [3 9 2 8 4 3 8 3 9]\n",
      " [5 1 5 8 8 9 2 7 1]\n",
      " [3 6 7 8 9 2 2 6 8]\n",
      " [3 6 4 7 2 4 5 6 9]\n",
      " [1 5 3 4 5 1 9 5 6]]\n",
      "\n",
      "Is valid: False\n",
      "============================================================\n",
      "\n",
      "Epoch  631/3000 | Loss: 5.5443 | Acc: 0.1231 | LR: 0.000895\n",
      "Epoch  632/3000 | Loss: 5.5520 | Acc: 0.1200 | LR: 0.000894\n",
      "Epoch  633/3000 | Loss: 5.5328 | Acc: 0.1233 | LR: 0.000894\n",
      "Epoch  634/3000 | Loss: 5.5509 | Acc: 0.1179 | LR: 0.000894\n",
      "Epoch  635/3000 | Loss: 5.5360 | Acc: 0.1230 | LR: 0.000893\n",
      "Epoch  636/3000 | Loss: 5.5429 | Acc: 0.1210 | LR: 0.000893\n",
      "Epoch  637/3000 | Loss: 5.5327 | Acc: 0.1221 | LR: 0.000893\n",
      "Epoch  638/3000 | Loss: 5.5312 | Acc: 0.1185 | LR: 0.000892\n",
      "Epoch  639/3000 | Loss: 5.5390 | Acc: 0.1215 | LR: 0.000892\n",
      "Epoch  640/3000 | Loss: 5.5401 | Acc: 0.1217 | LR: 0.000892\n",
      "Epoch  641/3000 | Loss: 5.5440 | Acc: 0.1233 | LR: 0.000892\n",
      "Epoch  642/3000 | Loss: 5.5486 | Acc: 0.1224 | LR: 0.000891\n",
      "Epoch  643/3000 | Loss: 5.5244 | Acc: 0.1228 | LR: 0.000891\n",
      "Epoch  644/3000 | Loss: 5.5459 | Acc: 0.1180 | LR: 0.000891\n",
      "Epoch  645/3000 | Loss: 5.5281 | Acc: 0.1234 | LR: 0.000890\n",
      "Epoch  646/3000 | Loss: 5.5249 | Acc: 0.1224 | LR: 0.000890\n",
      "Epoch  647/3000 | Loss: 5.5509 | Acc: 0.1200 | LR: 0.000890\n",
      "Epoch  648/3000 | Loss: 5.5481 | Acc: 0.1215 | LR: 0.000889\n",
      "Epoch  649/3000 | Loss: 5.5411 | Acc: 0.1226 | LR: 0.000889\n",
      "Epoch  650/3000 | Loss: 5.5576 | Acc: 0.1214 | LR: 0.000889\n",
      "Epoch  651/3000 | Loss: 5.5324 | Acc: 0.1207 | LR: 0.000888\n",
      "Epoch  652/3000 | Loss: 5.5315 | Acc: 0.1245 | LR: 0.000888\n",
      "Epoch  653/3000 | Loss: 5.5418 | Acc: 0.1220 | LR: 0.000888\n",
      "Epoch  654/3000 | Loss: 5.5364 | Acc: 0.1244 | LR: 0.000887\n",
      "Epoch  655/3000 | Loss: 5.5464 | Acc: 0.1209 | LR: 0.000887\n",
      "Epoch  656/3000 | Loss: 5.5379 | Acc: 0.1225 | LR: 0.000887\n",
      "Epoch  657/3000 | Loss: 5.5418 | Acc: 0.1203 | LR: 0.000886\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to train the model with the current hyperparameters.\n",
    "# You can rerun this cell to experiment with different:\n",
    "# - Learning rates (LEARNING_RATE)\n",
    "# - Batch sizes (BATCH_SIZE)\n",
    "# - Training strategies (K_MAX, WEIGHT_DECAY, GRAD_CLIP_MAX_NORM)\n",
    "# - Logging intervals (LOG_INTERVAL, EVAL_INTERVAL)\n",
    "# \n",
    "# The model from Cell 7 and dataset from Cell 6 will be used!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Training...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model, losses, accuracies = train_sudoku_diffusion(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    grad_clip_max_norm=GRAD_CLIP_MAX_NORM,\n",
    "    log_interval=LOG_INTERVAL,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    k_max=K_MAX,\n",
    "    device=DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c56309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
