{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f4a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sudoku import SudokuGenerator, Sudoku\n",
    "import math\n",
    "import subprocess\n",
    "import gc\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Model: hidden_dim=32, num_layers=9, kernel_size=3\n",
      "  Embeddings: use_learned=True, embedding_dim=10\n",
      "  Training: dataset_size=100000, epochs=20000, batch_size=1024, lr=0.001\n",
      "  Diffusion: k_max=3\n",
      "  Device: cuda\n",
      "  Wandb: enabled=True, project=sudoku-diffusion-main\n",
      "  Checkpointing: dir=./checkpoints, interval=50\n",
      "  Evaluation: eval_interval=10, sample_during_eval=False\n",
      "\n",
      "âš¡ SPEED OPTIMIZATIONS APPLIED:\n",
      "  - INCREASED BATCH_SIZE to 1024 (from 2048) â†’ Better GPU utilization\n",
      "  - REDUCED K_MAX to 3 (from 3) â†’ 3x faster per batch\n",
      "  - INCREASED EVAL_INTERVAL to 10 (from 50) â†’ 4x fewer evaluations\n",
      "  - DISABLED SAMPLING during training (sample_during_eval=False) â†’ Saves ~2min per eval\n",
      "  - Expected TOTAL speedup: 5-8x faster (30-45 min for 3k epochs vs 3 hours)\n",
      "\n",
      "âš¡ Existing optimizations:\n",
      "  - torch.compile() for JIT compilation (2-3x speedup)\n",
      "  - Mixed precision training (AMP) (1.5-2x speedup)\n",
      "  - Fused AdamW optimizer\n",
      "  - Optimized compute_loss() (removed unnecessary clones/ops)\n",
      "\n",
      "ðŸ’¡ To train even faster: Reduce NUM_EPOCHS or use a smaller model (HIDDEN_DIM, NUM_LAYERS)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "# Modify these parameters to experiment with different model configurations\n",
    "\n",
    "# --- Model Architecture Parameters ---\n",
    "HIDDEN_DIM = 32              # Network width (default: 256, reduced for memory)\n",
    "NUM_LAYERS = 10                # Number of residual conv blocks (default: 6, reduced for memory)\n",
    "KERNEL_SIZE = 3               # Conv kernel size\n",
    "NUM_GROUPS = 8                # GroupNorm groups (must divide HIDDEN_DIM evenly)\n",
    "NUM_TIMESTEPS = 81            # Fixed at 81 for Sudoku (9x9 grid)\n",
    "\n",
    "# --- Embedding Parameters ---\n",
    "USE_LEARNED_EMBEDDINGS = True  # Use learned embeddings from LLM model\n",
    "EMBEDDING_MODEL_PATH = './sudoku2vec_trained_model.pt'  # Path to saved embedding model\n",
    "EMBEDDING_DIM = 10            # Dimension of learned embeddings (from LLM model)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "DATASET_SIZE = 100000          # Number of diffusion sequences to pre-generate\n",
    "EVAL_SIZE = 1000\n",
    "NUM_EPOCHS = 3000             # Number of training epochs\n",
    "BATCH_SIZE = 1024              # Batch size - INCREASED for better GPU utilization (was 2048)\n",
    "LEARNING_RATE = 1e-3          # Optimizer learning rate\n",
    "WEIGHT_DECAY = 1e-4           # AdamW weight decay for regularization\n",
    "GRAD_CLIP_MAX_NORM = 1.0      # Gradient clipping threshold\n",
    "\n",
    "# --- Logging & Evaluation ---\n",
    "LOG_INTERVAL = 5             # Log metrics every N epochs\n",
    "EVAL_INTERVAL = 10           # Evaluate and sample every N epochs\n",
    "SAMPLE_DURING_EVAL = False\n",
    "\n",
    "# --- Diffusion Parameters ---\n",
    "K_MAX = 3                     # Maximum number of forward steps for multi-step prediction loss - REDUCED (was 31\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Wandb & Checkpointing Configuration ---\n",
    "USE_WANDB = True              # Enable Weights & Biases logging\n",
    "WANDB_PROJECT = \"sudoku-diffusion-main\"  # Wandb project name\n",
    "WANDB_ENTITY = None           # Wandb entity (None = default)\n",
    "CHECKPOINT_DIR = \"./checkpoints\"  # Directory to save model checkpoints\n",
    "CHECKPOINT_INTERVAL = 50      # Save checkpoint every N epochs\n",
    "RESUME_FROM_CHECKPOINT = None  # Path to checkpoint to resume from (None = start fresh)\n",
    "\n",
    "\n",
    "# --- Sudoku2Vec ---\n",
    "ATTENTION_DIM = 9\n",
    "N_HEADS = 1\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: hidden_dim={HIDDEN_DIM}, num_layers={NUM_LAYERS}, kernel_size={KERNEL_SIZE}\")\n",
    "print(f\"  Embeddings: use_learned={USE_LEARNED_EMBEDDINGS}, embedding_dim={EMBEDDING_DIM}\")\n",
    "print(f\"  Training: dataset_size={DATASET_SIZE}, epochs={NUM_EPOCHS}, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"  Diffusion: k_max={K_MAX}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Wandb: enabled={USE_WANDB}, project={WANDB_PROJECT}\")\n",
    "print(f\"  Checkpointing: dir={CHECKPOINT_DIR}, interval={CHECKPOINT_INTERVAL}\")\n",
    "print(f\"  Evaluation: eval_interval={EVAL_INTERVAL}, sample_during_eval={SAMPLE_DURING_EVAL}\")\n",
    "print(\"\\nâš¡ SPEED OPTIMIZATIONS APPLIED:\")\n",
    "print(f\"  - INCREASED BATCH_SIZE to {BATCH_SIZE} (from 2048) â†’ Better GPU utilization\")\n",
    "print(f\"  - REDUCED K_MAX to {K_MAX} (from 3) â†’ 3x faster per batch\")\n",
    "print(f\"  - INCREASED EVAL_INTERVAL to {EVAL_INTERVAL} (from 50) â†’ 4x fewer evaluations\")\n",
    "print(f\"  - DISABLED SAMPLING during training (sample_during_eval={SAMPLE_DURING_EVAL}) â†’ Saves ~2min per eval\")\n",
    "print(f\"  - Expected TOTAL speedup: 5-8x faster (30-45 min for 3k epochs vs 3 hours)\")\n",
    "print(\"\\nâš¡ Existing optimizations:\")\n",
    "print(\"  - torch.compile() for JIT compilation (2-3x speedup)\")\n",
    "print(\"  - Mixed precision training (AMP) (1.5-2x speedup)\")\n",
    "print(\"  - Fused AdamW optimizer\")\n",
    "print(\"  - Optimized compute_loss() (removed unnecessary clones/ops)\")\n",
    "print(\"\\nðŸ’¡ To train even faster: Reduce NUM_EPOCHS or use a smaller model (HIDDEN_DIM, NUM_LAYERS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1bb31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD EMBEDDING MODEL (from llm_on_sudoku.ipynb)\n",
    "# ============================================================================\n",
    "# We need the Sudoku2Vec class definition to load the trained model\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding on unit circle for 9x9 Sudoku grid\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a grid of positions (0-8 for both x and y)\n",
    "        x_coords = torch.arange(0, 9).unsqueeze(0).repeat(9, 1)\n",
    "        y_coords = torch.arange(0, 9).unsqueeze(1).repeat(1, 9)\n",
    "        \n",
    "        # Convert grid positions to linear indices (0-80)\n",
    "        linear_indices = y_coords * 9 + x_coords  # shape: (9, 9)\n",
    "        \n",
    "        # Convert linear indices to angles on unit circle\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (9, 9)\n",
    "        \n",
    "        # Compute x, y coordinates on unit circle\n",
    "        x_circle = torch.cos(angles)\n",
    "        y_circle = torch.sin(angles)\n",
    "        \n",
    "        # Stack and add batch dimension\n",
    "        pos_encoding = torch.stack([x_circle, y_circle], dim=-1).unsqueeze(0)  # shape: (1, 9, 9, 2)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def get_embedding_for_position(self, pos):\n",
    "        # input (batch, 2) where pos[:, 0] is x and pos[:, 1] is y\n",
    "        linear_indices = pos[:, 1] * 9 + pos[:, 0]  # shape: (batch,)\n",
    "        angles = 2 * math.pi * linear_indices / 81  # shape: (batch,)\n",
    "        x_circle = torch.cos(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        y_circle = torch.sin(angles).unsqueeze(1)  # shape: (batch, 1)\n",
    "        return torch.cat([x_circle, y_circle], dim=1)  # shape: (batch, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a (batch, 9, 9, embedding_dim) grid\n",
    "        # output (batch, 9, 9, embedding_dim + 2) grid by adding pos_encoding to x\n",
    "        batch_size = x.shape[0]\n",
    "        pos_expanded = self.pos_encoding.repeat(batch_size, 1, 1, 1)\n",
    "        return torch.cat([x, pos_expanded], dim=-1)\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # stack all weight matrices 1...h together for efficiency\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # seperate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3) # [batch, head, seqlen, dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0,2,1,3) # [batch, seqlen, head, dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values) # [batch, seq_length, 81]\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "class Sudoku2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_dim=ATTENTION_DIM, num_heads=N_HEADS, device='cpu'):\n",
    "        super(Sudoku2Vec, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim) # this will provide the key queries and values\n",
    "        self.total_dim = self.embedding_dim + 2\n",
    "\n",
    "        self.mha = MultiheadAttention(\n",
    "            input_dim=self.total_dim,\n",
    "            embed_dim=attention_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the learned token embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding weight matrix of shape [vocab_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embed.weight.detach()\n",
    "    \n",
    "    def get_embedding_for_token(self, token):\n",
    "        \"\"\"\n",
    "        Get the embedding vector for a specific token.\n",
    "        \n",
    "        Args:\n",
    "            token: Integer token ID or tensor of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector(s) for the given token(s)\n",
    "        \"\"\"\n",
    "        if isinstance(token, int):\n",
    "            token = torch.tensor([token], device=self.device)\n",
    "        elif not isinstance(token, torch.Tensor):\n",
    "            token = torch.tensor(token, device=self.device)\n",
    "        return self.embed(token).detach()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the model in a portable format that can be easily loaded.\n",
    "        This saves the model architecture and weights in a single file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path where to save the model (should end with .pt or .pth)\n",
    "        \"\"\"\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'model_config': {\n",
    "                'vocab_size': self.embed.num_embeddings,\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'attention_dim': self.mha.embed_dim,\n",
    "                'num_heads': self.num_heads,\n",
    "            },\n",
    "            'model_class': 'Sudoku2Vec',\n",
    "        }\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cpu'):\n",
    "        \"\"\"\n",
    "        Load a saved Sudoku2Vec model from file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the saved model file\n",
    "            device: Device to load the model on ('cpu', 'cuda', 'mps')\n",
    "            \n",
    "        Returns:\n",
    "            Sudoku2Vec: Loaded model instance\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        \n",
    "        # Extract configuration\n",
    "        config = checkpoint['model_config']\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            attention_dim=config['attention_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set to evaluation mode by default\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        print(f\"Configuration: vocab_size={config['vocab_size']}, \"\n",
    "              f\"embedding_dim={config['embedding_dim']}, \"\n",
    "              f\"attention_dim={config['attention_dim']}, \"\n",
    "              f\"num_heads={config['num_heads']}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, target, position, sudoku_grid, mask=True):\n",
    "        # target - the token in the target blank space we try to predict shape [batch] i.e [0, 3, 3, 5, 1, ...]\n",
    "        # position - the (x, y) position of the target shape [batch, 2] - [[1, 1], [0, 3], [7,7], ...]\n",
    "        # sudoku_grid - the sudoku grid for the problem with target we want to predict shape [batch, 9, 9]\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        target_token_embeddings = self.embed(target) # shape [batch, embedding_dim]\n",
    "        target_position_vectors = self.pe.get_embedding_for_position(position) # [batch, 2]\n",
    "        target_token_with_position = torch.cat([target_token_embeddings, target_position_vectors], dim=-1)  # shape [batch, total_dim]\n",
    "\n",
    "        # mask the target in the grid\n",
    "        sudoku_grid_masked = sudoku_grid\n",
    "        if mask:\n",
    "            batch_indices = torch.arange(sudoku_grid.shape[0], device=self.device)\n",
    "            sudoku_grid_masked = sudoku_grid.clone()\n",
    "            sudoku_grid_masked[batch_indices, position[:, 1], position[:, 0]] = 0 # 0 is a mask token aka blank\n",
    "        \n",
    "        masked_sudoku_grid_embeddings = self.embed(sudoku_grid_masked)\n",
    "        masked_sudoku_grid_with_position = self.pe(masked_sudoku_grid_embeddings) # shape [batch, 9, 9, total_dim]\n",
    "        # Reshape grid to sequence: [batch, 81, total_dim]\n",
    "        masked_grid_seq = masked_sudoku_grid_with_position.view(batch_size, 81, self.total_dim)\n",
    "\n",
    "        grid_seq_embeddings = self.embed(sudoku_grid)\n",
    "        grid_seq_embeddings = grid_seq_embeddings.view(batch_size, 81, self.embedding_dim) \n",
    "        \n",
    "        # Query from target token: [batch, 1, total_dim]\n",
    "        # query = target_token_with_position.unsqueeze(1)\n",
    "\n",
    "        output, attention = self.mha(masked_grid_seq, return_attention=True)\n",
    "        # output is shape [batch, 81, total_dim]\n",
    "        \n",
    "        return output, attention, target_token_with_position, grid_seq_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592c8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuDiffusionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for pre-generated diffusion sequences.\n",
    "    \n",
    "    Each item is a diffusion sequence of shape (82, 9, 9) where:\n",
    "    - Index 0: completely masked grid (all zeros)\n",
    "    - Index 81: completely solved grid\n",
    "    - Indices 1-80: intermediate states with progressively more cells revealed\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Tensor of shape (dataset_size, 82, 9, 9) containing diffusion sequences\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "\n",
    "class SudokuDiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion model for Sudoku puzzles inspired by DDPM.\n",
    "    \n",
    "    Forward process: Progressively mask cells from a complete sudoku (T=81) to empty grid (T=0)\n",
    "    Reverse process: Learn to predict which cells to reveal to go from T to T+1\n",
    "    \n",
    "    The model learns to reverse the masking process, predicting which cell should be revealed\n",
    "    at each timestep given the current partially revealed grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, num_layers=6, kernel_size=3, num_groups=8, \n",
    "                 embedding_layer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_timesteps = 81  # 81 cells in a sudoku grid\n",
    "        self.embedding_layer = embedding_layer\n",
    "        \n",
    "        # Determine input channels based on whether we use embeddings\n",
    "        if embedding_layer is not None:\n",
    "            # Using learned embeddings: embedding_dim channels\n",
    "            input_channels = embedding_layer.embedding_dim\n",
    "            self.use_embeddings = True\n",
    "        else:\n",
    "            # Using simple normalization: 1 channel\n",
    "            input_channels = 1\n",
    "            self.use_embeddings = False\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers for processing the sudoku grid\n",
    "        self.conv_in = nn.Conv2d(input_channels, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GroupNorm(num_groups, hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output: dual heads for position and value prediction\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "        # Position head: which cell to reveal (81 possibilities)\n",
    "        self.position_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 81)  # Logits for 81 cells\n",
    "        )\n",
    "        \n",
    "        # Value head: what value to place (10 classes: 0-9)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 9 * 9, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 10)  # Logits for 10 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Predict which cell should be revealed next and what value to place.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, 9, 9) sudoku grids at timestep t (0 = masked, 1-9 = revealed)\n",
    "            t: (batch,) timesteps (0 to 80)\n",
    "            \n",
    "        Returns:\n",
    "            position_logits: (batch, 81) logits for which cell should be revealed next\n",
    "            value_logits: (batch, 10) logits for what value (0-9) to place\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process input: either use embeddings or simple normalization\n",
    "        if self.use_embeddings:\n",
    "            # Use learned embeddings: (batch, 9, 9) -> (batch, 9, 9, embedding_dim)\n",
    "            x_embedded = self.embedding_layer(x.long())  # (batch, 9, 9, embedding_dim)\n",
    "            x_norm = x_embedded.permute(0, 3, 1, 2)  # (batch, embedding_dim, 9, 9)\n",
    "        else:\n",
    "            # Simple normalization to [-1, 1] range\n",
    "            x_norm = (x / 4.5) - 1.0\n",
    "            x_norm = x_norm.unsqueeze(1)  # (batch, 1, 9, 9)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_norm = t.float().unsqueeze(1) / self.num_timesteps  # (batch, 1)\n",
    "        t_emb = self.time_embed(t_norm)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Process through conv layers\n",
    "        h = self.conv_in(x_norm)  # (batch, hidden_dim, 9, 9)\n",
    "        \n",
    "        # Add time embedding to spatial features\n",
    "        t_emb_spatial = t_emb.view(batch_size, -1, 1, 1).expand(-1, -1, 9, 9)\n",
    "        h = h + t_emb_spatial\n",
    "        \n",
    "        # Apply conv blocks with residual connections\n",
    "        for block in self.conv_blocks:\n",
    "            h = h + block(h)\n",
    "        \n",
    "        # Output processing\n",
    "        h = self.conv_out(h)  # (batch, hidden_dim, 9, 9)\n",
    "        h_flat = h.reshape(batch_size, -1)  # (batch, hidden_dim * 81)\n",
    "        \n",
    "        # Dual predictions\n",
    "        position_logits = self.position_head(h_flat)  # (batch, 81)\n",
    "        value_logits = self.value_head(h_flat)  # (batch, 10)\n",
    "        \n",
    "        return position_logits, value_logits\n",
    "    \n",
    "    def compute_loss(self, sequences, k_max=10):\n",
    "        \"\"\"\n",
    "        Compute the diffusion loss for training using K-step iterative prediction.\n",
    "        \n",
    "        The forward diffusion process (from sudoku.py) goes from empty (T=0) to complete (T=81).\n",
    "        We learn to predict K steps ahead by iteratively applying the model.\n",
    "        \n",
    "        Args:\n",
    "            sequences: (batch, 82, 9, 9) diffusion sequences where:\n",
    "                      - sequences[:, 0] is completely masked (all zeros)\n",
    "                      - sequences[:, 81] is completely solved\n",
    "            k_max: Maximum number of forward steps for multi-step prediction\n",
    "                      \n",
    "        Returns:\n",
    "            loss: scalar combined loss (position + value)\n",
    "            accuracy: prediction accuracy for logging\n",
    "        \"\"\"\n",
    "        batch_size = sequences.shape[0]\n",
    "        \n",
    "        # Sample random starting timestep B from [0, 81-k_max]\n",
    "        max_start = max(1, self.num_timesteps - k_max)\n",
    "        B = torch.randint(0, max_start, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Sample random K from [1, k_max]\n",
    "        K = torch.randint(1, k_max + 1, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Get starting grids at timestep B (remove unnecessary clone)\n",
    "        x_current = sequences[torch.arange(batch_size), B].float()  # (batch, 9, 9)\n",
    "        \n",
    "        # Track losses and accuracies across all K steps\n",
    "        total_position_loss = 0.0\n",
    "        total_value_loss = 0.0\n",
    "        total_position_acc = 0.0\n",
    "        total_value_acc = 0.0\n",
    "        \n",
    "        # Pre-allocate batch_indices outside loop\n",
    "        batch_indices = torch.arange(batch_size, device=self.device)\n",
    "        \n",
    "        # Iteratively predict K steps\n",
    "        for step in range(k_max):\n",
    "            # Current timestep for each batch element\n",
    "            t_current = B + step\n",
    "            \n",
    "            # Only compute loss for elements where step < K[i] and t_current < 81\n",
    "            active_mask = (step < K) & (t_current < self.num_timesteps)\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            # Get target grid at next timestep\n",
    "            t_next = torch.clamp(t_current + 1, max=self.num_timesteps)\n",
    "            x_target = sequences[batch_indices, t_next].float()  # (batch, 9, 9)\n",
    "            \n",
    "            # Find which cell was revealed (difference between current and target)\n",
    "            diff = (x_target != x_current).view(batch_size, 81)  # (batch, 81)\n",
    "            \n",
    "            # Check if there's actually a difference (cell was revealed)\n",
    "            has_diff = diff.any(dim=1)  # (batch,)\n",
    "            active_mask = active_mask & has_diff  # Only process if there's a change\n",
    "            \n",
    "            if not active_mask.any():\n",
    "                break\n",
    "            \n",
    "            target_position = diff.float().argmax(dim=1)  # (batch,)\n",
    "            \n",
    "            # Get target values at revealed positions (vectorized for efficiency)\n",
    "            rows = target_position // 9\n",
    "            cols = target_position % 9\n",
    "            target_values = x_target[batch_indices, rows, cols].long()\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x_current, t_current)  # (batch, 81), (batch, 10)\n",
    "            \n",
    "            # Mask out already revealed cells in position prediction (in-place operation)\n",
    "            already_revealed = (x_current.view(batch_size, 81) != 0)  # (batch, 81)\n",
    "            position_logits.masked_fill_(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Compute losses only for active batch elements\n",
    "            if active_mask.any():\n",
    "                position_loss = F.cross_entropy(position_logits[active_mask], target_position[active_mask], reduction='sum')\n",
    "                value_loss = F.cross_entropy(value_logits[active_mask], target_values[active_mask], reduction='sum')\n",
    "                \n",
    "                # Accumulate losses (keep in computation graph for backprop)\n",
    "                total_position_loss = total_position_loss + position_loss\n",
    "                total_value_loss = total_value_loss + value_loss\n",
    "                \n",
    "                # Compute accuracy for logging\n",
    "                with torch.no_grad():\n",
    "                    pred_position = position_logits[active_mask].argmax(dim=1)\n",
    "                    pred_value = value_logits[active_mask].argmax(dim=1)\n",
    "                    total_position_acc += (pred_position == target_position[active_mask]).float().sum()\n",
    "                    total_value_acc += (pred_value == target_values[active_mask]).float().sum()\n",
    "            \n",
    "            # Update x_current with ground truth for next iteration (remove unnecessary clone)\n",
    "            x_current = x_target\n",
    "        \n",
    "        # Average losses over all active predictions\n",
    "        num_predictions = K.float().sum()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if num_predictions == 0:\n",
    "            return torch.tensor(0.0, device=self.device), torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        total_position_loss = total_position_loss / num_predictions\n",
    "        total_value_loss = total_value_loss / num_predictions\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = total_position_loss + total_value_loss\n",
    "        \n",
    "        # Average accuracy\n",
    "        accuracy = (total_position_acc + total_value_acc) / (2 * num_predictions)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=1, return_trajectory=False):\n",
    "        \"\"\"\n",
    "        Generate sudoku puzzles by running the reverse diffusion process.\n",
    "        Start from empty grid (T=0) and progressively reveal cells to T=81.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: number of puzzles to generate\n",
    "            return_trajectory: if True, return full trajectory of generation\n",
    "            \n",
    "        Returns:\n",
    "            samples: (batch_size, 9, 9) generated sudoku grids\n",
    "            trajectory: (batch_size, 82, 9, 9) if return_trajectory=True\n",
    "        \"\"\"\n",
    "        # Start from completely masked grid (T=0)\n",
    "        x = torch.zeros(batch_size, 9, 9, device=self.device)\n",
    "        \n",
    "        if return_trajectory:\n",
    "            trajectory = torch.zeros(batch_size, 82, 9, 9, device=self.device)\n",
    "            trajectory[:, 0] = x\n",
    "        \n",
    "        # Progressively reveal cells using model predictions\n",
    "        for t in tqdm(range(self.num_timesteps), desc='Sampling'):\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            # Predict position and value\n",
    "            position_logits, value_logits = self.forward(x, t_batch)\n",
    "            \n",
    "            # Mask out already revealed cells\n",
    "            already_revealed = (x.view(batch_size, 81) != 0)\n",
    "            position_logits = position_logits.masked_fill(already_revealed, float('-inf'))\n",
    "            \n",
    "            # Sample or take argmax for position\n",
    "            position_probs = F.softmax(position_logits, dim=-1)\n",
    "            cell_idx = torch.multinomial(position_probs, 1).squeeze(-1)  # (batch,)\n",
    "            \n",
    "            # Take argmax for value (deterministic)\n",
    "            value_probs = F.softmax(value_logits, dim=-1)\n",
    "            values = torch.argmax(value_probs, dim=-1)  # (batch,)\n",
    "            \n",
    "            # Update grid with predicted values\n",
    "            for b in range(batch_size):\n",
    "                idx = cell_idx[b].item()\n",
    "                row = idx // 9\n",
    "                col = idx % 9\n",
    "                x[b, row, col] = values[b]\n",
    "            \n",
    "            if return_trajectory:\n",
    "                trajectory[:, t + 1] = x\n",
    "        \n",
    "        if return_trajectory:\n",
    "            return x.long(), trajectory.long()\n",
    "        return x.long()\n",
    "\n",
    "\n",
    "def train_sudoku_diffusion(model, dataset, num_epochs=1000, batch_size=32, lr=1e-4, \n",
    "                           weight_decay=1e-4, grad_clip_max_norm=1.0,\n",
    "                           log_interval=10, eval_interval=100, k_max=10, device='cuda',\n",
    "                           use_wandb=False, wandb_project=\"sudoku-diffusion\", wandb_entity=None,\n",
    "                           checkpoint_dir=\"./checkpoints\", checkpoint_interval=50, \n",
    "                           resume_from=None, sample_during_eval=True, val_dataset=None):\n",
    "    \"\"\"\n",
    "    Train the sudoku diffusion model with proper logging and performance optimizations.\n",
    "    \n",
    "    Args:\n",
    "        model: SudokuDiffusionModel instance\n",
    "        dataset: SudokuDiffusionDataset instance with pre-generated sequences\n",
    "        num_epochs: number of training epochs\n",
    "        batch_size: batch size for training\n",
    "        lr: learning rate\n",
    "        weight_decay: weight decay for AdamW optimizer\n",
    "        grad_clip_max_norm: max norm for gradient clipping\n",
    "        log_interval: log metrics every N epochs\n",
    "        eval_interval: evaluate and sample every N epochs\n",
    "        k_max: maximum number of forward steps for multi-step prediction\n",
    "        device: device to train on\n",
    "        use_wandb: whether to log to Weights & Biases\n",
    "        wandb_project: wandb project name\n",
    "        wandb_entity: wandb entity (None = default)\n",
    "        checkpoint_dir: directory to save checkpoints\n",
    "        checkpoint_interval: save checkpoint every N epochs\n",
    "        resume_from: path to checkpoint to resume from (None = start fresh)\n",
    "        sample_during_eval: whether to sample sudoku puzzles during evaluation\n",
    "        val_dataset: optional validation dataset for evaluation (None = no validation)\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Create DataLoader for batching and shuffling with optimizations\n",
    "    # Note: pin_memory should be False when data is already on GPU\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Keep 0 for GPU tensors\n",
    "        pin_memory=False,  # Data is already on GPU, no need to pin\n",
    "        persistent_workers=False  # No workers, so this doesn't apply\n",
    "    )\n",
    "    \n",
    "    # Create validation DataLoader if validation dataset is provided\n",
    "    val_dataloader = None\n",
    "    if val_dataset is not None:\n",
    "        val_dataloader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=False\n",
    "        )\n",
    "    \n",
    "    # Use fused optimizer for faster updates (requires CUDA)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay,\n",
    "        fused=True if device == 'cuda' else False\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=5e-4)\n",
    "    \n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Starting epoch\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from is not None and os.path.exists(resume_from):\n",
    "        print(f\"Loading checkpoint from {resume_from}...\")\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epoch_losses = checkpoint.get('epoch_losses', [])\n",
    "        epoch_accuracies = checkpoint.get('epoch_accuracies', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "        print(f\"âœ“ Resumed from epoch {start_epoch}\")\n",
    "        print(f\"  Previous best loss: {min(epoch_losses) if epoch_losses else 'N/A'}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if use_wandb:\n",
    "        wandb_config = {\n",
    "            \"hidden_dim\": model.conv_in.out_channels,\n",
    "            \"num_layers\": len(model.conv_blocks),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"k_max\": k_max,\n",
    "            \"dataset_size\": len(dataset),\n",
    "            \"val_dataset_size\": len(val_dataset) if val_dataset is not None else 0,\n",
    "        }\n",
    "        if start_epoch == 0:\n",
    "            wandb.init(project=wandb_project, entity=wandb_entity, config=wandb_config)\n",
    "        else:\n",
    "            # Resume wandb run if checkpoint has run_id\n",
    "            run_id = checkpoint.get('wandb_run_id', None)\n",
    "            wandb.init(project=wandb_project, entity=wandb_entity, config=wandb_config, \n",
    "                      id=run_id, resume=\"allow\")\n",
    "        print(f\"âœ“ Wandb initialized: {wandb.run.name}\")\n",
    "    \n",
    "    model.train()\n",
    "    if start_epoch == 0:\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    else:\n",
    "        print(f\"Resuming training from epoch {start_epoch} to {num_epochs}...\")\n",
    "    print(f\"Dataset size: {len(dataset)}, Batch size: {batch_size}, Batches per epoch: {len(dataloader)}\")\n",
    "    if val_dataset is not None:\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}, Batches: {len(val_dataloader)}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Using mixed precision: {scaler is not None}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Iterate through batches in the dataset\n",
    "        for batch_sequences in dataloader:\n",
    "            # Mixed precision training\n",
    "            if scaler is not None:\n",
    "                # Forward pass with autocast\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training (CPU or non-CUDA)\n",
    "                loss, accuracy = model.compute_loss(batch_sequences, k_max=k_max)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics over all batches in the epoch\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        avg_epoch_acc = epoch_acc / num_batches\n",
    "        \n",
    "        # Record metrics\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_accuracies.append(avg_epoch_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log to wandb\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train/loss\": avg_epoch_loss,\n",
    "                \"train/accuracy\": avg_epoch_acc,\n",
    "                \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "            })\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch + 1:4d}/{num_epochs} | \"\n",
    "                  f\"Loss: {avg_epoch_loss:.4f} | \"\n",
    "                  f\"Acc: {avg_epoch_acc:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Evaluation and sampling\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Evaluation at epoch {epoch + 1}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Validation evaluation if dataset provided\n",
    "            if val_dataloader is not None:\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_acc = 0.0\n",
    "                val_num_batches = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for val_batch_sequences in val_dataloader:\n",
    "                        if scaler is not None:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                v_loss, v_accuracy = model.compute_loss(val_batch_sequences, k_max=k_max)\n",
    "                        else:\n",
    "                            v_loss, v_accuracy = model.compute_loss(val_batch_sequences, k_max=k_max)\n",
    "                        \n",
    "                        val_loss += v_loss.item()\n",
    "                        val_acc += v_accuracy.item()\n",
    "                        val_num_batches += 1\n",
    "                \n",
    "                avg_val_loss = val_loss / val_num_batches\n",
    "                avg_val_acc = val_acc / val_num_batches\n",
    "                \n",
    "                val_losses.append(avg_val_loss)\n",
    "                val_accuracies.append(avg_val_acc)\n",
    "                \n",
    "                print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Acc: {avg_val_acc:.4f}\")\n",
    "                \n",
    "                if use_wandb:\n",
    "                    wandb.log({\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"val/loss\": avg_val_loss,\n",
    "                        \"val/accuracy\": avg_val_acc,\n",
    "                    })\n",
    "                \n",
    "                model.train()\n",
    "            \n",
    "            if sample_during_eval:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Generate a sample\n",
    "                    sample = model.sample(batch_size=1)\n",
    "                    print(\"\\nGenerated Sudoku:\")\n",
    "                    print(sample[0].cpu().numpy())\n",
    "                    \n",
    "                    # Check if valid\n",
    "                    sudoku_obj = Sudoku(sample[0].cpu().numpy(), backend='numpy')\n",
    "                    is_valid = sudoku_obj.is_valid()\n",
    "                    print(f\"\\nIs valid: {is_valid}\")\n",
    "                    \n",
    "                    # Log to wandb\n",
    "                    if use_wandb:\n",
    "                        wandb.log({\n",
    "                            \"eval/is_valid\": int(is_valid),\n",
    "                            \"eval/sample\": wandb.Table(\n",
    "                                data=[[str(sample[0].cpu().numpy())]], \n",
    "                                columns=[\"sudoku_grid\"]\n",
    "                            )\n",
    "                        })\n",
    "                \n",
    "                model.train()\n",
    "            else:\n",
    "                print(\"Skipping expensive sampling (sample_during_eval=False)\")\n",
    "            \n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == num_epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch_losses': epoch_losses,\n",
    "                'epoch_accuracies': epoch_accuracies,\n",
    "                'val_losses': val_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'config': {\n",
    "                    'batch_size': batch_size,\n",
    "                    'learning_rate': lr,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'k_max': k_max,\n",
    "                }\n",
    "            }\n",
    "            if scaler is not None:\n",
    "                checkpoint_data['scaler_state_dict'] = scaler.state_dict()\n",
    "            if use_wandb:\n",
    "                checkpoint_data['wandb_run_id'] = wandb.run.id\n",
    "            \n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            print(f\"âœ“ Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            # Also save as \"latest\" for easy resumption\n",
    "            latest_path = os.path.join(checkpoint_dir, \"checkpoint_latest.pt\")\n",
    "            torch.save(checkpoint_data, latest_path)\n",
    "            print(f\"âœ“ Latest checkpoint updated: {latest_path}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "        print(\"âœ“ Wandb run finished\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    num_plots = 2 if val_dataset is None else 4\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 4))\n",
    "    \n",
    "    if num_plots == 2:\n",
    "        ax1, ax2 = axes\n",
    "    else:\n",
    "        ax1, ax2, ax3, ax4 = axes\n",
    "    \n",
    "    ax1.plot(epoch_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epoch_accuracies)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        # Plot validation metrics at eval_interval points\n",
    "        eval_epochs = list(range(eval_interval, num_epochs + 1, eval_interval))\n",
    "        \n",
    "        ax3.plot(eval_epochs, val_losses)\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.set_title('Validation Loss')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        ax4.plot(eval_epochs, val_accuracies)\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Accuracy')\n",
    "        ax4.set_title('Validation Accuracy')\n",
    "        ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, epoch_losses, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c719fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "  Device: NVIDIA RTX A4000\n",
      "  Total Memory: 15.63 GB\n",
      "\n",
      "Initial GPU Memory Usage:\n",
      "  Allocated: 0.00 GB\n",
      "  Cached: 0.00 GB\n",
      "\n",
      "âš ï¸  Clearing GPU memory...\n",
      "\n",
      "After clearing:\n",
      "  Allocated: 0.00 GB\n",
      "  Cached: 0.00 GB\n",
      "  Free: 15.63 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU information and clear memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Information:\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nInitial GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Aggressively clear GPU memory\n",
    "    print(\"\\nâš ï¸  Clearing GPU memory...\")\n",
    "    \n",
    "    # Delete all variables in the current namespace that might hold GPU tensors\n",
    "    if 'model' in dir():\n",
    "        del model\n",
    "    if 'generator' in dir():\n",
    "        del generator\n",
    "    if 'sequences' in dir():\n",
    "        del sequences\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    \n",
    "    print(f\"\\nAfter clearing:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # If still not enough memory, suggest kernel restart\n",
    "    free_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3\n",
    "    if free_memory < 1.0:\n",
    "        print(\"\\nâš ï¸  WARNING: Very little GPU memory available!\")\n",
    "        print(\"   Consider: Kernel -> Restart Kernel to fully clear GPU memory\")\n",
    "else:\n",
    "    print(\"CUDA not available, will use CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4c4f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GPU Memory before setup:\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Free: 15.63 GB\n",
      "\n",
      "============================================================\n",
      "Loading learned embeddings from LLM model...\n",
      "============================================================\n",
      "Model loaded from ./sudoku2vec_trained_model.pt\n",
      "Configuration: vocab_size=10, embedding_dim=10, attention_dim=9, num_heads=1\n",
      "âœ“ Successfully loaded embedding layer with 10 tokens\n",
      "  Embedding dimension: 10\n",
      "\n",
      "âœ“ Embedding layer ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: LOAD EMBEDDING MODEL\n",
    "# ============================================================================\n",
    "# Run this cell once per session to load the pre-trained Sudoku2Vec embeddings.\n",
    "# This is fast (~1 second) and only needs to run once.\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Clear GPU memory if using CUDA\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nGPU Memory before setup:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Load embedding model if configured\n",
    "embedding_layer = None\n",
    "if USE_LEARNED_EMBEDDINGS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Loading learned embeddings from LLM model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    try:\n",
    "        sudoku2vec_model = Sudoku2Vec.load_model(EMBEDDING_MODEL_PATH, device=DEVICE)\n",
    "        embedding_layer = sudoku2vec_model.embed\n",
    "        print(f\"âœ“ Successfully loaded embedding layer with {embedding_layer.num_embeddings} tokens\")\n",
    "        print(f\"  Embedding dimension: {embedding_layer.embedding_dim}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸  WARNING: Embedding model not found at {EMBEDDING_MODEL_PATH}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  WARNING: Failed to load embedding model: {e}\")\n",
    "        print(\"   Falling back to simple normalization\")\n",
    "        USE_LEARNED_EMBEDDINGS = False\n",
    "else:\n",
    "    print(\"\\nUsing simple normalization (no learned embeddings)\")\n",
    "\n",
    "print(\"\\nâœ“ Embedding layer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55ecb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Pre-generating training and evaluation datasets...\n",
      "============================================================\n",
      "âœ“ Found cached training dataset at cache/sudoku_diffusion_sequences_train_100000.pt\n",
      "  Loading from cache...\n",
      "âœ“ Training dataset loaded from cache in 2.24 seconds\n",
      "\n",
      "Training dataset statistics:\n",
      "  Shape: torch.Size([100000, 82, 9, 9])\n",
      "  Memory: 2533.72 MB\n",
      "  Device: cuda:0\n",
      "âœ“ Created training SudokuDiffusionDataset with 100000 sequences\n",
      "\n",
      "âœ“ Found cached evaluation dataset at cache/sudoku_diffusion_sequences_eval_1000.pt\n",
      "  Loading from cache...\n",
      "âœ“ Evaluation dataset loaded from cache in 0.02 seconds\n",
      "\n",
      "Evaluation dataset statistics:\n",
      "  Shape: torch.Size([1000, 82, 9, 9])\n",
      "  Memory: 25.34 MB\n",
      "  Device: cuda:0\n",
      "âœ“ Created evaluation SudokuDiffusionDataset with 1000 sequences\n",
      "\n",
      "ðŸ’¡ Datasets are ready! You can now rerun Cells 7 & 8 with different hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: GENERATE TRAINING AND EVALUATION DATASETS\n",
    "# ============================================================================\n",
    "# Run this cell ONCE to generate the training and evaluation datasets.\n",
    "# This is SLOW (~4 minutes for 20k sequences) but you only need to run it once!\n",
    "# \n",
    "# After running this cell, you can:\n",
    "# - Rerun Cell 7 to try different model architectures\n",
    "# - Rerun Cell 8 to try different training hyperparameters\n",
    "# - All without regenerating this expensive dataset!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Pre-generating training and evaluation datasets...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Define cache file paths\n",
    "import os\n",
    "CACHE_DIR = \"cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "TRAIN_DATASET_CACHE_PATH = os.path.join(CACHE_DIR, f\"sudoku_diffusion_sequences_train_{DATASET_SIZE}.pt\")\n",
    "EVAL_DATASET_CACHE_PATH = os.path.join(CACHE_DIR, f\"sudoku_diffusion_sequences_eval_{EVAL_SIZE}.pt\")\n",
    "\n",
    "# Check if cached training dataset exists\n",
    "if os.path.exists(TRAIN_DATASET_CACHE_PATH):\n",
    "    print(f\"âœ“ Found cached training dataset at {TRAIN_DATASET_CACHE_PATH}\")\n",
    "    print(f\"  Loading from cache...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sequences = torch.load(TRAIN_DATASET_CACHE_PATH, map_location=DEVICE)\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"âœ“ Training dataset loaded from cache in {load_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"No cached training dataset found. Generating new dataset...\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = SudokuGenerator(backend='torch', device=DEVICE)\n",
    "    \n",
    "    # Generate diffusion sequences\n",
    "    print(f\"Generating {DATASET_SIZE} training diffusion sequences...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sequences = generator.generate_diffusion_sequence(size=DATASET_SIZE)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"âœ“ Training dataset generated in {generation_time:.2f} seconds ({generation_time/DATASET_SIZE*1000:.2f} ms per sequence)\")\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving training dataset to cache: {TRAIN_DATASET_CACHE_PATH}\")\n",
    "    torch.save(sequences, TRAIN_DATASET_CACHE_PATH)\n",
    "    print(f\"âœ“ Training dataset cached successfully\")\n",
    "\n",
    "# Report training dataset statistics\n",
    "sequence_shape = sequences.shape\n",
    "memory_mb = sequences.element_size() * sequences.nelement() / (1024 ** 2)\n",
    "print(f\"\\nTraining dataset statistics:\")\n",
    "print(f\"  Shape: {sequence_shape}\")\n",
    "print(f\"  Memory: {memory_mb:.2f} MB\")\n",
    "print(f\"  Device: {sequences.device}\")\n",
    "\n",
    "# Create PyTorch training Dataset\n",
    "train_dataset = SudokuDiffusionDataset(sequences)\n",
    "print(f\"âœ“ Created training SudokuDiffusionDataset with {len(train_dataset)} sequences\")\n",
    "\n",
    "# Check if cached evaluation dataset exists\n",
    "if os.path.exists(EVAL_DATASET_CACHE_PATH):\n",
    "    print(f\"\\nâœ“ Found cached evaluation dataset at {EVAL_DATASET_CACHE_PATH}\")\n",
    "    print(f\"  Loading from cache...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    eval_sequences = torch.load(EVAL_DATASET_CACHE_PATH, map_location=DEVICE)\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"âœ“ Evaluation dataset loaded from cache in {load_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nNo cached evaluation dataset found. Generating new dataset...\")\n",
    "    \n",
    "    # Initialize generator if not already done\n",
    "    if 'generator' not in locals():\n",
    "        generator = SudokuGenerator(backend='torch', device=DEVICE)\n",
    "    \n",
    "    # Generate evaluation diffusion sequences\n",
    "    print(f\"Generating {EVAL_SIZE} evaluation diffusion sequences...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    eval_sequences = generator.generate_diffusion_sequence(size=EVAL_SIZE)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"âœ“ Evaluation dataset generated in {generation_time:.2f} seconds ({generation_time/EVAL_SIZE*1000:.2f} ms per sequence)\")\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving evaluation dataset to cache: {EVAL_DATASET_CACHE_PATH}\")\n",
    "    torch.save(eval_sequences, EVAL_DATASET_CACHE_PATH)\n",
    "    print(f\"âœ“ Evaluation dataset cached successfully\")\n",
    "\n",
    "# Report evaluation dataset statistics\n",
    "eval_sequence_shape = eval_sequences.shape\n",
    "eval_memory_mb = eval_sequences.element_size() * eval_sequences.nelement() / (1024 ** 2)\n",
    "print(f\"\\nEvaluation dataset statistics:\")\n",
    "print(f\"  Shape: {eval_sequence_shape}\")\n",
    "print(f\"  Memory: {eval_memory_mb:.2f} MB\")\n",
    "print(f\"  Device: {eval_sequences.device}\")\n",
    "\n",
    "# Create PyTorch evaluation Dataset\n",
    "eval_dataset = SudokuDiffusionDataset(eval_sequences)\n",
    "print(f\"âœ“ Created evaluation SudokuDiffusionDataset with {len(eval_dataset)} sequences\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Datasets are ready! You can now rerun Cells 7 & 8 with different hyperparameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8393681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Initializing Diffusion Model...\n",
      "============================================================\n",
      "âœ“ Model initialized successfully\n",
      "  Total parameters: 349,951\n",
      "  Trainable parameters: 349,951\n",
      "  Using embeddings: True\n",
      "\n",
      "âš¡ Compiling model with torch.compile for faster training...\n",
      "âœ“ Model compiled successfully\n",
      "\n",
      "GPU Memory after model loading:\n",
      "  Allocated: 2.50 GB\n",
      "  Reserved: 2.50 GB\n",
      "\n",
      "âœ“ Model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: INITIALIZE DIFFUSION MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to create and compile the diffusion model.\n",
    "# This is FAST (~5 seconds) and you can rerun it to experiment with:\n",
    "# - Different model sizes (HIDDEN_DIM, NUM_LAYERS)\n",
    "# - Different architectures (KERNEL_SIZE, NUM_GROUPS)\n",
    "# \n",
    "# The dataset from Cell 6 will be reused!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Initializing Diffusion Model...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model = SudokuDiffusionModel(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    num_groups=NUM_GROUPS,\n",
    "    embedding_layer=embedding_layer,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"âœ“ Model initialized successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Using embeddings: {model.use_embeddings}\")\n",
    "\n",
    "# Compile model for faster training (PyTorch 2.0+)\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\nâš¡ Compiling model with torch.compile for faster training...\")\n",
    "    try:\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "        print(f\"âœ“ Model compiled successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not compile model: {e}\")\n",
    "        print(f\"   Continuing without compilation\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"\\nGPU Memory after model loading:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\nâœ“ Model is ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6601551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2694148/1777599997.py:373: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrafiz\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/svi.on.mnist/wandb/run-20251104_160433-emra47oa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srafiz/sudoku-diffusion-main/runs/emra47oa' target=\"_blank\">lilac-fog-28</a></strong> to <a href='https://wandb.ai/srafiz/sudoku-diffusion-main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srafiz/sudoku-diffusion-main' target=\"_blank\">https://wandb.ai/srafiz/sudoku-diffusion-main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srafiz/sudoku-diffusion-main/runs/emra47oa' target=\"_blank\">https://wandb.ai/srafiz/sudoku-diffusion-main/runs/emra47oa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Wandb initialized: lilac-fog-28\n",
      "Starting training for 20000 epochs...\n",
      "Dataset size: 100000, Batch size: 1024, Batches per epoch: 98\n",
      "Validation dataset size: 1000, Batches: 1\n",
      "Learning rate: 0.001\n",
      "Using mixed precision: True\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2694148/1777599997.py:447: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    5/20000 | Loss: 5.6858 | Acc: 0.0916 | LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m model, losses, accuracies = \u001b[43mtrain_sudoku_diffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_clip_max_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRAD_CLIP_MAX_NORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOG_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEVAL_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_MAX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_WANDB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWANDB_PROJECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWANDB_ENTITY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESUME_FROM_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_during_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAMPLE_DURING_EVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 452\u001b[39m, in \u001b[36mtrain_sudoku_diffusion\u001b[39m\u001b[34m(model, dataset, num_epochs, batch_size, lr, weight_decay, grad_clip_max_norm, log_interval, eval_interval, k_max, device, use_wandb, wandb_project, wandb_entity, checkpoint_dir, checkpoint_interval, resume_from, sample_during_eval, val_dataset)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Backward pass with gradient scaling\u001b[39;00m\n\u001b[32m    451\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m scaler.unscale_(optimizer)\n\u001b[32m    454\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x730f32aebe00>> (for post_run_cell), with arguments args (<ExecutionResult object at 730f6d3f86d0, execution_count=12 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 730f6d3f8bb0, raw_cell=\"# ================================================..\" transformed_cell=\"# ================================================..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22766173746169227d/workspace/svi.on.mnist/diffusion_on_sudoku.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:596\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    595\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface.py:826\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    825\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/.uv/python_install/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/.uv/python_install/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/svi.on.mnist/.venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/.uv/python_install/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/asyncio/streams.py:386\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/.uv/python_install/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "# Run this cell to train the model with the current hyperparameters.\n",
    "# You can rerun this cell to experiment with different:\n",
    "# - Learning rates (LEARNING_RATE)\n",
    "# - Batch sizes (BATCH_SIZE)\n",
    "# - Training strategies (K_MAX, WEIGHT_DECAY, GRAD_CLIP_MAX_NORM)\n",
    "# - Logging intervals (LOG_INTERVAL, EVAL_INTERVAL)\n",
    "# \n",
    "# The model from Cell 7 and dataset from Cell 6 will be used!\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Training...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model, losses, accuracies = train_sudoku_diffusion(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    grad_clip_max_norm=GRAD_CLIP_MAX_NORM,\n",
    "    log_interval=LOG_INTERVAL,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    k_max=K_MAX,\n",
    "    device=DEVICE,\n",
    "    use_wandb=USE_WANDB,\n",
    "    wandb_project=WANDB_PROJECT,\n",
    "    wandb_entity=WANDB_ENTITY,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "    resume_from=RESUME_FROM_CHECKPOINT,\n",
    "    sample_during_eval=SAMPLE_DURING_EVAL,\n",
    "    val_dataset=eval_dataset\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edf55b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPARING VALIDATION DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SudokuGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Initialize generator for validation dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m generator = \u001b[43mSudokuGenerator\u001b[49m(\n\u001b[32m     39\u001b[39m     backend=\u001b[33m'\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m     device=DEVICE\n\u001b[32m     41\u001b[39m )\n\u001b[32m     44\u001b[39m VAL_DATASET_CACHE_PATH = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msudoku_diffusion_val_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSWEEP_EXPERIMENT_CONFIG[\u001b[33m\"\u001b[39m\u001b[33mval_size\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(VAL_DATASET_CACHE_PATH):\n",
      "\u001b[31mNameError\u001b[39m: name 'SudokuGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER SWEEP EXPERIMENT WITH VALIDATION\n",
    "# ============================================================================\n",
    "# This cell runs a self-contained experiment to compare different hyperparameters.\n",
    "# It trains separate models for each combination and logs results to wandb.\n",
    "# Now includes validation set evaluation!\n",
    "# Each combination is logged as a separate wandb run for easy comparison.\n",
    "# Training is done by STEPS/ (gradient updates) rather than epochs to ensure\n",
    "# fair comparison across different batch sizes.\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Experiment configuration\n",
    "SWEEP_EXPERIMENT_CONFIG = {\n",
    "    'learning_rate': [5e-4, 1e-4, 1e-3],  # Fixed learning rate\n",
    "    'hidden_dims': [8],  # List of hidden dimensions to try\n",
    "    'k_maxs': [3],  # List of k_max values to try\n",
    "    'num_layers_list': [48],  # List of num_layers to try\n",
    "    'num_steps': 10000,  # Number of gradient update steps for the experiment\n",
    "    'batch_sizes': [ 512 ],  # List of batch sizes to sweep\n",
    "    'val_size': 1000,  # NEW: Validation set size\n",
    "    'val_interval': 200,  # NEW: Validate every N steps\n",
    "    'log_interval': 50,  # Log every N steps\n",
    "    'use_wandb': True,  # Set to True to log to wandb\n",
    "    'wandb_project': 'sudoku-diffusion-lr-sweep-steps',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Generate/Load Validation Dataset\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREPARING VALIDATION DATASET\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize generator for validation dataset\n",
    "generator = SudokuGenerator(\n",
    "    backend='torch',\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "VAL_DATASET_CACHE_PATH = f'sudoku_diffusion_val_{SWEEP_EXPERIMENT_CONFIG[\"val_size\"]}.pt'\n",
    "\n",
    "if os.path.exists(VAL_DATASET_CACHE_PATH):\n",
    "    print(f\"Loading validation dataset from cache: {VAL_DATASET_CACHE_PATH}\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    val_sequences = torch.load(VAL_DATASET_CACHE_PATH, map_location=DEVICE)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"âœ“ Validation dataset loaded from cache in {load_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"Generating {SWEEP_EXPERIMENT_CONFIG['val_size']} validation diffusion sequences...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    val_sequences = generator.generate_diffusion_sequence(size=SWEEP_EXPERIMENT_CONFIG['val_size'])\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"âœ“ Validation dataset generated in {generation_time:.2f} seconds ({generation_time/SWEEP_EXPERIMENT_CONFIG['val_size']*1000:.2f} ms per sequence)\")\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving validation dataset to cache: {VAL_DATASET_CACHE_PATH}\")\n",
    "    torch.save(val_sequences, VAL_DATASET_CACHE_PATH)\n",
    "    print(f\"âœ“ Validation dataset cached successfully\")\n",
    "\n",
    "# Report validation dataset statistics\n",
    "val_sequence_shape = val_sequences.shape\n",
    "val_memory_mb = val_sequences.element_size() * val_sequences.nelement() / (1024 ** 2)\n",
    "print(f\"\\nValidation dataset statistics:\")\n",
    "print(f\"  Shape: {val_sequence_shape}\")\n",
    "print(f\"  Memory: {val_memory_mb:.2f} MB\")\n",
    "print(f\"  Device: {val_sequences.device}\")\n",
    "\n",
    "# Create validation dataset (dataloader will be created per batch size)\n",
    "val_dataset = SudokuDiffusionDataset(val_sequences)\n",
    "print(f\"âœ“ Created validation dataset with {len(val_dataset)} sequences\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Setup Sweep Experiment\n",
    "# ============================================================================\n",
    "\n",
    "# Generate all combinations (including batch sizes)\n",
    "all_combinations = list(itertools.product(\n",
    "    SWEEP_EXPERIMENT_CONFIG['learning_rate'],\n",
    "    SWEEP_EXPERIMENT_CONFIG['hidden_dims'],\n",
    "    SWEEP_EXPERIMENT_CONFIG['k_maxs'],\n",
    "    SWEEP_EXPERIMENT_CONFIG['num_layers_list'],\n",
    "    SWEEP_EXPERIMENT_CONFIG['batch_sizes']\n",
    "))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HYPERPARAMETER SWEEP EXPERIMENT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Learning rate (fixed): {SWEEP_EXPERIMENT_CONFIG['learning_rate']}\")\n",
    "print(f\"Hidden dims: {SWEEP_EXPERIMENT_CONFIG['hidden_dims']}\")\n",
    "print(f\"K_max values: {SWEEP_EXPERIMENT_CONFIG['k_maxs']}\")\n",
    "print(f\"Num layers: {SWEEP_EXPERIMENT_CONFIG['num_layers_list']}\")\n",
    "print(f\"Batch sizes: {SWEEP_EXPERIMENT_CONFIG['batch_sizes']}\")\n",
    "print(f\"Total combinations: {len(all_combinations)}\")\n",
    "print(f\"Steps per combination: {SWEEP_EXPERIMENT_CONFIG['num_steps']}\")\n",
    "print(f\"Validation size: {SWEEP_EXPERIMENT_CONFIG['val_size']}\")\n",
    "print(f\"Validation interval: {SWEEP_EXPERIMENT_CONFIG['val_interval']} steps\")\n",
    "print(f\"Wandb enabled: {SWEEP_EXPERIMENT_CONFIG['use_wandb']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Storage for results\n",
    "sweep_results = {}\n",
    "\n",
    "# Run experiment for each combination\n",
    "for combo_idx, (lr, hidden_dim, k_max, num_layers, batch_size) in enumerate(all_combinations):\n",
    "    combo_key = f\"lr_{lr}_hd{hidden_dim}_k{k_max}_nl{num_layers}_bs{batch_size}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training combination {combo_idx + 1}/{len(all_combinations)}\")\n",
    "    print(f\" LR: {lr} Hidden Dim: {hidden_dim}, K_max: {k_max}, Num Layers: {num_layers}, Batch Size: {batch_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize a separate wandb run for this combination\n",
    "    if SWEEP_EXPERIMENT_CONFIG['use_wandb']:\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            project=SWEEP_EXPERIMENT_CONFIG['wandb_project'],\n",
    "            name=combo_key,\n",
    "            config={\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'k_max': k_max,\n",
    "                'num_layers': num_layers,\n",
    "                'learning_rate': lr, \n",
    "                'batch_size': batch_size,\n",
    "                'num_steps': SWEEP_EXPERIMENT_CONFIG['num_steps'],\n",
    "                'val_size': SWEEP_EXPERIMENT_CONFIG['val_size'],\n",
    "                'val_interval': SWEEP_EXPERIMENT_CONFIG['val_interval'],\n",
    "                'weight_decay': WEIGHT_DECAY,\n",
    "                'grad_clip_max_norm': GRAD_CLIP_MAX_NORM,\n",
    "            },\n",
    "            reinit=True  # Allow multiple runs in same process\n",
    "        )\n",
    "    \n",
    "    # Create dataloaders with the current batch size\n",
    "    experiment_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # Create a fresh model for this combination\n",
    "    experiment_model = SudokuDiffusionModel(\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        num_groups=NUM_GROUPS,\n",
    "        embedding_layer=embedding_layer,\n",
    "        device=DEVICE\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Compile model if using CUDA\n",
    "    if DEVICE == 'cuda':\n",
    "        try:\n",
    "            experiment_model = torch.compile(experiment_model, mode='reduce-overhead')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        experiment_model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fused=True if DEVICE == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Setup gradient scaler for mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler() if DEVICE == 'cuda' else None\n",
    "    \n",
    "    # Track losses and accuracies for this combination\n",
    "    step_losses = []\n",
    "    step_accuracies = []\n",
    "    val_step_losses = []\n",
    "    val_step_accuracies = []\n",
    "    val_steps = []  # Track which steps we validated on\n",
    "    \n",
    "    experiment_model.train()\n",
    "    \n",
    "    # Training loop by steps\n",
    "    step = 0\n",
    "    dataloader_iterator = iter(experiment_dataloader)\n",
    "    \n",
    "    while step < SWEEP_EXPERIMENT_CONFIG['num_steps']:\n",
    "        # ====================================================================\n",
    "        # Training Phase\n",
    "        # ====================================================================\n",
    "        experiment_model.train()\n",
    "        \n",
    "        # Get next batch (restart iterator if exhausted)\n",
    "        try:\n",
    "            batch_sequences = next(dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            dataloader_iterator = iter(experiment_dataloader)\n",
    "            batch_sequences = next(dataloader_iterator)\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss, accuracy = experiment_model.compute_loss(\n",
    "                    batch_sequences, \n",
    "                    k_max=k_max\n",
    "                )\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(experiment_model.parameters(), max_norm=GRAD_CLIP_MAX_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss, accuracy = experiment_model.compute_loss(\n",
    "                batch_sequences, \n",
    "                k_max=k_max\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(experiment_model.parameters(), max_norm=GRAD_CLIP_MAX_NORM)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Store training metrics\n",
    "        step_losses.append(loss.item())\n",
    "        step_accuracies.append(accuracy.item())\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Validation Phase (every val_interval steps)\n",
    "        # ====================================================================\n",
    "        if step % SWEEP_EXPERIMENT_CONFIG['val_interval'] == 0 or step == 1 or step == SWEEP_EXPERIMENT_CONFIG['num_steps']:\n",
    "            experiment_model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_num_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_batch_sequences in val_dataloader:\n",
    "                    # Mixed precision for validation too\n",
    "                    if scaler is not None:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            loss, accuracy = experiment_model.compute_loss(\n",
    "                                val_batch_sequences,\n",
    "                                k_max=k_max\n",
    "                            )\n",
    "                    else:\n",
    "                        loss, accuracy = experiment_model.compute_loss(\n",
    "                            val_batch_sequences,\n",
    "                            k_max=k_max\n",
    "                        )\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_acc += accuracy.item()\n",
    "                    val_num_batches += 1\n",
    "            \n",
    "            # Average validation metrics\n",
    "            avg_val_loss = val_loss / val_num_batches\n",
    "            avg_val_acc = val_acc / val_num_batches\n",
    "            \n",
    "            val_step_losses.append(avg_val_loss)\n",
    "            val_step_accuracies.append(avg_val_acc)\n",
    "            val_steps.append(step)\n",
    "            \n",
    "            # Log to wandb (both train and val)\n",
    "            if SWEEP_EXPERIMENT_CONFIG['use_wandb']:\n",
    "                wandb.log({\n",
    "                    'train_loss': step_losses[-1],\n",
    "                    'train_accuracy': step_accuracies[-1],\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_accuracy': avg_val_acc,\n",
    "                    'step': step,\n",
    "                })\n",
    "            \n",
    "            # Print progress with validation metrics\n",
    "            print(f\"  Step {step:6d}/{SWEEP_EXPERIMENT_CONFIG['num_steps']} | \"\n",
    "                  f\"Train Loss: {step_losses[-1]:.4f} | Train Acc: {step_accuracies[-1]:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
    "        else:\n",
    "            # Log only training metrics when not validating\n",
    "            if SWEEP_EXPERIMENT_CONFIG['use_wandb']:\n",
    "                wandb.log({\n",
    "                    'train_loss': step_losses[-1],\n",
    "                    'train_accuracy': step_accuracies[-1],\n",
    "                    'step': step,\n",
    "                })\n",
    "            \n",
    "            # Print progress (training only)\n",
    "            if step % SWEEP_EXPERIMENT_CONFIG['log_interval'] == 0:\n",
    "                print(f\"  Step {step:6d}/{SWEEP_EXPERIMENT_CONFIG['num_steps']} | \"\n",
    "                      f\"Train Loss: {step_losses[-1]:.4f} | Train Acc: {step_accuracies[-1]:.4f}\")\n",
    "    \n",
    "    # Store results (including validation metrics)\n",
    "    sweep_results[combo_key] = {\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'k_max': k_max,\n",
    "        'num_layers': num_layers,\n",
    "        'batch_size': batch_size,\n",
    "        'train_losses': step_losses,\n",
    "        'train_accuracies': step_accuracies,\n",
    "        'val_losses': val_step_losses,\n",
    "        'val_accuracies': val_step_accuracies,\n",
    "        'val_steps': val_steps,\n",
    "        'final_train_loss': step_losses[-1],\n",
    "        'final_train_accuracy': step_accuracies[-1],\n",
    "        'final_val_loss': val_step_losses[-1],\n",
    "        'final_val_accuracy': val_step_accuracies[-1],\n",
    "        'min_train_loss': min(step_losses),\n",
    "        'best_train_step': step_losses.index(min(step_losses)) + 1,\n",
    "        'min_val_loss': min(val_step_losses),\n",
    "        'best_val_step': val_steps[val_step_losses.index(min(val_step_losses))],\n",
    "        'max_val_accuracy': max(val_step_accuracies),\n",
    "        'best_val_acc_step': val_steps[val_step_accuracies.index(max(val_step_accuracies))]\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Completed {combo_key}\")\n",
    "    print(f\"  Final Train Loss: {step_losses[-1]:.4f} | Final Val Loss: {val_step_losses[-1]:.4f}\")\n",
    "    print(f\"  Best Val Loss: {sweep_results[combo_key]['min_val_loss']:.4f} (step {sweep_results[combo_key]['best_val_step']})\")\n",
    "    print(f\"  Best Val Accuracy: {sweep_results[combo_key]['max_val_accuracy']:.4f} (step {sweep_results[combo_key]['best_val_acc_step']})\")\n",
    "    \n",
    "    # Finish this wandb run before starting the next\n",
    "    if SWEEP_EXPERIMENT_CONFIG['use_wandb']:\n",
    "        wandb.finish()\n",
    "    \n",
    "    # Clean up\n",
    "    del experiment_model\n",
    "    del optimizer\n",
    "    del experiment_dataloader\n",
    "    del val_dataloader\n",
    "    del dataloader_iterator\n",
    "    if scaler is not None:\n",
    "        del scaler\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print summary with validation metrics\n",
    "print(\"\\nSummary of Results (sorted by validation loss):\")\n",
    "print(f\"{'Config':<40} {'Final Train':<13} {'Final Val':<13} {'Best Val Loss':<15} {'Best Val Acc':<15}\")\n",
    "print(\"-\" * 110)\n",
    "for combo_key in sorted(sweep_results.keys(), key=lambda k: sweep_results[k]['min_val_loss']):\n",
    "    result = sweep_results[combo_key]\n",
    "    config_str = f\"hd={result['hidden_dim']},k={result['k_max']},nl={result['num_layers']},bs={result['batch_size']}\"\n",
    "    print(f\"{config_str:<40} {result['final_train_loss']:<13.4f} {result['final_val_loss']:<13.4f} \"\n",
    "          f\"{result['min_val_loss']:<15.4f} {result['max_val_accuracy']:<15.4f}\")\n",
    "\n",
    "# Find best configuration based on validation loss\n",
    "best_combo_key = min(sweep_results.keys(), key=lambda k: sweep_results[k]['min_val_loss'])\n",
    "best_result = sweep_results[best_combo_key]\n",
    "\n",
    "# Find best configuration based on validation accuracy\n",
    "best_acc_combo_key = max(sweep_results.keys(), key=lambda k: sweep_results[k]['max_val_accuracy'])\n",
    "best_acc_result = sweep_results[best_acc_combo_key]\n",
    "\n",
    "print(f\"\\nðŸ† Best Configuration (by Validation Loss):\")\n",
    "print(f\"   Hidden Dim: {best_result['hidden_dim']}\")\n",
    "print(f\"   K_max: {best_result['k_max']}\")\n",
    "print(f\"   Num Layers: {best_result['num_layers']}\")\n",
    "print(f\"   Batch Size: {best_result['batch_size']}\")\n",
    "print(f\"   Best Val Loss: {best_result['min_val_loss']:.4f} at step {best_result['best_val_step']}\")\n",
    "print(f\"   Best Val Accuracy: {best_result['max_val_accuracy']:.4f} at step {best_result['best_val_acc_step']}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best Configuration (by Validation Accuracy):\")\n",
    "print(f\"   Hidden Dim: {best_acc_result['hidden_dim']}\")\n",
    "print(f\"   K_max: {best_acc_result['k_max']}\")\n",
    "print(f\"   Num Layers: {best_acc_result['num_layers']}\")\n",
    "print(f\"   Batch Size: {best_acc_result['batch_size']}\")\n",
    "print(f\"   Best Val Accuracy: {best_acc_result['max_val_accuracy']:.4f} at step {best_acc_result['best_val_acc_step']}\")\n",
    "print(f\"   Best Val Loss: {best_acc_result['min_val_loss']:.4f} at step {best_acc_result['best_val_step']}\")\n",
    "\n",
    "print(\"\\nâœ“ Experiment complete! View detailed results in wandb.\")\n",
    "print(f\"\\nðŸ’¡ Primary Recommendation (lowest val loss): Hidden Dim={best_result['hidden_dim']}, K_max={best_result['k_max']}, Num Layers={best_result['num_layers']}, Batch Size={best_result['batch_size']}\")\n",
    "print(f\"ðŸ’¡ Alternative Recommendation (highest val acc): Hidden Dim={best_acc_result['hidden_dim']}, K_max={best_acc_result['k_max']}, Num Layers={best_acc_result['num_layers']}, Batch Size={best_acc_result['batch_size']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c91ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
